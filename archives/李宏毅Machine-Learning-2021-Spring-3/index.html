<!DOCTYPE HTML>
<html lang="english">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="renderer" content="webkit">
    <meta name="HandheldFriendly" content="true">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Maverick,AlanDecode,Galileo,blog" />
    <meta name="generator" content="Maverick 1.2.1" />
    <meta name="template" content="Prism" />
    <link rel="alternate" type="application/rss+xml" title="walker's code blog &raquo; RSS 2.0" href="/feed/index.xml" />
    <link rel="alternate" type="application/atom+xml" title="walker's code blog &raquo; ATOM 1.0" href="/feed/atom/index.xml" />
    <link rel="stylesheet" href="/assets/prism-b9d78ff38a.css">
    <link rel="stylesheet" href="/assets/ExSearch/ExSearch-182e5a8869.css">
    <link href="https://fonts.googleapis.com/css?family=Fira+Code&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css">
    <script>
        var ExSearchConfig = {
            root: "",
            api: "/66077cf5542cc50c8af36c6d25f0819c.json"
        }

    </script>
    
<title>李宏毅Machine Learning 2021 Spring笔记[3] - walker's code blog</title>
<meta name="author" content="walker" />
<meta name="description" content="CNN" />
<meta property="og:title" content="李宏毅Machine Learning 2021 Spring笔记[3] - walker's code blog" />
<meta property="og:description" content="CNN" />
<meta property="og:site_name" content="walker's code blog" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/archives/%E6%9D%8E%E5%AE%8F%E6%AF%85Machine-Learning-2021-Spring-3/" />
<meta property="og:image" content="" />
<meta property="article:published_time" content="2021-10-14T13:25:00-00.00" />
<meta name="twitter:title" content="李宏毅Machine Learning 2021 Spring笔记[3] - walker's code blog" />
<meta name="twitter:description" content="CNN" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:image" content="" />


    
</head>

<body>
    <div class="container prism-container">
        <header class="prism-header" id="prism__header">
            <h1 class="text-uppercase brand"><a class="no-link" href="/" target="_self">walker's code blog</a></h1>
            <p>coder, reader</p>
            <nav class="prism-nav"><ul><li><a class="no-link text-uppercase " href="/" target="_self">Home</a></li><li><a class="no-link text-uppercase " href="/archives/" target="_self">Archives</a></li><li><a class="no-link text-uppercase " href="/about/" target="_self">About</a></li><li><a href="#" target="_self" class="search-form-input no-link text-uppercase">Search</a></li></ul></nav>
        </header>
        <div class="prism-wrapper" id="prism__wrapper">
            
<main>
    <section class="prism-section row" id="prism__content">
        <article class="yue col-md-8 offset-md-2">
            <h1 class="prism-post-title">李宏毅Machine Learning 2021 Spring笔记[3]</h1>
            <div class="prism-post-time">
                <time class="text-uppercase">
                    October 14 2021
                </time>
            </div>
            <div class="prism-content-body">
                <h1>CNN</h1>
<ol>
<li><strong>Receptive field</strong></li>
</ol>
<p>不管是计算机，还是人脑，去认一个物，都是去判断特定的patten（所以就会有错认的图片产生），这也说明，如果神经网络要去辨识物体，是不需要每个神经元都把整张图片看一次的，只需要关注一些特征区域就好了。（感受野, <code>Receptive field</code>)</p><p>如果你一直用3x3，会不会看不到大的patten呢？$\rightarrow$ 会也不会。</p><p>首先，小的filter当然是不可能看到它的感受野以外的部分，但是，神经网络是多层架构，你这层的输出再被卷一次，这时候每一个数字代表的就是之前的9个像素计算的结果，这一轮的9个数字就是上一层的81个像素（因为stride的原因，大部分是重复的）的计算结果，换言之，感受野大大增强了，也就是说，你只需要增加层数，就可以在小的filter上得到大的patten.</p><ol start="2">
<li><strong>filter &amp; feature map</strong></li>
</ol>
<p>从神经元角度和全连接角度出发的话，每个框其实可以有自己的参数的（即你用了64步把整个图片扫描完的话，就有64组参数），而事实上为了简化模型，可以让某些框对应同样的参数（<strong>参数共享</strong>），原因就是同一特征可能出现在多个位置，比如人有两只脚。</p><p>再然后，实际上每一次都是用一组参数扫完全图的，意思是在每个角落都只搜索这<strong>一个特征</strong>。</p><p>我们把这种机制叫<code>filter</code>，一个filter只找一种特征，乘加出来的结果叫<code>feature map</code>，即这个filter提取出来的特征图。</p><p>因此，</p><ul>
<li>你想提取多少个特征，就得有多少个filter</li>
<li>表现出来就成了你这一层输出有多少个channel</li>
<li>这就是为什么你的图片进来是3channel，出来就是N个channel了，取决于你设计了多少个filter</li>
</ul>
<ol start="3">
<li><strong>Pooling &amp; subsampling</strong></li>
</ol>
<p>由于图像的视觉特征，你把它放大或缩小都能被人眼认出来，因此就产生了pooling这种机制，可以降低样本的大小，这主要是为了减小运算量吧（硬件性能足够就可以不考虑它）。</p><ol start="4">
<li><strong>Data Augmentation</strong></li>
</ol>
<p>CNN并不能识别缩放、旋转、裁切、翻转过的图片，因此训练数据的增强也是必要的。</p><h2>AlphaGo</h2>
<p><strong>layer 1</strong></p><ol>
<li>能被影像化的问题就可以尝试CNN，围棋可以看成是一张19x19的图片</li>
<li>每一个位置被总结出了48种可能的情况(超参1)</li>
<li>所以输入就是19x19x48</li>
<li>用0来padding成23x23</li>
<li>很多patten、定式也是影像化的，可以被filter扫出来</li>
<li>总结出5x5大小的filter就够用了（超参2）</li>
<li>就用了192个fitler（即每一次output有48层channel)（超参3）</li>
<li>stride = 1</li>
<li>ReLU</li>
</ol>
<p><strong>layer 2-12</strong></p><ol>
<li>padding成 21x21</li>
<li>192个 3x3 filter with stride = 1</li>
<li>ReLU</li>
</ol>
<p><strong>layer 13</strong></p><ol>
<li>1x1 filter stride = 1</li>
<li>bias</li>
<li>softmax</li>
</ol>
<p>其中192(个filter)这个超参对比了128，256，384等，也就是说人类并不理解它每一次都提取了什么特征。</p><blockquote>
<p>subsampling对围棋也有用吗？ 上面的结构看出并没有用，事实上，围棋你抽掉一行一列影响是很大的。</p></blockquote>
<h1>Self-Attention</h1>
<p>前面说的都是输入为一个向量（总会拉平成一维向量），如果是多个向量呢？有这样的场景吗？</p><ul>
<li>一段文字，每一个文字都用one-hot或word-embedding来表示<ul>
<li>不但是多个向量，而且还长短不齐</li>
</ul>
</li>
<li>一段语音，每25ms采样形成一个向量，步长为每10ms重复采样，形成向量序列<ul>
<li>400 sample points (16khz)</li>
<li>39-dim MFCC</li>
<li>80-dim filter bank output</li>
<li>参考人类语言处理课程</li>
</ul>
</li>
<li>一个Graph组向量（比如social network)<ul>
<li>每个节点（每个人的profile）就是一个向量</li>
</ul>
</li>
<li>一个分子结构<ul>
<li>每个原子就是一个one-hot</li>
</ul>
</li>
</ul>
<p><strong>输出是什么样的？</strong></p><ol>
<li>一个向量对应一个输出<ul>
<li>文字 -&gt; POS tagging</li>
<li>语音 -&gt; a, a, b, b(怎么去重也参考<a href="https://speech.ee.ntu.edu.tw/~hylee/dlhlp/2020-spring.html">人类语言处理</a>课程)</li>
<li>graph -&gt; 每个节点输出特性（比如每个人的购买决策）</li>
</ul>
</li>
<li>只有一个输出<ul>
<li>文字 -&gt; 情绪分析，舆情分析</li>
<li>语音 -&gt; 判断是谁说的</li>
<li>graph -&gt; 输出整个graph的特性，比如亲水性如何</li>
</ul>
</li>
<li>不定输出（由network自己决定）<ul>
<li>这就叫seq2seq</li>
<li>文字 -&gt; 翻译</li>
<li>语音 -&gt; 真正的语音识别</li>
</ul>
</li>
</ol>
<p>self-attention</p><p>稍稍回顾一下self attention里最重要的q, k, v的部分：</p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="/archives/assets/97e9b2e0e7ac0f115036ca6bd0c71849.png" alt=""/></figure><p>图示的是q2与所有的k相乘，再分别与对应的v相乘，然后相加，得到q2对应的输出：b2的过程。</p><p>下图则是矩阵化后的结论：
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="/archives/assets/429312b4d209231715578ba2ba3a80dc.png" alt=""/></figure>
具体细节看专题</p><p>真正要学的，就是图中的$W^q, W^k, W^v$</p><h2>Multi-head Self-attention</h2>
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="/archives/assets/4ac2f963c12f8046cf93824b2a2c5f9f.png" alt=""/></figure><p>CNN是Self-attention的特例</p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="/archives/assets/9c914c0fa757448a64dfa2c88ae110e5.png" alt=""/></figure><h2>Self-attention for Graph</h2>
<p>了解更多：<a href="https://youtu.be/eybCCtNKwzA">https://youtu.be/eybCCtNKwzA</a></p><h1>Transformer</h1>
<p>Transformer是一个seq2seq的model</p><p>以下场景，不管看上去像不像是seq2seq的特征，都可以尝试用seq2seq（trnasformer）来“硬train一发”</p><ul>
<li>QA类的问题，送进去question + context，输出answer<ul>
<li>翻译，摘要，差别，情感分析，只要训练能套上上面的格式，就有可能</li>
</ul>
</li>
<li>文法剖析，送入是句子，输出是树状的语法结构<ul>
<li>把树状结构摊平（其实就是多层括号）</li>
<li>然后就用这个对应关系来当成翻译来训练（即把语法当成翻译）</li>
</ul>
</li>
<li>multi-label classification<ul>
<li>你不能在做multi-class classification的时候取top-k,因为有的属于一个类，有的属于三个类，k不定</li>
<li>所以你把每个输入和N个输出也丢到seq2seq里去硬train一发，网络会自己学到每个文章属于哪“些”类别（不定个数，也像翻译一样）</li>
</ul>
</li>
<li>object dectection<ul>
<li>这个更匪夷所思，感兴趣看论文：<a href="https://arxiv.org/abs/2005.12872(End-to-End">https://arxiv.org/abs/2005.12872(End-to-End</a> Object Detection with Transformers)</li>
</ul>
</li>
</ul>
<h2>Encoder</h2>
<p>Q, K, V(relavant/similarity), zero padding mask, layer normalization, residual等, 具体看<code>self-attention</code>一节。</p><h2>Decoder</h2>
<h3>AT v.s. NAT</h3>
<p>我们之前用的decoder都是一个一个字地预测（输出的）</p><ul>
<li>所以才有position-mask（用来屏蔽当前位置后面的字）</li>
</ul>
<p>这种叫<code>Auto Regressive</code>，简称<code>AT</code>,<code>NAT</code>即<code>Non Auto Regressive</code></p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="/archives/assets/a4e144ed0e5f9ad95a029f3224fc46e3.png" alt=""/></figure><p>它一次生成输出的句子。</p><p>至于seq2seq的输出是不定长的，它是怎么在一次输出里面确定长度的，上图已经给出了几种做法：</p><ol>
<li>另做一个predictor来输出一个数字，表示应该输出的长度</li>
<li>直接用一个足够长的<bos>做输入（比如300个），那输出也就有300个，取到第一个<eos>为止</li>
</ol>
<p>因为不是一个一个生成了，好处</p><ol>
<li>可以平行运算。</li>
<li>输出的长度更可控</li>
</ol>
<blockquote>
<p>NAT通常表现不如AT好 (why? <strong>Multi-mmodality</strong>)</p></blockquote>
<p>detail: <a href="https://youtu.be/jvyKmU4OM3c">https://youtu.be/jvyKmU4OM3c</a> (Non-Autoregressive Sequence Generation)</p><h3>AT</h3>
<p>在decoder里最初有让人看不懂的三个箭头从encode的输出里指出来:</p><figure class="vertical-figure" style="flex: 41.08681245858184" ><img width="1240" height="1509" src="/archives/assets/04bb017a6b7821b1f8e1f676a4e89c13.png" alt=""/></figure><p>其实这就是<code>cross attention</code></p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="/archives/assets/943872fa064ed57098cf57694c851ace.png" alt=""/></figure><p>它就是把自己第一层(self-attention后)的输出乘一个$W^q$得到的<code>q</code>，去跟encoder的输出分别乘$W^k, W^v$得到的k和v运算($\sum q \times k \times v$)得到当前位置的输出的过程。</p><p>而且研究者也尝试过各种<code>cross attention</code>的方法，而不仅仅是本文中的无论哪一层都用<code>encoder</code>最后一层的输出做q和v这一种方案：</p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="/archives/assets/d1a0d2db75519d1eb4cd58bc9a0c65a5.png" alt=""/></figure><h2>Training Tips</h2>
<h3>复制机制</h3>
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="/archives/assets/637e1eace14b2504906690435792aa40.png" alt=""/></figure><p>一些场景，训练的时候没必要去“生成”阅读材料里提到的一些概念，只需要把它“复制”出来即可，比如上述的人名，专有名字，概念等，以及对文章做摘要等。</p><ul>
<li>Pointer Network: <a href="https://youtu.be/VdOyqNQ9aww">https://youtu.be/VdOyqNQ9aww</a></li>
<li>Copying Mechanism in Seq2Seq <a href="https://arxiv.org/abs/1603.06393">https://arxiv.org/abs/1603.06393</a></li>
</ul>
<h3>Guided Attention</h3>
<p>像语音这种连续性的，需要强制指定(guide)它的attention顺序，相对而言，文字跳跃感可以更大，语音一旦不连续就失去了可听性了，一些关键字：</p><ul>
<li>Monotonic Attention</li>
<li>Location-aware attention</li>
</ul>
<h3>Beam Search</h3>
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="/archives/assets/0068ee933dbbe3ab1f2fdc59431bc72b.png" alt=""/></figure><h3>Optimizing Evaluation Metrics / BLEU</h3>
<ul>
<li><p>训练的时候loss用的是cross entropy，要求loss越小越好，</p></li>
<li><p>而在evaluation的时候，我们用的是预测值与真值的<code>BLEU score</code>，要求score越大越好</p></li>
<li><p>那么越小的cross entropy loss真的能产生越高的BLEU score吗？ 未必</p></li>
<li><p>那么能不能在训练的时候也用BLEU score呢？ 不行，它太复杂没法微分，就没法bp做梯度了。</p></li>
</ul>
<h3>Exposure bias</h3>
<p>训练时候应用了<code>Teaching force</code>，用了全部或部分真值当作预测结果来训练（或防止一错到底），而eval的时候确实就是一错到底的模式了。</p><h1>Self-supervised Learning</h1>
<ul>
<li>芝麻街家庭：elmo, bert, erine...</li>
<li>bert就是transformer的encoder</li>
</ul>
<h2>Bert</h2>
<h3>GLUE</h3>
<p>GLUE: General Language Understanding Evaluation</p><p>基本上就是看以下这九个模型的得分：</p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="/archives/assets/bc43a4b9f8009e33413a2f5917c37fa2.png" alt=""/></figure><p>训练：</p><ol>
<li>预测mask掉的词(masked token prediction)<ul>
<li>为训练数据集添加部分掩码，预测可能的输出</li>
<li>类似word2vec的C-Bow</li>
</ul>
</li>
<li>预测下一个句子（分类，比如是否相关）(next sentence prediction)<ul>
<li>在句首添加<cls>用来接分类结果</li>
<li>用<sep>来表示句子分隔</li>
</ul>
</li>
</ol>
<p>下游任务（Downstream Task） &lt;- Fine Tune:</p><ol>
<li>sequence -&gt; class: sentiment analysis<ul>
<li>这是需要有label的</li>
<li><cls>节点对的linear部分是随机初始化</li>
<li>bert部分是pre-train的</li>
</ul>
</li>
<li>sequence -&gt; sequence(等长): POS tagging</li>
<li>2 sequences -&gt; class: NLI(从句子A能否推出句子B)(Natural Language Inferencee)<ul>
<li>也比如文章下面的留言的立场分析</li>
<li>用<cls>输出分类结果，用<sep>分隔句子</li>
</ul>
</li>
<li>Extraction-based Question Answering: 基于已有文本的问答系统<ul>
<li>答案一定是出现在文章里面的</li>
<li>输入文章和问题的向量</li>
<li>输出两个数字(start, end)，表示答案在文章中的索引</li>
</ul>
</li>
</ol>
<p>QA输出：</p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="/archives/assets/af1aa0bae691c8837e101c1a4d97c30f.png" alt=""/></figure><p>思路：</p><ol>
<li>用<cls>input<sep>document 的格式把输入摆好</li>
<li>用pre-trained的bert模型输出同样个数的向量</li>
<li>准备两个与bert模型等长的向量（比如768维）a, b（random initialized)</li>
<li>a与document的每个向量相乘(inner product)</li>
<li>softmax后，找到最大值，对应的位置(argmax)即为start index</li>
<li>同样的事b再做一遍，得到end index</li>
</ol>
<figure  style="flex: 131.07822410147992" ><img width="1240" height="473" src="/archives/assets/5fb69b6f706227e3cc81eaa740c28606.png" alt=""/></figure><h3>Bert train seq2seq</h3>
<p>也是可能的。就是你把输入“弄坏”，比如去掉一些字词，打乱词序，倒转，替换等任意方式，让一个decoder把它还原。 -&gt; <strong>BART</strong></p><h3>附加知识</h3>
<p>有研究人员用bert去分类DNA，蛋白质，音乐。以DNA为例，元素为A,C,G,T,分别对应4个随机词汇，再用bert去分类（用一个英文的pre-trained model），同样的例子用在了蛋白质和音乐上，居然发现效果全部要好于“纯随机”。</p><p>如果之前的实验说明了bert看懂了我们的文章，那么这个荒诞的实验（用完全无关的随意的英文单词代替另一学科里面的类别）似乎证明了事情没有那么简单。</p><h3>More</h3>
<ol>
<li><a href="https://youtu.be/1_gRK9EIQpc">https://youtu.be/1_gRK9EIQpc</a></li>
<li><a href="https://youtu.be/Bywo7m6ySlk">https://youtu.be/Bywo7m6ySlk</a></li>
</ol>
<h2>Multi-lingual Bert</h2>
<p>略</p><h2>GPT-3</h2>
<p>训练是predict next token...so it can do generation(能做生成)</p><blockquote>
<p>Language Model 都能做generation</p></blockquote>
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="/archives/assets/a4687a3585d0e42cada14edb4d3d0890.png" alt=""/></figure><p><a href="https://youtu.be/DOG1L9lvsDY">https://youtu.be/DOG1L9lvsDY</a></p><p>别的模型是pre-train后，再fine-tune， GPT-3是想实现zero-shot，</p><h3>Image</h3>
<p><strong>SimCLR</strong></p><ul>
<li><a href="https://arxiv.org/abs/2002.05709">https://arxiv.org/abs/2002.05709</a></li>
<li><a href="https://github.com/google-research/simclr">https://github.com/google-research/simclr</a></li>
</ul>
<p><strong>BYOL</strong></p><ul>
<li><strong>B</strong>ootstrap <strong>y</strong>our <strong>o</strong>own <strong>l</strong>atent</li>
<li><a href="https://arxiv.org/abs/2006.07733">https://arxiv.org/abs/2006.07733</a></li>
</ul>
<h3>Speech</h3>
<p>在bert上有九个任务(GLUE)来差别效果好不好，在speech领域还缺乏这样的数据库。</p><h2>Auto Encoder</h2>
<p>也是一种<code>self-supervised</code> Learning Framework -&gt; 也叫 pre-train, 回顾：
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="/archives/assets/6d2689336625e12011479fa23633aa94.png" alt=""/></figure></p><p>在这个之前，其实有个更古老的任务，它就是<code>Auto Encoder</code></p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="/archives/assets/b5667c49d1673c62d5fdbb794fd32053.png" alt=""/></figure><ul>
<li>用图像为例，通过一个网络encode成一个向量后，再通过一个网络解码(reconstrucion)回这张图像（哪怕有信息缺失）</li>
<li>中间生成的那个向量可以理解为对原图进行的压缩</li>
<li>或者说一种降维</li>
</ul>
<p>降维的课程：</p><ul>
<li>PCA: <a href="https://youtu.be/iwh5o_M4BNU">https://youtu.be/iwh5o_M4BNU</a></li>
<li>t-SNE: <a href="https://youtu.be/GBUEjkpoxXc">https://youtu.be/GBUEjkpoxXc</a></li>
</ul>
<p>有一个de-noising的Auto-encoder, 给入的是加了噪音的数据，经过encode-decode之后还原的是没有加噪音的数据</p><p>这就像加了噪音去训练bert</p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="/archives/assets/db0ed507503f41304ead7808d285a7f8.png" alt=""/></figure><h3>Feature Disentangle</h3>
<p>去解释auto-encoder压成的向量就叫<code>Feature Disentagle</code>，比如一段音频，哪些是内容，哪些是人物；一段文字，哪些表示语义，哪些是语法；一张图片，哪些表示物体，哪些表示纹理，等。</p><p>应用： voice conversion -&gt; 变声器</p><p>传统的做法应该是每一个语句，都有两种语音的资料，N种语言/语音的话，就需要N份。有Feature Disentangle的话，只要有两种语音的encoder，就能知道哪些是语音特征，哪些是内容特征，拼起来，就能用A的语音去读B的内容。所以<strong>前提</strong>就是能分析压缩出来的向量。</p><h3>Discrete Latent Representation</h3>
<p>如果压缩成的向量不是实数，而是一个binary或one-hot</p><ul>
<li>binary: 每一个维度几乎都有它的含义，我们只需要看它是0还是1</li>
<li>one-hot: 直接变分类了。-&gt; <code>unsupervised classification</code></li>
</ul>
<p><strong>VQVAE</strong></p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="/archives/assets/7d4061daabfdfcf415de004a69a1636b.png" alt=""/></figure><ul>
<li>Vector Quantized Variational Auot-encoder <a href="https://arxiv.org/abs/1711.00937">https://arxiv.org/abs/1711.00937</a></li>
</ul>
<h3>Text as Representation</h3>
<ul>
<li><a href="https://arxiv.org/abs/1810.02851">https://arxiv.org/abs/1810.02851</a></li>
</ul>
<p>如果压缩成的不是一个向量，而也是一段<code>word sequence</code>，那么是不是就成了<code>summary</code>的任务？ 只要encoder和decoder都是seq2seq的model</p><p>-&gt; seq2seq2seq auto-encoder -&gt; <code>unsupervised summarization</code></p><p>事实上训练的时候encoder和decoder可能产生强关联，这个时候就引入一个额外的<code>discriminator</code>来作判别:
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="/archives/assets/e470d48770f1cee066eca5b76e49e983.png" alt=""/></figure></p><p>有点像cycle GAN，一个generator接一个discriminator，再接另一个generator</p><h3>abnormal detection</h3>
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="/archives/assets/d8a6da742b367ace4528d833cb8623fa.png" alt=""/></figure><ul>
<li>Part 1: <a href="https://youtu.be/gDp2LXGnVLQ">https://youtu.be/gDp2LXGnVLQ</a></li>
<li>Part 2: <a href="https://youtu.be/cYrNjLxkoXs">https://youtu.be/cYrNjLxkoXs</a></li>
<li>Part 3: <a href="https://youtu.be/ueDlm2FkCnw">https://youtu.be/ueDlm2FkCnw</a></li>
<li>Part 4: <a href="https://youtu.be/XwkHOUPbc0Q">https://youtu.be/XwkHOUPbc0Q</a></li>
<li>Part 5: <a href="https://youtu.be/Fh1xFBktRLQ">https://youtu.be/Fh1xFBktRLQ</a></li>
<li>Part 6: <a href="https://youtu.be/LmFWzmn2rFY">https://youtu.be/LmFWzmn2rFY</a></li>
<li>Part 7: <a href="https://youtu.be/6W8FqUGYyDo">https://youtu.be/6W8FqUGYyDo</a></li>
</ul>

            </div>
        </article>
        <div class="prism-post-meta col-md-8 offset-md-2">
    <span>walker</span>
    
    <span>/</span>
    <span>
        <a class="category no-link" href="/category/posts/" target="_self">
        posts
        </a>
    </span>
    
    
    <span>/</span>
    
    <span class="prism-tag">
        <a class="no-link" href="/tag/%E6%9D%8E%E5%AE%8F%E6%AF%85/" target="_self">#李宏毅</a>
    </span>
    
    <span class="prism-tag">
        <a class="no-link" href="/tag/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" target="_self">#机器学习</a>
    </span>
    
    <span class="prism-tag">
        <a class="no-link" href="/tag/CNN/" target="_self">#CNN</a>
    </span>
    
    <span class="prism-tag">
        <a class="no-link" href="/tag/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_self">#卷积神经网络</a>
    </span>
    
    <span class="prism-tag">
        <a class="no-link" href="/tag/GPT/" target="_self">#GPT</a>
    </span>
    
    <span class="prism-tag">
        <a class="no-link" href="/tag/seq2seq/" target="_self">#seq2seq</a>
    </span>
    
    <span class="prism-tag">
        <a class="no-link" href="/tag/GLUE/" target="_self">#GLUE</a>
    </span>
    
    <span class="prism-tag">
        <a class="no-link" href="/tag/Bert/" target="_self">#Bert</a>
    </span>
    
    <span class="prism-tag">
        <a class="no-link" href="/tag/Transformer/" target="_self">#Transformer</a>
    </span>
    
    <span class="prism-tag">
        <a class="no-link" href="/tag/AlphaGo/" target="_self">#AlphaGo</a>
    </span>
    
    <span class="prism-tag">
        <a class="no-link" href="/tag/self%20attention/" target="_self">#self attention</a>
    </span>
    
    <span class="prism-tag">
        <a class="no-link" href="/tag/self%20supervised%20learning/" target="_self">#self supervised learning</a>
    </span>
    
    
    
    <span>/</span>
    <span class="leancloud_visitors" id="/archives/%E6%9D%8E%E5%AE%8F%E6%AF%85Machine-Learning-2021-Spring-3/" data-flag-title="李宏毅Machine Learning 2021 Spring笔记[3]"><span class="leancloud-visitors-count"></span> Views</span>
    
</div>
    </section>

    
<section id="prism__pagination" class="prism-pagination" class="col-md-8 offset-md-2">
    <ul>
        
        <li class="next">
            <a class="no-link" href="/archives/%E6%9D%8E%E5%AE%8F%E6%AF%85Machine-Learning-2021-Spring-4/" target="_self" title="李宏毅Machine Learning 2021 Spring笔记[4]"><i class="fa fa-chevron-left" aria-hidden="true"></i>Newer</a>
        </li>
        
        
        <li class="prev">
            <a class="no-link" href="/archives/%E6%9D%8E%E5%AE%8F%E6%AF%85Machine-Learning-2021-Spring-2/" target="_self" title="李宏毅Machine Learning 2021 Spring笔记[2]">Older<i class="fa fa-chevron-right" aria-hidden="true"></i></a>
        </li>
        
    </ul>
</section>


    
    <script>
        var initValine = function() {
            new Valine({"enable": true, "el": "#vcomments", "appId": "7tP92LoqK2cggW61DvJmWBo0-gzGzoHsz", "appKey": "iQCtrtlr8eKrQllM03GMESMJ", "visitor": true, "recordIP": true});
        }

    </script>
    <script defer src='https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js' onload="initValine()"></script>
    <div class="prism-comment-section container" id="prism__comment">
        <div class="row">
            <div class="col-md-8 offset-md-2">
                <div id="vcomments"></div>
            </div>
        </div>
    </div>
    

</main>

            <footer id="prism__footer">
                <section>
                    <div>
                        <nav class="social-links">
                            <ul><li><a class="no-link" title="Twitter" href="https://twitter.com/walkerwzy" target="_blank" rel="noopener noreferrer nofollow"><i class="gi gi-twitter"></i></a></li><li><a class="no-link" title="GitHub" href="https://github.com/walkerwzy" target="_blank" rel="noopener noreferrer nofollow"><i class="gi gi-github"></i></a></li><li><a class="no-link" title="Weibo" href="https://weibo.com/1071696872" target="_blank" rel="noopener noreferrer nofollow"><i class="gi gi-weibo"></i></a></li></ul>
                        </nav>
                    </div>

                    <section id="prism__external_links">
                        <ul>
                            
                            <li>
                                <a class="no-link" target="_blank" href="https://github.com/AlanDecode/Maverick" rel="noopener noreferrer nofollow">Maverick</a>：🏄‍ Go My Own Way.
                                <span>|</span>
                            </li>
                            
                            <li>
                                <a class="no-link" target="_blank" href="https://www.imalan.cn" rel="noopener noreferrer nofollow">Triple NULL</a>：Home page for AlanDecode.
                                <span>|</span>
                            </li>
                            
                        </ul>
                    </section>

                    <div class="copyright">
                        <p class="copyright-text">
                            <span class="brand">walker's code blog</span>
                            <span>Copyright © 2022 walker</span>
                        </p>
                        <p class="copyright-text powered-by">
                            | Powered by <a href="https://github.com/AlanDecode/Maverick" class="no-link" target="_blank" rel="noopener noreferrer nofollow">Maverick</a> | Theme <a href="https://github.com/Reedo0910/Maverick-Theme-Prism" target="_blank" class="no-link" rel="noopener noreferrer nofollow">Prism</a>
                        </p>
                    </div>
                    <div class="footer-addon">
                        
                    </div>
                </section>
                <script>
                    var site_build_date = "2019-12-06T12:00+08:00"

                </script>
                <script src="/assets/prism-efa8685153.js"></script>
            </footer>
        </div>
    </div>
    </div>

    <script src="/assets/ExSearch/jquery.min.js"></script>
    <script src="/assets/ExSearch/ExSearch-493cb9cd89.js"></script>

    <!--katex-->
    <link rel="stylesheet" href="/assets/katex.min.css">
    <script defer src="/assets/katex.min.js"></script>
    <script>
        mathOpts = {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "\\[", right: "\\]", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false }
            ]
        };

    </script>
    <script defer src="/assets/auto-render.min.js" onload="renderMathInElement(document.body, mathOpts);"></script>

    
</body>

</html>