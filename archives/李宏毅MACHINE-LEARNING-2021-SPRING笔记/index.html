<!DOCTYPE HTML>
<html lang="english">
    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="renderer" content="webkit">
    <meta name="HandheldFriendly" content="true">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Maverick,AlanDecode,Galileo,blog" />
    <meta name="generator" content="Maverick 1.2.1" />
    <meta name="template" content="Galileo" />
    <link rel="alternate" type="application/rss+xml" title="Maverick &raquo; RSS 2.0" href="/feed/index.xml" />
    <link rel="alternate" type="application/atom+xml" title="Maverick &raquo; ATOM 1.0" href="/feed/atom/index.xml" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/galileo-8d8763e752.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/ExSearch/ExSearch-182e5a8868.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/katex.min.css">
    <link href="https://fonts.googleapis.com/css?family=Fira+Code&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700&display=swap">
    <script>
        var ExSearchConfig = {
            root: "",
            api: "https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/3ba785d75f2fc02bf53acd6824476cf0.json"
        }
    </script>
    
<title>李宏毅MACHINE-LEARNING-2021-SPRING笔记 - Maverick</title>
<meta name="author" content="walker" />
<meta name="description" content="" />
<meta property="og:title" content="李宏毅MACHINE-LEARNING-2021-SPRING笔记 - Maverick" />
<meta property="og:description" content="" />
<meta property="og:site_name" content="Maverick" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/archives/%E6%9D%8E%E5%AE%8F%E6%AF%85MACHINE-LEARNING-2021-SPRING%E7%AC%94%E8%AE%B0/" />
<meta property="og:image" content="" />
<meta property="article:published_time" content="2021-10-03T00:00:00-00.00" />
<meta name="twitter:title" content="李宏毅MACHINE-LEARNING-2021-SPRING笔记 - Maverick" />
<meta name="twitter:description" content="" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:image" content="" />


    
    </head>
    
    <body>
        
        <div class="container">
            <header id="ga-header">
                <div first>
                    <aside id="ga-brand">
                        <h1 class="brand"><a class="no-style" href="/">Maverick</a></h1>
                        <p>This is Maverick, Theme Galileo.</p>
                    </aside>
                </div>
                <div second id="ga-nav">
                    <nav class="navs">
                        <ul><li><a class="ga-highlight" href="/" target="_self">Home</a></li><span class="separator">·</span><li><a class="ga-highlight" href="/archives/" target="_self">Archives</a></li><span class="separator">·</span><li><a class="ga-highlight" href="/about/" target="_self">About</a></li><span class="separator">·</span><li><a href="#" target="_self" class="search-form-input ga-highlight">Search</a></li></ul>
                    </nav>
                </div>
            </header>
            <div class="wrapper">
                
<main>    
    <section class="ga-section ga-content">
        <article class="yue">
            <h1 class="ga-post_title">李宏毅MACHINE-LEARNING-2021-SPRING笔记</h1>
            <span class="ga-post_meta ga-mono">
                <span>walker</span>
                <time>
                    2021-10-03
                </time>
                
                in <a no-style class="category" href="/category/AI/">
                    AI
                </a>
                
                
                <span class="leancloud_visitors" 
                    id="/archives/%E6%9D%8E%E5%AE%8F%E6%AF%85MACHINE-LEARNING-2021-SPRING%E7%AC%94%E8%AE%B0/" 
                    data-flag-title="李宏毅MACHINE-LEARNING-2021-SPRING笔记"> · <i class="leancloud-visitors-count"></i> Views</span>
                
            </span>
            <div class="ga-content_body">
                <blockquote>
<p>纯听课时一些思路和笔记，没有教程作用。
这个课程后面就比较水了，大量的全是介绍性的东西，也罗列了大量的既往课程和论文，如果你在工作过研究中碰到了它提过的场景或问题，倒是可以把它作索引用。</p></blockquote>
<h1>Linear Model</h1>
<h2>Piecewise Linear</h2>
<p>线性模型永远只有一条直线，那么对于折线（曲线），能怎样更好地建模呢？这里考虑一种方法，</p><ol>
<li>用一个<code>常数</code>加一个<code>整流函数</code><ul>
<li>即左右两个阈值外y值不随x值变化，阈值内才是线性变化（天生就拥有两个折角）。</li>
</ul>
</li>
<li>每一个转折点加一个新的整流函数</li>
</ol>
<p>如下：
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-8fa16cf5a344e174.png" /></figure></p><p>如果是曲线，也可以近似地理解为数个折线构成的（取决于近似的精度），而蓝色的整流函数不好表示，事实上有sigmoid函数与它非常接近（它是曲线），所以蓝线又可以叫：<code>Hard Sigmoid</code></p><p>所以，最终成了一个常数(<code>bias</code>)和数个<code>sigmoid</code>函数来逼近真实的曲线。同时，每一个转折点<code>i</code>上s函数的具体形状（比如有多斜多高），就由一个新的线性变换来控制：$b_i + w_ix_n$，把<code>i</code>上<strong>累积的线性变换</strong>累加，就得到与$x_n$最可能逼近的曲线。</p><p>下图演示了3个转折点的情况：</p><figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-9fd947d54a144ccf.png" /></figure><p>至此，一个简单的对b,w依赖的函数变成了对（$w_i, b_i, c_i$)和, x, b的依赖，即多了很多变量。</p><ul>
<li>$y = b + wx_1$</li>
<li>$y = b + \sum_i c_i sigmoid(b_i + w_i x_{\color{red} 1})$</li>
</ul>
<p>注意这个$x_1$，即只转了一个x就要堆一个<code>sum</code>，而目前也只是演示了只有一个特征的情况。</p><p>如果更复杂一点的模型，每次不是看一个x，而看n个x，（比如利用前7天的观看数据来预测第8天的，那么建模的时候就是每一个数都要与前7天的数据建立w和b的关系）：</p><blockquote>
<p>其实就是由一个feature变成了n个feature了，一般的教材会用不同的feature来讲解（比如影响房价的除了时间，还有面积，地段等等），而这里只是增加了天数，可能会让人没有立刻弄清楚两者其实是同一个东西。其实就是x1, x2, x3...不管它们对应的是同一<strong>类</strong>特征，而是完全不同的多个<strong>角度</strong>的特征。</p></blockquote>
<p>现在就有一堆$wx$了</p><ul>
<li>$y = b + \sum_j w_j x_j$</li>
<li>现在就变成了(注意，其实就是把加号右边完整代入）：</li>
<li>$y = b + \sum_i c_i sigmoid(b_i + \color{red}{\sum_j w_{ij} x_j})$</li>
</ul>
<p>展开计算，再根据特征，又可以看回矩阵了（而不是从矩阵出发来思考）：</p><figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-8f33e9949d1295aa.png" /></figure><p>矩阵运算结果为(r)，再sigmoid后，设结果为a:</p><ul>
<li>$a_i = c_i \sigma(r_i)$</li>
<li>$y = b + \sum_i a_i$ c 和 a要乘加，仍然可以矩阵化（其实是向量化）：</li>
<li>$y = b + c^T a$， 把上面的展开回去：</li>
<li>$y = b + c^T \sigma(\bold b + W x)$<ul>
<li>前后两个b是不同的，一个是数值，一个是向量</li>
</ul>
</li>
</ul>
<p>这里，我们把目前所有的“未知数”全部拉平拼成了一个向量 $\theta$：
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-c9eb59a590e5ae82.png" /></figure></p><p>这里，如果把$c^T$写成<code>W'</code>你会发现，我们已经推导出了一个2层的神经网络：一个隐层，一个输出层：</p><ul>
<li>b+wx 是第一层 得到<code>a</code></li>
<li>对<code>a</code>进行一次sigmoid（别的教材里会说是激活）得到<code>a'</code></li>
<li>把<code>a'</code>当作输入，再进行一次 b+wx (这就是隐层了)</li>
<li>得到的输出就是网络的输出<code>o</code></li>
</ul>
<blockquote>
<p>这里在用另一个角度来尝试解释神经网络，激活函数等，但要注意，sigmoid的引入原本是去”对着折线描“的，也就是说是人为选择的，而这里仍然变成了机器去”学习“，即没有告诉它哪些地方是转折点。也就是说有点陷入了用机器学习解释机器学习的情况。</p></blockquote>
<blockquote>
<p>但是如果是纯曲线，那么其实是可以无数个sigmoid来组合的，就不存在要去拟合某些“特定的点”，那样只要找到最合适“数量”的sigmoig就行了（因为任何一个点都可以算是折点）</p></blockquote>
<h2>Loss</h2>
<p>loss 没什么变化，仍旧是一堆$\theta$代入后求的值与y的差，求和。并期望找到使loss最小化的$\theta$：</p><p>$\bold \theta = arg\ \underset{\theta}{min}\ L$</p><h1>Optimization</h1>
<p>真实世界训练样本会很大，</p><ul>
<li>我们往往不会把整个所有数据直接算一次loss，来迭代梯度，</li>
<li>而是分成很多小份(mini-batch)每一小份计算一次loss（然后迭代梯度）</li>
<li>下一个小batch认前一次迭代的结果</li>
<li>也就是说，其实这是一个不严谨的迭代，用别人数据的结果来当成本轮数据的前提<ul>
<li>最准确的当然是所有数据计算梯度和迭代。</li>
<li>一定要找补的话，可以这么认为：<ul>
<li>即使一个小batch，也是可以训练到合理的参数的</li>
<li>所以前一个batch训练出来的数据，是一定程度上合理的</li>
<li>现在换了新的数据，但保持上一轮的参数，反而可以防止<code>过拟合</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-9d953fa0a5c68501.png" /></figure><p>minibatch还有一个极端就是batchsize=1，即每次看完一条数据就与真值做loss，这当然是可以的，而且它非常快。但是：</p><ol>
<li>小batch虽然快，但是它非常noisy（及每一笔数据都有可能是个例，没有其它数据来抵消它的影响）</li>
<li>因为有gpu平行运算的原因，只要不是batch非常大（比如10000以上），其实mini-batch并不慢</li>
<li>如果是小样本，mini-batch反而更快，因为它一来可以平行运算，在计算gradient的时候不比小batch慢，但是它比小batch要小几个数量级的update.</li>
</ol>
<p>仍然有个但是：实验证明小的batch size会有更高的准确率。
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-343cae0915fcaaa8.png" /></figure></p><p>两个local minimal，右边那个认为是不好的，因为它只要有一点偏差，与真值就会有巨大的差异。但是没懂为什么大的batch会更容易落在右边。</p><p>这是什么问题？其实是optimization的问题，后面会用一些方法来解决。</p><h2>Sigmoid -&gt; RelU</h2>
<p>前面我们用了soft的折线来模拟折线，其实还可以叠加两个真的折线(<code>ReLU</code>)，这才是我一直说的<code>整流函数</code>的名字的由来。</p><figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-4479e5576c2ed5a0.png" /></figure><p>仔细看图，c和c'在第二个转折的右边，一个是向无穷大变，一个是向无穷小变，只要找到合理的斜率，就能抵消掉两个趋势，变成一条直线。</p><p>如果要用ReLU，那么简单替换一下：</p><ul>
<li>$y = b + \sum_i {\color{ccdd00}{c_i}} sigmoid(\color{green}{b_i} + \sum_j \color{blue}{w_{ij}} x_j)$</li>
<li>$y = b + \sum_{\color{red}2i} {\color{ccdd00}{c_i}} \color{red}{max}(\color{red}0,\ \color{green}{b_i} + \sum_j \color{blue}{w_{ij}} x_j)$</li>
</ul>
<p>红色的即为改动的部分，也呼应了2个relu才构成一个sigmoid的铺垫。</p><p>把每一个a当成之前的x，我们可以继续套上新的w,b,c等，生成新的a-&gt;a'
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-ef692679760b967f.png" /></figure></p><figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-3a13832b5c1c6b04.png" /></figure><p>而如果再叠一层，在课程里的资料里，在训练集上loss仍然能下降（到0.1），但是在测试集里，loss反而上升了（0.44)，这意味着开始过拟合了。</p><p>这就是反向介绍神经元和神经网络。先介绍数学上的动机，组成网络后再告诉你这是什么，而不是一上来就给你扯什么是神经元什么是神经网络，再来解释每一个神经元干了什么。</p><p>而传统的神经网络课程里，sigmoid是在逻辑回归里才引入的，是为了把输出限定在1和0之间。显然这里的目的不是这样的，是为了用足够多的sigmoid或relu来逼近真实的曲线（折线）</p><h2>Framework of ML</h2>
<h3>通用步骤：</h3>
<ol>
<li>设定一个函数来描述问题$y = f_\theta(x)$, 其中$\theta$就是所有未知数（参数）</li>
<li>设定一个损失函数$L(\theta)$</li>
<li>求让损失函数尽可能小的$\theta^* = arg\ \underset{\theta}{\rm min}L(\theta)$</li>
</ol>
<h3>拟合不了的原因：</h3>
<ol>
<li>过大的loss通常“暗示”了模型不合适（<strong>model bias</strong>），比如上面的用前1天数据预测后一天，可以尝试改成前7天，前30天等。<ul>
<li>大海里捞针，针其实不在海里</li>
</ul>
</li>
<li>优化问题，梯度下降不到目标值<ul>
<li>针在大海里，我却没有办法把它找出来</li>
</ul>
</li>
</ol>
<h3>如何判断是loss optimization没做好？</h3>
<p>用不同模型来比较（更简单的，更浅的）
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-a47194566126e9d4.png" /></figure></p><p>上图中，为什么56层的表现还不如20层呢？是<code>overfitting</code>吗？<strong>不一定</strong>。</p><p>我们看一下在训练集里的表现，56层居然也不如20层，这合理吗？ <strong>不合理</strong></p><blockquote>
<p>但凡20层能做到的，多出的36层可以直接全部identity（即复制前一层的输出），也不可能比20层更差（神经网络总可以学到的）</p></blockquote>
<p>这时，就是你的loss optimization有问题了。</p><h3>如何解决overfitting</h3>
<ol>
<li>增加数据量<ul>
<li>增加数据量的绝对数量</li>
<li>data augmentation数据增强（比如反复随机从训练集里取，或者对图像进行旋转缩放位移和裁剪等）</li>
</ul>
</li>
<li>缩减模型弹性<ul>
<li>（低次啊，更少的参数「特征」啊）</li>
<li>更少的神经元，层数啊</li>
<li>考虑共用参数</li>
<li>early stopping</li>
<li>regularization<ul>
<li>让损失函数与每个特征系数直接挂勾，就变成了惩罚项</li>
<li>因为它的值越大，会让损失函数越大，这样可以“惩罚”过大的权重</li>
</ul>
</li>
<li>dropout<ul>
<li>随机丢弃一些计算结果</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2>Missmatch</h2>
<p>课上一个测试，预测2/26的观看人数（周五，历史数据都是观看量低），但因为公开了这个测试，引起很多人疯狂点击，结果造成了这一天的预测结果非常差。</p><p>这个不叫overfitting，而是<code>mismatch</code>，表示的是<strong>训练集和测试集的分布是不一样的</strong></p><p>mismatch的问题，再怎么增加数据也是不可能解决的。</p><h2>optimization problems</h2>
<p>到目前为止，有两个问题没有得到解决：</p><ol>
<li>loss optimization有问题怎么解决<ul>
<li>其实就是判断是不是saddle point（鞍点）</li>
</ul>
</li>
<li>mismatch怎么解决</li>
</ol>
<h3>saddle point</h3>
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-153bf93ab74dd8b4.png" /></figure><p>hessian矩阵是二次微分，当一次微分为0的时候，二次微分并不一定为0。这是题眼。</p><p>对于红杠内的部分，设$\theta - \theta^T = v$，有：</p><ul>
<li>for all v: $v^T H v &gt; 0 \rightarrow \theta'$附近的$\theta$都要更大<ul>
<li>-&gt; 确实是在<code>local minima</code></li>
</ul>
</li>
<li>for all v: $v^T H v &lt; 0 \rightarrow \theta'$附近的$\theta$都要更小<ul>
<li>-&gt; 确实是在<code>local maxima</code></li>
</ul>
</li>
<li>而时大时小，说明是在<code>saddle point</code></li>
</ul>
<p>事实上我们不可能去检查<code>所有的v</code>，这里用Hessian matrix来判断：</p><ul>
<li>$\rm H$ is <code>positive definite</code> $\rightarrow$ all eigen values are positive $\rightarrow$ local minimal</li>
<li>$\rm H$ is <code>negative definite</code> $\rightarrow$ all eigen values are negative $\rightarrow$ local maximal</li>
</ul>
<p>用一个很垃圾的网络举例，输入是1，输出是1，有w1, w2两层网络参数，因为函数简单，两次微分得到的hessian矩阵还是比较简单直观的：
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-43de51903c56d851.png" /></figure></p><p>由于特征值有正有负，我们判断在些(0, 0)这个<code>critical point</code>，它是一个<code>saddle point</code>.</p><p>如果你判断出当前的参数确实卡在了鞍点，它同时也指明了<code>update direction</code>!</p><figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-162e0908ac2ce4d4.png" /></figure><p>图中，</p><ol>
<li>先构建出了一个小于0的结果，以便找到可以让$L(\theta)$收敛的目标</li>
<li>这个结果依赖于找到这样一个u<ul>
<li>这个u是$\theta, \theta'$相减的结果</li>
<li>它还是$H$的<code>eigen vector</code></li>
<li>它的<code>eigen value</code>$\rightarrow \lambda$ 还要小于0</li>
</ul>
</li>
</ol>
<p>实际上，<code>eigen value</code>是可以直接求出来的（上例已经求出来了），由它可以推出<code>eigen vector</code>，比如[1, 1]$^T$（自行补相关课程），往往会一对多，应该都是合理的，我们顺着共中一个u去更新$\theta$，就可以继续收敛loss。</p><blockquote>
<p>实际不会真的去计算hessian matrix?</p></blockquote>
<h3>Momentum</h3>
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-eb2390a6beff8f1d.png" /></figure><p>不管是较为平坦的面，还是saddle point，如果小球以图示的方式滚下去，真实的物理世界是不可能停留在那个gradient为0或接近于0的位置的，因为它有“动量”，即惯性，甚至还可能滚过local minima，这恰好是我们需要的特性。
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-5c2123a5abb30e13.png" /></figure>
不但考虑当前梯度，还考虑之前累积的值（动量），这个之前，是之前所有的动量，而不是前一步的：
$
\begin{aligned}
m^0 &amp;= 0 \
m^1 &amp;= -\eta g^0 \
m^2 &amp;= -\lambda \eta g^0 - \eta g^1 \
&amp;\vdots
\end{aligned}
$</p><h3>adaptive learning rate</h3>
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-3036d6027b02b243.png" /></figure><p>不是什么时候loss卡住了就说明到了极点(最小值，鞍点，平坦的点)</p><p>看下面这个error surface，两个参数，一个变动非常平缓，一个非常剧烈，如果应用相同的<code>learning rate</code>，要么反复横跳（过大），要么就再也挪不动步（太小）：</p><figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-a4ad370fd272ab52.png" /></figure><h3>Adagrad (Root Mean Square)</h3>
<p>于是有了下面的优化方法，思路与<code>l2正则化</code>差不多，利用不同参数本身gradient的大小来“惩罚”它起到的作用。</p><ol>
<li>这里用的是相除，因为我的梯度越小，步伐就可以跨得更大了。</li>
<li>并且采用的是梯度的平方和(<code>Root Mean Square</code>)</li>
</ol>
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-ee61f47985824c5a.png" /></figure><figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-2a5770ec8edaeaf6.png" /></figure><p>图中可以看出平缓的$\theta_1$就可以应用大的学习率，反之亦然。这个方法就是<code>Adagrad</code>的由来。不同的参数用不同的步伐来迭代，这是一种思路。</p><p>这就解决问题了吗？看下面这个新月形的error surface，不卖关子了，这个以前接触的更多，即梯度随时间的变化而不同，</p><figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-9d0ffe171786a907.png" /></figure><h3>RMSProp</h3>
<p>这个方法是找不到论文的。核心思想是在<code>Adagrad</code>做平方和的时候，给了一个$\alpha$作为当前这个梯度的权重(0,1)，而把前面产生的$\sigma$直接应用$(1-\alpha)$：</p><ul>
<li>$\theta_i^{t+1} \leftarrow \theta_i^t - \frac{\eta}{\color{red}{\sigma_i^t}} g_i^t$</li>
<li>$\sigma_i^t = \sqrt{\alpha(\theta_i^{t-1})^2 + (1-\alpha)(g_i^t)^2}$</li>
</ul>
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-e5ecd2f7cb3fcb27.png" /></figure><h3>Adam: (RMSProp + Momentum)</h3>
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-82a0f7e2d48fadb7.png" /></figure><h3>Learning Rate Scheduling</h3>
<p>终于来到了最直观的lr scheduling部分，也是最容易理解的，随着时间的变化（如果你拟合有效的话），越接近local minima，lr越小。</p><p>而RMSProp一节里说的lr随时间变化并不是这一节里的随时间变化，而是设定一个权重，始终让<strong>当前</strong>的梯度拥有最高权重，注重的是当前与过往，而schedule则考量的是有计划的减小。</p><p>下图中，应用了adam优化后，由于长久以来横向移动累积的小梯度会突然爆发，形成了图中的局面，应用了scheduling后，人为在越靠近极值学习率越低，很明显直接就解决了这个问题。
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-61681993d7933f62.png" /></figure></p><p>而<code>warm up</code>没有在原理或直观上讲解更多，了解一下吧，实操上是很可行的，很多知名的网络都用了它：</p><figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-59bca02513b04524.png" /></figure><p>要强行解释的话，就是adam的$\theta$是一个基于统计的结果，所以要在看了足够多的数据之后才有意义，因此采用了一开始小步伐再增加到大步伐这样一个过度，拿到足够的数据之后，才开始一个正常的不断减小的schedule的过程。</p><p>更多可参考：<code>RAdam</code>: <a href="https://arxiv.org/abs/1908.03265">https://arxiv.org/abs/1908.03265</a></p><h3>Summary of Optimization</h3>
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-527578692542295f.png" /></figure><p>回顾下<code>Momentum</code>，它就是不但考虑当前的梯度，还考虑之前所有的梯度（加起来），通过数学计算，当然是能算出它的”动量“的。</p><p>那么同样是累计过往的梯度，一个在分母（$\theta$)，一个在分子（momentum)，那不是抵消了吗？</p><ol>
<li>momentum是相加，保留了方向</li>
<li>$\sigma$是平方和，只保留了大小</li>
</ol>
<h2>Batch Normalization</h2>
<p>沿着cost surface找到最低点有一个思路，就是能不能把山“铲平”？即把地貌由崎岖变得平滑点？ <code>batch normalization</code>就是其中一种把山铲平的方法。
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-2b4da7bf4fe85b71.png" /></figure></p><p>其实就是人为控制了error的范围，让它在各个feature上面的“数量级”基本一致（均值0，方差1），这样产生的error surface不会出现某参数影响相当小，某些影响又相当大，而纯粹是因为input本身量级不同的原因（比如房价动以百万计，而年份是一年一年增的）</p><p>error surface可以想象成每一个特征拥有一个轴（课程用二到三维演示），BN让每条轴上的ticks拥有差不多的度量。</p><p>然后，你把它丢到深层网络里去，你的输出的分布又是不可控的，要接下一个网络的话，你的输出又成了下一个网络的输入。虽然你在输出前nomalization过了，但是可能被极大和极小的权重w又给变了了数量级不同的输出</p><p>再然后，不像第一层，输入的数据来自于训练资料，下一层的输入是要在上一层的输出进行sigmoid之后的</p><p>再然后，你去看看sigmoid函数的形状，它在大于一定值或小于一定值之后，对x的变化是非常不敏感了，这样非常容易了出现梯度消失的现象。</p><p>于是，出于以下两个原因，我们都会考虑在输出后也接一次batch normalization::</p><ol>
<li>归一化（$\mu=0, \delta=1$)</li>
<li>把输入压缩到一个（sigmoid梯度较大的）小区间内</li>
</ol>
<p>照这个思路，我们是需要在sigmoid之前进行一次BN的，而有的教材会告诉你之前之后做都没关系，那么之后去做就丧失了以上第二条的好处。</p><p><strong>副作用</strong></p><ul>
<li>以前$x_1 \rightarrow z_1 \rightarrow a_1$</li>
<li>现在$\tilde z_1$是用所有$z_i$算出来的，不再是独立的了</li>
</ul>
<p><strong>后记1</strong></p><p>最后，实际还会把$\tilde z_i$再这么处理一次：</p><ul>
<li>$\hat z_i = \gamma \odot \tilde z_i + \beta$</li>
</ul>
<p>不要担心又把量级和偏移都做回去了，会以1和0为初始值慢慢learn的。</p><p><strong>后记2</strong></p><p>推理的时候，如果batch size不够，甚至只有一条时，怎么去算$\mu, \sigma$呢？</p><p>pytorch在训练的时候会计算<code>moving average</code>of $\mu$ and $\sigma$ of the batches.(每次把当前批次的均值和历史均值来计算一个新的历史均值$\bar \mu$)</p><ul>
<li>$\bar \mu \leftarrow p \bar \mu + (1-p)\mu_t$</li>
</ul>
<p>推理的时候用$\bar \mu, \bar \sigma$。</p><p>最后，用了BN，平滑了error surface，学习率就可以设大一点了，加速收敛。</p><h1>Classification</h1>
<p>用数字来表示class，就会存在认为1跟2比较近与3比较远的可能（从数学运算来看也确实是的，毕竟神经网络就是不断地乘加和与真值减做对比），所以引入了one-hot，它的特征就是class之间无关联。</p><p>恰恰是这个特性，使得用one-hot来表示词向量的时候成了一个要克服的缺点。预测单词确实是一个分类问题，然后词与词之间却并不是无关的，恰恰是有距离远近的概念的，而把它还原回数字也解决不了问题，因为单个数字与前后的数字确实近了，但是空间上还是可以和很多数字接近的，所以向量还是必要的，于是又继续打补丁，才有了稠密矩阵embedding的诞生。</p><h2>softmax</h2>
<p>softmax的一个简单的解释就是你的真值是0和1的组合(one-hot)，但你的预测值可以是任何数，因为你需要把它normalize到(0,1)的区间。</p><p>当class只有两个时，用softmax和用sigmoid是一样的。</p><h2>loss</h2>
<p>可以继续用MeanSquare Error(MSE) $ e = \sum_i(\hat y_i - y'_i)^2$，但更常用的是：</p><h3>Cross-entropy</h3>
<p>$e = - \sum_i \hat y_i lny'_i$</p><blockquote>
<p><code>Minimizing cross-entropy</code> is equivalent to <code>maximizing likelihood</code></p></blockquote>
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-011803fa18ea1c80.png" /></figure><p>linear regression是想从真值与预测值的差来入手找到最合适的参数，而logistic regression是想找到一个符合真值分布的的预测分布。</p><p>在吴恩达的课程里，这个损失函数是”找出来的“：</p><figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-e5bfbd4de3527f60.png" /></figure><ol>
<li>首先，$\theta x$后的值可以是任意值，所以再sigmoid一下，以下记为hx</li>
<li>hx的意思就是<code>y为1的概率</code></li>
<li>我需要一个损失函数，希望当真值是0时，预测y为1的概率的误差应该为无穷大<ul>
<li>也就是说hx=0时，损失函数的结果应该是无穷大</li>
<li>而hx=1时, 损失应该为0</li>
</ul>
</li>
<li>同理，当y为1时，hx=0时损失应该是无穷大，hx=1时损失为0</li>
<li>这时候才告诉你，log函数<strong>刚好长这样</strong>，请回看上面的两张图</li>
</ol>
<p>而别的地方是告诉你log是为了把概率连乘变成连加，方便计算。李宏毅这里干脆就直接告诉你公式长这样了。。。</p><p>这里绕两个弯就好了：</p><ol>
<li>y=1时，预测y为1的概率为1， y=0时，应预测y=1的概率为0</li>
<li>而这里是做损失函数，所以预测对了损失为0，错了损失无穷大</li>
<li>预测为1的概率就是hx，横轴也是hx</li>
</ol>
<blockquote>
<p>课程里说softmax和cross entorpy紧密到pytorch里直接就把两者结合到一起了，应用cross entropy的时候把softmax加到了你的network的最后一层（也就是说你没必要手写）。这里说的只是pytorch是这么处理的吗？</p><p>----是的</p></blockquote>
<h3>CE v.s. MSE</h3>
<p>数学证明：<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/MLDS_2015_2/Lecture/Deep%20More%20(v2).ecm.mp4/index.html">http://speech.ee.ntu.edu.tw/~tlkagk/courses/MLDS_2015_2/Lecture/Deep%20More%20(v2).ecm.mp4/index.html</a></p><figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-29ba3177a9772077.png" /></figure><p>单看实验结果，初始位置同为loss较大的左上角，因为CE有明显的梯度，很容易找到右下角的极值，但是MSE即使loss巨大，但是却没有梯度。因此对于逻辑回归，选择交叉熵从实验来看是合理的，数学推导请看上面的链接。</p><h1>CNN</h1>
<ol>
<li><strong>Receptive field</strong></li>
</ol>
<p>不管是计算机，还是人脑，去认一个物，都是去判断特定的patten（所以就会有错认的图片产生），这也说明，如果神经网络要去辨识物体，是不需要每个神经元都把整张图片看一次的，只需要关注一些特征区域就好了。（感受野, <code>Receptive field</code>)</p><p>如果你一直用3x3，会不会看不到大的patten呢？$\rightarrow$ 会也不会。</p><p>首先，小的filter当然是不可能看到它的感受野以外的部分，但是，神经网络是多层架构，你这层的输出再被卷一次，这时候每一个数字代表的就是之前的9个像素计算的结果，这一轮的9个数字就是上一层的81个像素（因为stride的原因，大部分是重复的）的计算结果，换言之，感受野大大增强了，也就是说，你只需要增加层数，就可以在小的filter上得到大的patten.</p><ol start="2">
<li><strong>filter &amp; feature map</strong></li>
</ol>
<p>从神经元角度和全连接角度出发的话，每个框其实可以有自己的参数的（即你用了64步把整个图片扫描完的话，就有64组参数），而事实上为了简化模型，可以让某些框对应同样的参数（<strong>参数共享</strong>），原因就是同一特征可能出现在多个位置，比如人有两只脚。</p><p>再然后，实际上每一次都是用一组参数扫完全图的，意思是在每个角落都只搜索这<strong>一个特征</strong>。</p><p>我们把这种机制叫<code>filter</code>，一个filter只找一种特征，乘加出来的结果叫<code>feature map</code>，即这个filter提取出来的特征图。</p><p>因此，</p><ul>
<li>你想提取多少个特征，就得有多少个filter</li>
<li>表现出来就成了你这一层输出有多少个channel</li>
<li>这就是为什么你的图片进来是3channel，出来就是N个channel了，取决于你设计了多少个filter</li>
</ul>
<ol start="3">
<li><strong>Pooling &amp; subsampling</strong></li>
</ol>
<p>由于图像的视觉特征，你把它放大或缩小都能被人眼认出来，因此就产生了pooling这种机制，可以降低样本的大小，这主要是为了减小运算量吧（硬件性能足够就可以不考虑它）。</p><ol start="4">
<li><strong>Data Augmentation</strong></li>
</ol>
<p>CNN并不能识别缩放、旋转、裁切、翻转过的图片，因此训练数据的增强也是必要的。</p><h2>AlphaGo</h2>
<p><strong>layer 1</strong></p><ol>
<li>能被影像化的问题就可以尝试CNN，围棋可以看成是一张19x19的图片</li>
<li>每一个位置被总结出了48种可能的情况(超参1)</li>
<li>所以输入就是19x19x48</li>
<li>用0来padding成23x23</li>
<li>很多patten、定式也是影像化的，可以被filter扫出来</li>
<li>总结出5x5大小的filter就够用了（超参2）</li>
<li>就用了192个fitler（即每一次output有48层channel)（超参3）</li>
<li>stride = 1</li>
<li>ReLU</li>
</ol>
<p><strong>layer 2-12</strong></p><ol>
<li>padding成 21x21</li>
<li>192个 3x3 filter with stride = 1</li>
<li>ReLU</li>
</ol>
<p><strong>layer 13</strong></p><ol>
<li>1x1 filter stride = 1</li>
<li>bias</li>
<li>softmax</li>
</ol>
<p>其中192(个filter)这个超参对比了128，256，384等，也就是说人类并不理解它每一次都提取了什么特征。</p><blockquote>
<p>subsampling对围棋也有用吗？ 上面的结构看出并没有用，事实上，围棋你抽掉一行一列影响是很大的。</p></blockquote>
<h1>Self-Attention</h1>
<p>前面说的都是输入为一个向量（总会拉平成一维向量），如果是多个向量呢？有这样的场景吗？</p><ul>
<li>一段文字，每一个文字都用one-hot或word-embedding来表示<ul>
<li>不但是多个向量，而且还长短不齐</li>
</ul>
</li>
<li>一段语音，每25ms采样形成一个向量，步长为每10ms重复采样，形成向量序列<ul>
<li>400 sample points (16khz)</li>
<li>39-dim MFCC</li>
<li>80-dim filter bank output</li>
<li>参考人类语言处理课程</li>
</ul>
</li>
<li>一个Graph组向量（比如social network)<ul>
<li>每个节点（每个人的profile）就是一个向量</li>
</ul>
</li>
<li>一个分子结构<ul>
<li>每个原子就是一个one-hot</li>
</ul>
</li>
</ul>
<p><strong>输出是什么样的？</strong></p><ol>
<li>一个向量对应一个输出<ul>
<li>文字 -&gt; POS tagging</li>
<li>语音 -&gt; a, a, b, b(怎么去重也参考<a href="https://speech.ee.ntu.edu.tw/~hylee/dlhlp/2020-spring.html">人类语言处理</a>课程)</li>
<li>graph -&gt; 每个节点输出特性（比如每个人的购买决策）</li>
</ul>
</li>
<li>只有一个输出<ul>
<li>文字 -&gt; 情绪分析，舆情分析</li>
<li>语音 -&gt; 判断是谁说的</li>
<li>graph -&gt; 输出整个graph的特性，比如亲水性如何</li>
</ul>
</li>
<li>不定输出（由network自己决定）<ul>
<li>这就叫seq2seq</li>
<li>文字 -&gt; 翻译</li>
<li>语音 -&gt; 真正的语音识别</li>
</ul>
</li>
</ol>
<p>self-attention</p><p>稍稍回顾一下self attention里最重要的q, k, v的部分：</p><figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-2e4df1ab5ca25149.png" /></figure><p>图示的是q2与所有的k相乘，再分别与对应的v相乘，然后相加，得到q2对应的输出：b2的过程。</p><p>下图则是矩阵化后的结论：
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-69882dd1e6b2701e.png" /></figure>
具体细节看专题</p><p>真正要学的，就是图中的$W^q, W^k, W^v$</p><h2>Multi-head Self-attention</h2>
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-8ae9ebcbbd998558.png" /></figure><p>CNN是Self-attention的特例</p><figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-1e5ceafb18ac5d32.png" /></figure><h2>Self-attention for Graph</h2>
<p>了解更多：<a href="https://youtu.be/eybCCtNKwzA">https://youtu.be/eybCCtNKwzA</a></p><h1>Transformer</h1>
<p>Transformer是一个seq2seq的model</p><p>以下场景，不管看上去像不像是seq2seq的特征，都可以尝试用seq2seq（trnasformer）来“硬train一发”</p><ul>
<li>QA类的问题，送进去question + context，输出answer<ul>
<li>翻译，摘要，差别，情感分析，只要训练能套上上面的格式，就有可能</li>
</ul>
</li>
<li>文法剖析，送入是句子，输出是树状的语法结构<ul>
<li>把树状结构摊平（其实就是多层括号）</li>
<li>然后就用这个对应关系来当成翻译来训练（即把语法当成翻译）</li>
</ul>
</li>
<li>multi-label classification<ul>
<li>你不能在做multi-class classification的时候取top-k,因为有的属于一个类，有的属于三个类，k不定</li>
<li>所以你把每个输入和N个输出也丢到seq2seq里去硬train一发，网络会自己学到每个文章属于哪“些”类别（不定个数，也像翻译一样）</li>
</ul>
</li>
<li>object dectection<ul>
<li>这个更匪夷所思，感兴趣看论文：<a href="https://arxiv.org/abs/2005.12872(End-to-End">https://arxiv.org/abs/2005.12872(End-to-End</a> Object Detection with Transformers)</li>
</ul>
</li>
</ul>
<h2>Encoder</h2>
<p>Q, K, V(relavant/similarity), zero padding mask, layer normalization, residual等, 具体看<code>self-attention</code>一节。</p><h2>Decoder</h2>
<h3>AT v.s. NAT</h3>
<p>我们之前用的decoder都是一个一个字地预测（输出的）</p><ul>
<li>所以才有position-mask（用来屏蔽当前位置后面的字）</li>
</ul>
<p>这种叫<code>Auto Regressive</code>，简称<code>AT</code>,<code>NAT</code>即<code>Non Auto Regressive</code></p><figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-680f5c4380c93898.png" /></figure><p>它一次生成输出的句子。</p><p>至于seq2seq的输出是不定长的，它是怎么在一次输出里面确定长度的，上图已经给出了几种做法：</p><ol>
<li>另做一个predictor来输出一个数字，表示应该输出的长度</li>
<li>直接用一个足够长的<bos>做输入（比如300个），那输出也就有300个，取到第一个<eos>为止</li>
</ol>
<p>因为不是一个一个生成了，好处</p><ol>
<li>可以平行运算。</li>
<li>输出的长度更可控</li>
</ol>
<blockquote>
<p>NAT通常表现不如AT好 (why? <strong>Multi-mmodality</strong>)</p></blockquote>
<p>detail: <a href="https://youtu.be/jvyKmU4OM3c">https://youtu.be/jvyKmU4OM3c</a> (Non-Autoregressive Sequence Generation)</p><h3>AT</h3>
<p>在decoder里最初有让人看不懂的三个箭头从encode的输出里指出来:</p><figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-6574d04e094f240a.png" /></figure><p>其实这就是<code>cross attention</code></p><figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-8d8049bb84081547.png" /></figure><p>它就是把自己第一层(self-attention后)的输出乘一个$W^q$得到的<code>q</code>，去跟encoder的输出分别乘$W^k, W^v$得到的k和v运算($\sum q \times k \times v$)得到当前位置的输出的过程。</p><p>而且研究者也尝试过各种<code>cross attention</code>的方法，而不仅仅是本文中的无论哪一层都用<code>encoder</code>最后一层的输出做q和v这一种方案：</p><figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-12d3f05c2a546bfe.png" /></figure><h2>Training Tips</h2>
<h3>复制机制</h3>
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-46f9d4aecc642f6b.png" /></figure><p>一些场景，训练的时候没必要去“生成”阅读材料里提到的一些概念，只需要把它“复制”出来即可，比如上述的人名，专有名字，概念等，以及对文章做摘要等。</p><ul>
<li>Pointer Network: <a href="https://youtu.be/VdOyqNQ9aww">https://youtu.be/VdOyqNQ9aww</a></li>
<li>Copying Mechanism in Seq2Seq <a href="https://arxiv.org/abs/1603.06393">https://arxiv.org/abs/1603.06393</a></li>
</ul>
<h3>Guided Attention</h3>
<p>像语音这种连续性的，需要强制指定(guide)它的attention顺序，相对而言，文字跳跃感可以更大，语音一旦不连续就失去了可听性了，一些关键字：</p><ul>
<li>Monotonic Attention</li>
<li>Location-aware attention</li>
</ul>
<h3>Beam Search</h3>
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-8b07d1dc44895d0b.png" /></figure><h3>Optimizing Evaluation Metrics / BLEU</h3>
<ul>
<li><p>训练的时候loss用的是cross entropy，要求loss越小越好，</p></li>
<li><p>而在evaluation的时候，我们用的是预测值与真值的<code>BLEU score</code>，要求score越大越好</p></li>
<li><p>那么越小的cross entropy loss真的能产生越高的BLEU score吗？ 未必</p></li>
<li><p>那么能不能在训练的时候也用BLEU score呢？ 不行，它太复杂没法微分，就没法bp做梯度了。</p></li>
</ul>
<h3>Exposure bias</h3>
<p>训练时候应用了<code>Teaching force</code>，用了全部或部分真值当作预测结果来训练（或防止一错到底），而eval的时候确实就是一错到底的模式了。</p><h1>Self-supervised Learning</h1>
<ul>
<li>芝麻街家庭：elmo, bert, erine...</li>
<li>bert就是transformer的encoder</li>
</ul>
<h2>Bert</h2>
<h3>GLUE</h3>
<p>GLUE: General Language Understanding Evaluation</p><p>基本上就是看以下这九个模型的得分：</p><figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-0895ab4b9a46e931.png" /></figure><p>训练：</p><ol>
<li>预测mask掉的词(masked token prediction)<ul>
<li>为训练数据集添加部分掩码，预测可能的输出</li>
<li>类似word2vec的C-Bow</li>
</ul>
</li>
<li>预测下一个句子（分类，比如是否相关）(next sentence prediction)<ul>
<li>在句首添加<cls>用来接分类结果</li>
<li>用<sep>来表示句子分隔</li>
</ul>
</li>
</ol>
<p>下游任务（Downstream Task） &lt;- Fine Tune:</p><ol>
<li>sequence -&gt; class: sentiment analysis<ul>
<li>这是需要有label的</li>
<li><cls>节点对的linear部分是随机初始化</li>
<li>bert部分是pre-train的</li>
</ul>
</li>
<li>sequence -&gt; sequence(等长): POS tagging</li>
<li>2 sequences -&gt; class: NLI(从句子A能否推出句子B)(Natural Language Inferencee)<ul>
<li>也比如文章下面的留言的立场分析</li>
<li>用<cls>输出分类结果，用<sep>分隔句子</li>
</ul>
</li>
<li>Extraction-based Question Answering: 基于已有文本的问答系统<ul>
<li>答案一定是出现在文章里面的</li>
<li>输入文章和问题的向量</li>
<li>输出两个数字(start, end)，表示答案在文章中的索引</li>
</ul>
</li>
</ol>
<p>QA输出：</p><figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-6e6c9471f5515877.png" /></figure><p>思路：</p><ol>
<li>用<cls>input<sep>document 的格式把输入摆好</li>
<li>用pre-trained的bert模型输出同样个数的向量</li>
<li>准备两个与bert模型等长的向量（比如768维）a, b（random initialized)</li>
<li>a与document的每个向量相乘(inner product)</li>
<li>softmax后，找到最大值，对应的位置(argmax)即为start index</li>
<li>同样的事b再做一遍，得到end index</li>
</ol>
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-e94a647f82156c38.png" /></figure><h3>Bert train seq2seq</h3>
<p>也是可能的。就是你把输入“弄坏”，比如去掉一些字词，打乱词序，倒转，替换等任意方式，让一个decoder把它还原。 -&gt; <strong>BART</strong></p><h3>附加知识</h3>
<p>有研究人员用bert去分类DNA，蛋白质，音乐。以DNA为例，元素为A,C,G,T,分别对应4个随机词汇，再用bert去分类（用一个英文的pre-trained model），同样的例子用在了蛋白质和音乐上，居然发现效果全部要好于“纯随机”。</p><p>如果之前的实验说明了bert看懂了我们的文章，那么这个荒诞的实验（用完全无关的随意的英文单词代替另一学科里面的类别）似乎证明了事情没有那么简单。</p><h3>More</h3>
<ol>
<li><a href="https://youtu.be/1_gRK9EIQpc">https://youtu.be/1_gRK9EIQpc</a></li>
<li><a href="https://youtu.be/Bywo7m6ySlk">https://youtu.be/Bywo7m6ySlk</a></li>
</ol>
<h2>Multi-lingual Bert</h2>
<p>略</p><h2>GPT-3</h2>
<p>训练是predict next token...so it can do generation(能做生成)</p><blockquote>
<p>Language Model 都能做generation</p></blockquote>
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-687d9cf507137131.png" /></figure><p><a href="https://youtu.be/DOG1L9lvsDY">https://youtu.be/DOG1L9lvsDY</a></p><p>别的模型是pre-train后，再fine-tune， GPT-3是想实现zero-shot，</p><h3>Image</h3>
<p><strong>SimCLR</strong></p><ul>
<li><a href="https://arxiv.org/abs/2002.05709">https://arxiv.org/abs/2002.05709</a></li>
<li><a href="https://github.com/google-research/simclr">https://github.com/google-research/simclr</a></li>
</ul>
<p><strong>BYOL</strong></p><ul>
<li><strong>B</strong>ootstrap <strong>y</strong>our <strong>o</strong>own <strong>l</strong>atent</li>
<li><a href="https://arxiv.org/abs/2006.07733">https://arxiv.org/abs/2006.07733</a></li>
</ul>
<h3>Speech</h3>
<p>在bert上有九个任务(GLUE)来差别效果好不好，在speech领域还缺乏这样的数据库。</p><h2>Auto Encoder</h2>
<p>也是一种<code>self-supervised</code> Learning Framework -&gt; 也叫 pre-train, 回顾：
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-43a21995530788b6.png" /></figure></p><p>在这个之前，其实有个更古老的任务，它就是<code>Auto Encoder</code></p><figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-2c3764ec5ff4cb6b.png" /></figure><ul>
<li>用图像为例，通过一个网络encode成一个向量后，再通过一个网络解码(reconstrucion)回这张图像（哪怕有信息缺失）</li>
<li>中间生成的那个向量可以理解为对原图进行的压缩</li>
<li>或者说一种降维</li>
</ul>
<p>降维的课程：</p><ul>
<li>PCA: <a href="https://youtu.be/iwh5o_M4BNU">https://youtu.be/iwh5o_M4BNU</a></li>
<li>t-SNE: <a href="https://youtu.be/GBUEjkpoxXc">https://youtu.be/GBUEjkpoxXc</a></li>
</ul>
<p>有一个de-noising的Auto-encoder, 给入的是加了噪音的数据，经过encode-decode之后还原的是没有加噪音的数据</p><p>这就像加了噪音去训练bert</p><figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-71c69ce2d3693253.png" /></figure><h3>Feature Disentangle</h3>
<p>去解释auto-encoder压成的向量就叫<code>Feature Disentagle</code>，比如一段音频，哪些是内容，哪些是人物；一段文字，哪些表示语义，哪些是语法；一张图片，哪些表示物体，哪些表示纹理，等。</p><p>应用： voice conversion -&gt; 变声器</p><p>传统的做法应该是每一个语句，都有两种语音的资料，N种语言/语音的话，就需要N份。有Feature Disentangle的话，只要有两种语音的encoder，就能知道哪些是语音特征，哪些是内容特征，拼起来，就能用A的语音去读B的内容。所以<strong>前提</strong>就是能分析压缩出来的向量。</p><h3>Discrete Latent Representation</h3>
<p>如果压缩成的向量不是实数，而是一个binary或one-hot</p><ul>
<li>binary: 每一个维度几乎都有它的含义，我们只需要看它是0还是1</li>
<li>one-hot: 直接变分类了。-&gt; <code>unsupervised classification</code></li>
</ul>
<p><strong>VQVAE</strong></p><figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-f8bab3e37a58d91b.png" /></figure><ul>
<li>Vector Quantized Variational Auot-encoder <a href="https://arxiv.org/abs/1711.00937">https://arxiv.org/abs/1711.00937</a></li>
</ul>
<h3>Text as Representation</h3>
<ul>
<li><a href="https://arxiv.org/abs/1810.02851">https://arxiv.org/abs/1810.02851</a></li>
</ul>
<p>如果压缩成的不是一个向量，而也是一段<code>word sequence</code>，那么是不是就成了<code>summary</code>的任务？ 只要encoder和decoder都是seq2seq的model</p><p>-&gt; seq2seq2seq auto-encoder -&gt; <code>unsupervised summarization</code></p><p>事实上训练的时候encoder和decoder可能产生强关联，这个时候就引入一个额外的<code>discriminator</code>来作判别:
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-03f7f375744bc8cf.png" /></figure></p><p>有点像cycle GAN，一个generator接一个discriminator，再接另一个generator</p><h3>abnormal detection</h3>
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-39b0acc3bf74a2ef.png" /></figure><ul>
<li>Part 1: <a href="https://youtu.be/gDp2LXGnVLQ">https://youtu.be/gDp2LXGnVLQ</a></li>
<li>Part 2: <a href="https://youtu.be/cYrNjLxkoXs">https://youtu.be/cYrNjLxkoXs</a></li>
<li>Part 3: <a href="https://youtu.be/ueDlm2FkCnw">https://youtu.be/ueDlm2FkCnw</a></li>
<li>Part 4: <a href="https://youtu.be/XwkHOUPbc0Q">https://youtu.be/XwkHOUPbc0Q</a></li>
<li>Part 5: <a href="https://youtu.be/Fh1xFBktRLQ">https://youtu.be/Fh1xFBktRLQ</a></li>
<li>Part 6: <a href="https://youtu.be/LmFWzmn2rFY">https://youtu.be/LmFWzmn2rFY</a></li>
<li>Part 7: <a href="https://youtu.be/6W8FqUGYyDo">https://youtu.be/6W8FqUGYyDo</a></li>
</ul>
<h1>Adversarial Attack</h1>
<p>给你一张猫的图片，里面加入少许噪音，以保证肉眼看不出来有噪音的存在：</p><ol>
<li>期望分类器认为它不是猫</li>
<li>期望分类器认为它是一条鱼，一个键盘...</li>
</ol>
<p>比如你想要欺骗垃圾邮件过滤器</p><ul>
<li>找到一个与$x^0$非常近的向量x</li>
<li>网络正常输出y</li>
<li>真值为$\hat y$</li>
<li>$L(x) = -e(y, \hat y)$</li>
<li>$x^* = arg\underset{d(x^0, x) \leq \epsilon}{\rm min}\ L(x)$ 即要找到令损失最大的x<ol>
<li>这里L(x)我们取了反</li>
<li>$\epsilon$越小越好，指的是$x^0$要与x越接近越好（欺骗人眼）</li>
</ol>
</li>
<li>如果还期望它认成是$y^{target}$，那就再加上与其的的损失</li>
<li>$L(x) = -e(y, \hat y) + e(y, y^{target})$</li>
<li>注意两个error是反的，一个要求越远越好(真值），一个要求越近越好（target)</li>
</ul>
<p>怎么计算$d(x^0, x) \leq \epsilon$呢？</p><figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-683386630a1ecc47.png" /></figure><p>图上可知，如果都改变一点点，和某一个区域改动相当大，可能在L2-norm的方式计算出来是一样的，但是在L-infinity看来是不一样的（它只关心最大的变动）。</p><p>显然L-infinity更适合人眼的逻辑，全部一起微调人眼不能察觉，单单某一块大调，人眼是肯定可以看出来的。</p><p>而如果是语音的话，可能耳朵对突然某个声音的变化反而不敏感，整体语音风格变了却能立刻认出说话的人声音变了，这就要改变方案了。</p><h2>Attack Approach</h2>
<p>如何得到这个x呢？其实就是上面的损失函数。以前我们是为了train权重，现在train的就是x本身了。</p><ol>
<li>损失达到我们的要求 （有可能这时候与原x相关很远）</li>
<li>与原x的距离达到我们的要求, 怎么做？<ul>
<li>其实就是以$x^0$为中心，边长为$2\epsilon$的矩形才是期望区域</li>
<li>如果update后，$x^t$仍然落在矩形外，那么就在矩形里找一个离它最近的点，当作本轮更新后的$x^t$，进入下一轮迭代</li>
</ul>
</li>
</ol>
<p>Fast Gradient Sign Method(FGSM): <a href="https://arxiv.org/abs/1412.6572">https://arxiv.org/abs/1412.6572</a></p><ul>
<li>相比上面的迭代方法，FGSM只做一次更新</li>
<li>就是根据梯度，判断是正还是负，然后把原x进行一次加减$\epsilon$的操作（其实等于是落在了矩形的四个点上）</li>
<li>也就是说它直接取了四个点之一作为$x^0$</li>
</ul>
<h2>White Box v.s. Black Box</h2>
<p>讲上述方法的时候肯定都在疑惑，分类器是别人的，我怎么可能拿到别人的模型来训练我的攻击器？ -&gt; <strong>White Box Attack</strong></p><p>那么<code>Black Box Attack</code>是怎么实现的呢？</p><ol>
<li>如果我们知道对方的模型是用什么数据训练的话，我们也可以训练一个类似的(proxy network)<ul>
<li>很大概率都是用公开数据集训练的</li>
</ul>
</li>
<li>如果不知道的话呢？就只能尝试地丢一些数据进去，观察（记录）它的输出，然后再用这些测试的输入输出来训练自己的proxy network了。</li>
</ol>
<ul>
<li>one pixel attack<ul>
<li><a href="https://arxiv.org/abs/1710.08864">https://arxiv.org/abs/1710.08864</a></li>
<li><a href="https://youtu.be/tfpKIZIWidA">https://youtu.be/tfpKIZIWidA</a></li>
</ul>
</li>
<li>universal adversarial attack<ul>
<li>万能noise</li>
<li><a href="https://arxiv.org/abs/1610.08401">https://arxiv.org/abs/1610.08401</a></li>
</ul>
</li>
<li>声音</li>
<li>文本</li>
<li>物理世界<ul>
<li>比如欺骗人脸识别系统，去认成另一个人</li>
<li>又比如道路环境，车牌识别等，也可以被攻击</li>
<li>要考虑摄像头能识别的分辨率</li>
<li>要考虑训练时候用的图片颜色与真实世界颜色不一致的问题</li>
</ul>
</li>
<li>Adversarial Reprogramming</li>
<li>Backdoor in Model<ul>
<li>attack happens at the training phase</li>
<li><a href="https://arxiv.org/abs/1804.00792">https://arxiv.org/abs/1804.00792</a></li>
<li>be careful of unknown dataset...</li>
</ul>
</li>
</ul>
<h2>Defence</h2>
<h3>Passive Defense（被动防御）</h3>
<p>进入network前加一层filter</p><ul>
<li>稍微模糊化一点，就去除掉精心设计的noise了<ul>
<li>但是同时也影响了正常的图像</li>
</ul>
</li>
<li>对原图进行压缩</li>
<li>把输入用Generator重新生成一遍</li>
</ul>
<p>如果攻击都知道你怎么做了，其实很好破解，就把你的filter当作network的一部分重新开始设计noise，所以可以选择加入随机选择的一些预处理(让攻击者不可能针对性地训练)：</p><figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-a467e272d69cc306.png" /></figure><h3>Proactive Defense（主动防御）</h3>
<p>训练的时候就训练比较不容易被攻破的模型。比如训练过程中加入noise，把生成的结果重新标注回真值。</p><ul>
<li>training model</li>
<li>find the problem</li>
<li>fix it</li>
</ul>
<p>有点类似于<code>Data Augmentation</code></p><p>仍然阻挡不了新的攻击算法，即你对数据进行augment之外的范围。</p><h1>Explainable Machine Learning(可解释性)</h1>
<ul>
<li>correct answers $\neq$ intelligent</li>
<li>很多行业会要求结果必须可解释<ul>
<li>银行，医药，法律，驾驶....</li>
</ul>
</li>
</ul>
<p><strong>Local Explanation</strong></p><p>Why do you thing <strong>this image</strong> is a cat?</p><p><strong>Global Explanation</strong></p><p>What does a &quot;<strong>cat</strong>&quot; look like?</p><ol>
<li>遮挡或改变输入的某些部分，观察对已知输出的影响<ul>
<li>（比如拦到某些部分确实认不出图像是一条狗了）</li>
</ul>
</li>
<li>遮挡或改变输入的某些部分，把两种输出做loss，对比输入变化与loss变化：<ul>
<li>$|\frac{\varDelta e}{\varDelta x}| \rightarrow \frac{\partial e}{\partial x_n}$</li>
</ul>
</li>
</ol>
<p>把上述（任一种）每个部分（像素，单词）的影响结果输出，就是：<code>Saliency Map</code></p><h2>Saliency Map</h2>
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-a69f229d00a9d750.png" /></figure><p>图1，2就是为了分辨宝可梦和数码宝贝，人类一般很难区分出来，但机器居然轻松达到了98%的准确率，经过绘制<code>Saliency Map</code>，发现居然就是图片素材（格式）的原因，一个是png，一个是jpg，造成背景一个是透明一个是不透明的。</p><p>也就是说，能发现机器判断的依据不是我们关注的本体（高亮部分就是影响最大的部分，期望是在动物身上）</p><p>第三张图更可笑，机器是如何判断这是一只马的？居然也不是马的本体，而是左下角，标识图片出处的文字，可能是训练过程中同样的logo过多，造成了这个“人为特征”。</p><p>解决方案：</p><h3>Smooth Gradient</h3>
<p>随机给输入图片加入噪点，得到saliency map（们），然后取平均</p><figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-e3f0f1a301cbd178.png" /></figure><h3>Integrated gradient(IG)</h3>
<p>一个特征在从无到有的阶段，梯度还是明显的，但是到了一定程度，特征再增强，对gradient影响也不大了，比如从片子来判断大象，到了一定长度，一张图也不会“更像大象”</p><p>一种思路：<a href="https://arxiv.org/abs/1611.02639">https://arxiv.org/abs/1611.02639</a></p><h2>global explaination</h2>
<p><strong>What does a filter detect?</strong></p><p>如果经过某层（训练好的）filter，得到的feature map一些位置的值特别大，那说明这个filter提取的就是这类特征/patten。</p><p>我们去&quot;创造&quot;一张包含了这种patten的图片：$X^* = arg\ \underset{X}{\rm max}\sum_i\sum_j a_{ij}$，即这个图片是“训练/learn“出来的，通过找让X的每个元素($a_{ij}$)在被filter乘加后结果最大的方式。 -&gt; <code>gradient ascent</code></p><p>然后再去观察$X^*$有什么特征，就基本上可以认定这个（训练好的）filter提取的是什么样的patten了。
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-be596d0584d49d4e.png" /></figure></p><blockquote>
<p><code>adversarial attack</code> 类似的原理，但这是对单filter而言。如果你想用同样的思路去让输出y越大越好，得到X，看X是什么，得到的X大概率都是一堆噪音。如果能生成图像，那是<code>GAN</code>的范畴了。</p></blockquote>
<p>于是，尝试再加一个限制，即不但要让y最大，还要让X看起来最有可能像一个数字：</p><ul>
<li>$R(X)$: how likely X is a digit</li>
<li>$X^* = arg\ \underset{X}{\rm max}y_i + \color{red}{R(X)}$</li>
<li>$R(X) = -\sum_{i,j}|X_{i,j}|$ 比如这个规则，期望每个像素越黑越好</li>
</ul>
<h1>Domain Adaptation</h1>
<p><code>Transfer Learning</code>的一种，在训练数据集和实际使用的数据集不一样的时候。 <a href="https://youtu.be/qD6iD4TFsdQ">https://youtu.be/qD6iD4TFsdQ</a></p><p>需要你对<code>target domain</code>的数据集有一定的了解。</p><p>有一种比较好的情况就是，target domain既有数据，还有标注（但不是太多，如果太多的话就不需要<code>source domain</code>了，直接用target来训练就好了），那就像bert一样，去<code>fine tune</code>结果，要注意的是标本量过小，可能很容易<code>overfitting</code>.</p><p>如果target doamin有<strong>大量</strong>资料，但是没有标注呢？</p><h2>Domain Adversarial Training</h2>
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-dab4c03bac102639.png" /></figure><ul>
<li>把source domain的network分为特征提取器（取多少层cnn可以视为超参，并不一定要取所有层cnn）和分类器</li>
<li>然后在特征取层之后跟另一个分类器，用来判断图像来自于source还是target（有点像<code>Discriminator</code></li>
<li>与真值有一个loss，source, target之间也有一个loss，要求找到这样的参数组分别让两个loss最小</li>
<li>loos和也应该最小（图中用的是减，但其实$L_d$的期望是趋近于0，不管是正还是负都是期望越小越好）（不如加个绝对值？）</li>
<li>每一小块都有一组参数，是一起训练的</li>
<li>目的就是既要逼近训练集的真值，还要训练出一个网络能模糊掉source和target数据集的差别</li>
</ul>
<h3>Limit</h3>
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-6891c5cbf5d2c728.png" /></figure><p>如果target数据集如上图左，显然结果是会比上图右要差一点的，也就是说尽量要保持同分布。在这里用了另一个角度，就是让数据<strong>离boundary越远越好</strong></p><ul>
<li>Decision-boundary Iterative Refinement Training with a Teacher(<code>DIRT-T</code>)<ul>
<li><a href="https://arxiv.org/abs/1802.08735">https://arxiv.org/abs/1802.08735</a></li>
</ul>
</li>
<li>Maximum Classifier Discrepancy <a href="https://arxiv.org/abs/1712.02560">https://arxiv.org/abs/1712.02560</a></li>
</ul>
<h2>More</h2>
<ul>
<li>如果source 和 target 里的类别不完全一样呢？<ul>
<li>Universal domain adaptation</li>
</ul>
</li>
<li>如果target既没有label，数据量也非常少（比如就一张）呢？<ul>
<li>Test Time Training(TTT) <a href="https://arxiv.org/abs/1909.13231">https://arxiv.org/abs/1909.13231</a></li>
</ul>
</li>
</ul>
<p><strong>Domain Generalization</strong>
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-0c290ef2c9a19b50.png" /></figure></p><h1>Deep Reinforcement Learning (RL)</h1>
<ul>
<li><strong>Environment</strong> 给你 <code>Observation</code></li>
<li><strong>Actor</strong> 接收入 <code>Observation</code>, 输出 <code>Action</code></li>
<li><code>Action</code> 反馈给 <strong>Environment</strong>, 计算出 <code>Reward</code> 反馈给 <strong>Actor</strong></li>
<li>要求 <code>Reward</code> 最大</li>
</ul>
<p>与 GAN 的不同之处，不管是生成器还是判别器，都是一个network，而RL里面，Actor和Reward都是黑盒子，你只能看到结果。</p><h2>Policy Gradient</h2>
<p><a href="https://youtu.be/W8XF3ME8G2I">https://youtu.be/W8XF3ME8G2I</a></p><ol>
<li>先是用很类似监督学习的思路，给每一步的最优（或最差）方案一个label，有label就能做loss。先把它变成一个二分类的问题。</li>
<li>打分还可以不仅仅是“好”或“不好”，还可以是一个程度，比如1.5比0.5的“支持”力度要大一些，而-10显然意味着你千万不要这么做，非常拒绝。</li>
<li>比如某一步，可以有三种走法，可以用onehot来表示，其中一种走法可以是[1,0,0]$^T$，表示期望的走法是第一种。</li>
<li>但是也可以是[-1,0,0]$^T$，标识这种走法是不建议的</li>
<li>也可以是[3.5,0,0]$^T$等</li>
<li>后面会用<code>1, -1, 10, 3.5</code>这样的scalar来表示，但要记住其实它们是ont-hot中的那个非零数。</li>
</ol>
<p>现实世界中很多场景不可能执行完一步后就获得reward，或者是全局最佳的reward（比如下围棋）。</p><p><strong>v1</strong></p><p>一种思路是，每一步之后，把游戏/棋局进行完，把当前reward和后续所有步骤的reward加一起做reward -&gt; <code>cumulated reward</code> $\rightarrow G_t = \sum_{n=t}^Nr_n$</p><p><strong>v2</strong></p><p>这种思路仍然有问题，游戏步骤越长，当前步对最终步的影响越小。因此引入一个小于1的权重$\gamma &lt; 1$: $G_1' = r_1 + \gamma r_2 + \gamma^2r_3 + \cdots$</p><p>这样越远的权重越小： $G_t' = \sum_{n=t}^N \color{red}{\gamma^{n-t}} r_n$</p><blockquote>
<p>注意，目前得到的<code>G</code>就是为了给每一次对observation进行的action做loss的对象。</p></blockquote>
<p><strong>v3</strong></p><p>标准化reward。你有10分，是高是低？如果所有人都是20分，那就是低分，所以与G做对比的时候，通常要减去一个合适的值<code>b</code>，让得分的分布有正有负。</p><p><strong>Policy Gradient</strong></p><p>普通的gradient descent是搜集一遍数据，就可以跑for循环了，而PG不行，你每次得到梯度后，要重采一遍样，其实也很好理解，你下了某一步，经过后续50步后，输了，你的下一轮测试应该是下一盘随机的棋，而不是把更新好的参数再用到同一盘棋去。</p><p>还是不怎么好理解，至少要知道，我做参数是不为了训练出这一盘棋是怎么下出来的，而是根据这个（大多是输了的）结果，以及学到的梯度，去下一盘新的棋试试。</p><h2>Actor Critic</h2>
<p><strong>Critic</strong>:</p><ul>
<li>Given <code>actor</code> $\theta$, how good it is when <code>observing</code> s (and taking action a)</li>
</ul>
<p><strong>Value function</strong> $V^\theta(s)$:</p><ul>
<li>使用actor $\theta$的时候，预测会得到多少的<code>cumulated reward</code></li>
<li>分高分低其实还是取决于actor，同样的局面，不同的actor肯定拿的分不同。</li>
</ul>
<h3>Monte-Carlo based approach (MC)</h3>
<p>蒙特卡洛搜索，正常把游戏玩完，得到相应的G.</p><h3>Temporal-difference approach (TD)</h3>
<p>不用玩完整个游戏，就用前后时间段的数据来得到输出。
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-eae0e77ede8b2ae9.png" /></figure></p><p>关键词：</p><ul>
<li>我们既不知道v(t+1)，也不知道v(t)，但确实能知道<code>v(t+1)-v(t)</code>.</li>
</ul>
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-437a2be80e108b60.png" /></figure><p>这个例子没看懂，后面七次游戏为什么没有sa了？</p><p><strong>v3.5</strong></p><p>上文提到的V可以用来作更早提到的b:</p><ul>
<li>${S_t, a_t}\ A_t = G_t' - V^\theta(S_t)$</li>
<li>回顾一下，$V^\theta(S_t)$是看到某个游戏画面时算出来的reward</li>
<li>它包含$S_t$状态下，后续各种步骤的reward的平均值</li>
<li>而$G_t'$则是这一步下的rewared</li>
<li>两个数相减其实就是看你的这一步是比平均水平好还是差</li>
<li>比如你得到了个负值，代表在当前场景下，这个actor执行的步骤是低于平均胜率的，需要换一种走法。</li>
</ul>
<p><strong>v4</strong></p><p>3.5版下，G只有一个样本（一次游戏）的结果，这个版本里，把st再走一步，试难$S_{t+1}$的各种走法下reward的平均值，用它来替换G'，而它的值，就是当前的reward加上t+1时刻的V:</p><ul>
<li>$r_t + V^\theta(S_{t+1}) - V^\theta(S_t)$</li>
</ul>
<p>这就是：</p><h3>Advantage Actor-Critic</h3>
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-07e8e3fb880671ca.png" /></figure><p>就看图而言，感觉就是坚持这一步走完，后续所有可能的rewawrd， 减去， 从这一步开始就试验所有走法的reward</p><figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-2ff6bf8381c80d55.png" /></figure><p>More:</p><p>Deep Q Network (DQN)</p><ul>
<li><a href="https://arxiv.org/abs/1710.02298">https://arxiv.org/abs/1710.02298</a></li>
<li><a href="https://youtu.be/o_g9JUMw1Oc">https://youtu.be/o_g9JUMw1Oc</a></li>
<li><a href="https://youtu.be/2-zGCx4iv_k">https://youtu.be/2-zGCx4iv_k</a></li>
</ul>
<h2>Reward Shaping</h2>
<p>前面说过很多场景要得到reward非常困难（时间长，步骤长，或根本不会结束），这样的情况叫<code>sparse reward</code>，人类可以利用一些已知知识去人为设置一些reward以增强或削弱机器的某些行为。</p><p>比如游戏：</p><ol>
<li>原地不动一直慢慢减分</li>
<li>每多活一秒也慢慢减分（迫使你去获得更高的reward, 避免学到根本就不去战斗的方式）</li>
<li>每掉一次血也减分</li>
<li>每杀一个敌人就加分</li>
<li>以此类推，这样就不至于要等到一场比赛结束才有“一个”reward</li>
</ol>
<p>又比如训练机械手把一块有洞的木板套到一根棍子上：</p><ol>
<li>离棍子越近，就有一定的加分</li>
<li>其它有助于套进去的规则</li>
</ol>
<p>还可以给机器加上<strong>好奇心</strong>，让机器看到有用的“新的东西”也加分。</p><h2>No Reward, learn from demostration</h2>
<p>只有游戏场景才会有明确的reward，大多数现实场景都是没有reward的，比如训练自动驾驶的车，或者太过死板的reward既不能适应变化，也容易被打出漏洞，比如机器人三定律里，机器人不能伤害人类，却没有禁止囚禁人类，又比如摆放盘子，却没有给出力度，等盘子摔碎了，再去补一条𢱨碎盘子就负reward的规则，也晚了，由此引入模仿学习：</p><h3>Imitation Learning</h3>
<p>略</p><h1>Life-Long Learning</h1>
<p>持续学习，机器学习到一个模型后，继续学下一个模型（任务）。</p><ol>
<li>为什么不一个任务学一个模型<ul>
<li>不可能去存储所有的模型</li>
<li>一个任务的知识不能转移到另一个任务</li>
</ul>
</li>
<li>为什么不直接用迁移学习（迁移学习只关注迁移后的新任务）</li>
</ol>
<h2>Research Directions</h2>
<h3>Selective Synaptic Plasticity</h3>
<p>选择性的神经突触的可塑性？（Regularization-based Approach）</p><p><strong>Catastrophic Forgetting</strong> 灾难性的遗忘
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-135d5398dcfb50bf.png" /></figure></p><p>在任务1上学到的参数，到任务2里接着训练，顺着梯度到了任务2的最优参数，显然不再是任务1的做以参，这叫灾难性的遗忘</p><p>一种思路：</p><p>任务2里梯度要更新未必要往中心，也可以往中下方，这样既在任务2的低loss区域，也没有跑出任务1的低loss区域，实现的方式是找到对之前任务影响比较小的参数，主要去更新那些参数。比如上图中，显然$\theta_1$对任务1的loss影响越小，但是更新它之后会显著影响任务2的loss，而$\theta_2$的改动才是造成任务1loss变大的元凶。</p><figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-5f15705b33b403c6.png" /></figure><p>Elastic Weight Consolidation(EWC)</p><ul>
<li><a href="https://arxiv.org/abs/1612.00796">https://arxiv.org/abs/1612.00796</a></li>
</ul>
<p>Synaptic Intelligence(SI)</p><ul>
<li><a href="https://arxiv.org/abs/1703.04200">https://arxiv.org/abs/1703.04200</a></li>
</ul>
<p>Memory Aware Synapses(MAS)</p><ul>
<li><a href="https://arxiv.org/abs/1711.09601">https://arxiv.org/abs/1711.09601</a></li>
</ul>
<p>RWalk</p><ul>
<li><a href="https://arxiv.org/abs/1801.10112">https://arxiv.org/abs/1801.10112</a></li>
</ul>
<p>Sliced Cramer Preservation(SCP)</p><ul>
<li><a href="https://openreview.net/forum?id=BJge3TNKwH">https://openreview.net/forum?id=BJge3TNKwH</a></li>
</ul>
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-338099d4d7776dd0.png" /></figure><h3>Memory Reply</h3>
<ol>
<li>在训练task1的时候，同时训练一个相应的generator</li>
<li>训练task2的时候，用task1的generator生成pseudo-data，一起来训练生成新的model</li>
<li>同时也训练出一个task1&amp;2的generator</li>
<li>...</li>
</ol>
<h1>Network Compress</h1>
<h2>pruning (剪枝)</h2>
<p>Networks ar typically over-parameterized (there is significant redundant weights or neurons)</p><ul>
<li>可以看哪些参数通常比较大，或值的变化不影响loss（梯度小）-&gt; 权重，为0的次数少 -&gt; 神经元 等等</li>
<li>剪枝后精度肯定是会下降的</li>
<li>需要接着fine-tune</li>
<li>一次不要prune to much</li>
<li>剪参数和剪神经元效果是不一样的<ul>
<li>剪参数会影响矩阵运算，继而影响GPU加速</li>
</ul>
</li>
</ul>
<p>那么为什么不直接train一个小的network呢？</p><ul>
<li>小的network通常很难train到同样的准确率。 （大乐透假说）</li>
</ul>
<h2>Knowledge Distillation (知识蒸馏)</h2>
<p>老师模型训练出来的结果，用学生模型（小模型）去模拟（即是模拟整个输出，而不是模拟分类结果），让小模型能达到大模型同样的结果。</p><p>一般还会在输出的softmax里面加上温度参数（即平滑输出，不同大小的数除一个大于1的数，显然越大被缩小的倍数也越大，比如100/10=10，少了90，10/10=1, 只少了9，差别也从90变成了9）(或者兴趣个极端的例子，T取无穷大，那么每个输出就基本相等了)</p><h2>Parameter Quantization</h2>
<ol>
<li>Using less bits to represent a value</li>
<li>Weight clustering<ul>
<li>把weights分成预先确定好的簇（或根据分布来确定）</li>
<li>对每簇取均值，用均值代替整个簇里所有的值</li>
</ul>
</li>
<li>represent frequent clusters by less bits, represent rare clusters by more bits<ul>
<li>Huffman encoding</li>
</ul>
</li>
</ol>
<p>极限，<code>Binary Weights</code>，用两个bits来描述整个网络，扩展阅读。</p><h2>Depthwise Separable Convolution</h2>
<p>回顾下CNN的机制，参数量是：</p><ul>
<li>卷积核的大小 x 输入图像的通道数 x 输出的通道数</li>
<li>($k\times k$) x in_channel x out_channel</li>
</ul>
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-c15dee267b4ec8d1.png" /></figure><p>而<code>Depthwise Separable Convolution</code>由两个卷积组成：</p><ol>
<li>Depthwise Convolution<ul>
<li>很多人对CNN的误解刚好就是Depthwise Convolution的样子，即一个卷积核对应一个输入的channel（事实上是一组卷积核对应所有的输入channel）</li>
<li>因此它的参数个数 k x k x in_channel</li>
</ul>
</li>
<li>PointWise Convolution<ul>
<li>这里是为了补上通道与通道这间的关系</li>
<li>于是用了一个1x1的<code>标准</code>卷积（即每一组卷积核对应输入的所有通道）</li>
<li>输出channel也由这次卷积决定</li>
<li>应用标准卷积参数量：(1x1) x in_channel x out_channel</li>
</ul>
</li>
</ol>
<figure  size-undefined><img loading="lazy" width="-1" height="-1" src="../assets/1859625-6043951a828b505c.png" /></figure><p>两个参数量做对比, 设<code>in_channel = I</code>, <code>out_channel = O</code></p><ol>
<li>$p_1 = (k\times k) \times I \times O$</li>
<li>$p_2 = (k\times k) \times I + (1\times 1) \times I \times O = (k\times k) \times I + I \times O$</li>
<li>$\frac{p_2}{p_1} = \frac{I\cdot(k^2 + O)}{I\cdot{k^2\cdot O}}</li>
</ol>
<p>= \frac{1}{O} + \frac{1}{k^2} \approx \frac{1}{k^2} 
$</p><p>O代表out_channel，大型网络里256，512比比皆是，所以它可以忽略，那么前后参数量就由$k^2$决定了，如果是大小为3的卷积核，参数量就变成1/9了，已经是压缩得很可观了。</p><h3>Low rank approximation</h3>
<p>上面是应用，原理就是<code>Low rank approximation</code></p><p>以全连接网络举例</p><ol>
<li>如果一个一层的网络，输入<code>N</code>， 输出<code>M</code>，参数为<code>W</code>，那么参数量是<code>MxN</code></li>
<li>中间插入一个线性层<code>K</code>，<ul>
<li>参数变成：<code>V</code>:N-&gt;K, <code>U</code>:K-&gt;M,</li>
<li>参数量：<code>NxK</code> + <code>KxM</code></li>
</ul>
</li>
<li>只要K远小于M和N（比如数量级都不一致），那么参数量是比直接MxN要小很多的</li>
<li>这也限制了能够学习的参数的可能性（毕竟原始参数量怎么取都行）<ul>
<li>所以叫<code>Low rank</code> approximation</li>
</ul>
</li>
</ol>
<p><strong>to learn more</strong></p><p>SqueezeNet</p><ul>
<li><a href="https://arxiv.org/abs/1602.07360">https://arxiv.org/abs/1602.07360</a></li>
</ul>
<p>MobileNet</p><ul>
<li><a href="https://arxiv.org/abs/1704.04861">https://arxiv.org/abs/1704.04861</a></li>
</ul>
<p>ShuffleNet</p><ul>
<li><a href="https://arxiv.org/abs/1707.01083">https://arxiv.org/abs/1707.01083</a></li>
</ul>
<p>Xception</p><ul>
<li><a href="https://arxiv.org/abs/1610.02357">https://arxiv.org/abs/1610.02357</a></li>
</ul>
<p>GhostNet</p><ul>
<li><a href="https://arxiv.org/abs/1911.11907">https://arxiv.org/abs/1911.11907</a></li>
</ul>
<h2>Dynamic Computation</h2>
<ol>
<li>同一个网络，自己来决定计算量，比如是在不同的设备上，又或者是在同设备的不同时期（比如闲时和忙时，比如电量充足和虚电时）</li>
<li>为什么不为不同的场景准备不同的model呢？<ul>
<li>反而需要更大的存储空间，与问题起源（资源瓶颈）冲突了。</li>
</ul>
</li>
</ol>
<h3>Dynamic Depth</h3>
<p>在部分layer之后，每一层都插一个额外的layer，提前做预测和输出，由调用者根据具体情况决定需要多深的depth来产生输出。</p><p>训练的时候既要考虑网络终点的loss，还要考虑所有提前结束的layer的softmax结果，加到一起算个大的Loss</p><p>Multi-Scale Dense Network(MSDNet)</p><ul>
<li><a href="https://arxviv.org/abs/1703.09844">https://arxviv.org/abs/1703.09844</a></li>
</ul>
<h3>Dynamic Width</h3>
<p>训练的时候（同时？）对不同宽度（即神经元个数，或filter个数）进行计算（全部深度），也是把每种宽度最后产生的loss加起来当作总的Loss</p><p>在保留的宽度里，参数是一样的（所以应该就是同一轮训练里的参数了）</p><p>Slimmable Neural Networks</p><ul>
<li><a href="https://arxiv.org/abs/1812.08928">https://arxiv.org/abs/1812.08928</a></li>
</ul>
<h3>Computation based on Sample Difficulty</h3>
<p>上述决定采用什么样的network/model的是人工决定的，那么有没有让机器自己决定采用什么网络的呢？</p><p>比如一张简单的图片，几层或一层网张就能得到结果，而另一张可能前景和或背景更复杂的图片，需要很多层才能最终把特征提取出来，应用同一个模型的话就有点资源浪费了。</p><ul>
<li>SkipNet: Learning Dynamic Routing in Convolutional Networks</li>
<li>Runtime Neural Pruning</li>
<li>BlockDrop: Dynamic Inference Paths in Residual Networks</li>
</ul>
<h1>Meta Learning</h1>
<ul>
<li>学习的学习。</li>
<li>之前的machine learning，输出是明确的任务，比如是一个数字，还是一个分类；而meta-learning，输出是一个model/network，用这个model，可以去做machine learning的任务。</li>
<li>它就相当于一个“返函数的函数”</li>
<li>meta-learning 就是让机器学会去架构一个网络，初始化，学习率等等 $\leftarrow \varPhi$: <code>learnable components</code><ul>
<li>categorize meta learning based on what is learnable</li>
</ul>
</li>
</ul>
<blockquote>
<p>不再深入</p></blockquote>

            </div>
        </article>
        <div id="ga-tags">
    
    <span class="ga-tag">
        <a class="ga-highlight" href="/tag/%E6%9D%8E%E5%AE%8F%E6%AF%85/">#李宏毅</a>
    </span>
    
    <span class="ga-tag">
        <a class="ga-highlight" href="/tag/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">#机器学习</a>
    </span>
    
</div>
    </section>

    
<section id="ga-content_pager">

    <div class="next">
        <a class="ga-highlight" href="/archives/%E3%80%8ADeep-Learning-with-Python%E3%80%8B%E7%AC%94%E8%AE%B0/">《Deep-Learning-with-Python》笔记</a>
        <p class="yue">本来是打算趁这个时间好好看看花书的，前几章看下来确实觉得获益匪浅，但看下去就发现跟不上了，特别是抱着急功近利的心态的话，目前也沉不下去真的一节节吃透地往下看。这类书终归不是入门教材，是需要你有过一定的积累后再回过头来看的。于是想到了《Deep Learning with Python》，忘记这本书怎么来的了，但是在别的地方看到了有人推荐，说是Keras的作者写的非常好的一本入门书，翻了前面几十页后发现居然跟进去了，不该讲的地方没讲比如数学细节，而且思路也极其统一，从头贯穿到尾（比如representations, latent space,  hypothesis space），我觉得很受用。三百多页全英文，居然也没查几个单词就这么看完了，以前看文档最多十来页，也算一个突破了，可见其实还是一个耐心的问题。看完后书上做了很多笔记，于是顺着笔记读了第二遍，顺便就把笔记给电子化了。不是教程，不是导读。Fundamentals of deep learning</p>
    </div>


    <div class="prev">
        <a class="ga-highlight" href="/archives/%E4%B8%80%E5%BC%A0%E5%9B%BE%E8%AF%B4%E6%B8%85%E5%8C%88%E7%89%99%E5%88%A9%E7%AE%97%E6%B3%95%EF%BC%88Hungarian-Algorithm%EF%BC%89/">一张图说清匈牙利算法（Hungarian-Algorithm）</a>
        <p class="yue">做多目标跟踪的时候会碰到这个算法，每个人都有自己的说法讲清楚这个算法是干什么的？我的老师就跟我说过是什么给工人分配活干（即理解为指派问题），网上还看到有说红娘尽可能匹配多的情侣等，透过这些感性理解，基本上就能理解大概是最大匹配的问题了。然后加了限制：后来者优先。即后匹配的能抢掉前人已匹配的对象，这个是有数学依据还是只是一种实现思路我就没深究了。我的理解不会比别人更高级，之所以能用一张图说清楚，只不过是我作图的时候发现可以把过程画在一张图里，只需要把图示标清楚就好了，这样就不需要每一步画一张图了，一旦理解了，哪怕忘了，一瞅这张图也能立刻回忆起来。先上数据：import numpy as np</p>
    </div>

</section>


    
        <script>
            var initValine = function () {
                new Valine({"enable": true, "el": "#vcomments", "appId": "7tP92LoqK2cggW61DvJmWBo0-gzGzoHsz", "appKey": "iQCtrtlr8eKrQllM03GMESMJ", "visitor": true, "recordIP": true});
            }
        </script>
        <script defer src='https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js' onload="initValine()"></script>
        <div id="vcomments"></div>
    

</main>

                <footer class="ga-mono" id="ga-footer">
                    <section>
                        <span id="ga-uptime"></span>
                        <span class="brand">Maverick</span>
                    </section>
                    <section>
                        <p class="copyright">
                            <span>Copyright © 2022 AlanDecode</span>
                            <span>Powered by <a no-style href="https://github.com/AlanDecode/Maverick" target="_blank">Maverick & Galileo</a></span>
                        </p>
                        <div class="copyright">
                            <span class="footer-addon">
                                
                            </span>
                            <nav class="social-links">
                                <ul><li><a class="no-style" title="Twitter" href="https://twitter.com/walkerwzy" target="_blank"><i class="gi gi-twitter"></i>Twitter</a></li><span class="separator">·</span><li><a class="no-style" title="GitHub" href="https://github.com/walkerwzy" target="_blank"><i class="gi gi-github"></i>GitHub</a></li><span class="separator">·</span><li><a class="no-style" title="Weibo" href="https://weibo.com/1071696872" target="_blank"><i class="gi gi-weibo"></i>Weibo</a></li></ul>
                            </nav>
                        </div>
                    </section>
                    <script>
                        var site_build_date = "2019-12-06T12:00+08:00"
                    </script>
                    <script src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/galileo-7c8cea54ab.js"></script>
                </footer>
            </div>
        </div>
    </div>

    <!--katex-->
    <script defer src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/katex.min.js"></script>
    <script>
    mathOpts = {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "\\[", right: "\\]", display: true},
            {left: "$", right: "$", display: false},
            {left: "\\(", right: "\\)", display: false}
        ]
    };
    </script>
    <script defer src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/auto-render.min.js" onload="renderMathInElement(document.body, mathOpts);"></script>

    <script src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/ExSearch/jquery.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/ExSearch/ExSearch-493cb9cd89.js"></script>

    
    </body>
</html>