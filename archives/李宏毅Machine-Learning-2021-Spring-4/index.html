<!DOCTYPE HTML>
<html lang="english">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="renderer" content="webkit">
    <meta name="HandheldFriendly" content="true">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Maverick,AlanDecode,Galileo,blog" />
    <meta name="generator" content="Maverick 1.2.1" />
    <meta name="template" content="Prism" />
    <link rel="alternate" type="application/rss+xml" title="walker's code blog &raquo; RSS 2.0" href="/feed/index.xml" />
    <link rel="alternate" type="application/atom+xml" title="walker's code blog &raquo; ATOM 1.0" href="/feed/atom/index.xml" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/prism-b9d78ff38a.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/ExSearch/ExSearch-182e5a8869.css">
    <link href="https://fonts.googleapis.com/css?family=Fira+Code&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css">
    <script>
        var ExSearchConfig = {
            root: "",
            api: "https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/d69e93fb6d600dd3092b98eaebaa4bce.json"
        }

    </script>
    
<title>李宏毅Machine Learning 2021 Spring笔记[4] - walker's code blog</title>
<meta name="author" content="walker" />
<meta name="description" content="Adversarial Attack" />
<meta property="og:title" content="李宏毅Machine Learning 2021 Spring笔记[4] - walker's code blog" />
<meta property="og:description" content="Adversarial Attack" />
<meta property="og:site_name" content="walker's code blog" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/archives/%E6%9D%8E%E5%AE%8F%E6%AF%85Machine-Learning-2021-Spring-4/" />
<meta property="og:image" content="" />
<meta property="article:published_time" content="2021-10-15T21:29:00-00.00" />
<meta name="twitter:title" content="李宏毅Machine Learning 2021 Spring笔记[4] - walker's code blog" />
<meta name="twitter:description" content="Adversarial Attack" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:image" content="" />


    
</head>

<body>
    <div class="container prism-container">
        <header class="prism-header" id="prism__header">
            <h1 class="text-uppercase brand"><a class="no-link" href="/" target="_self">walker's code blog</a></h1>
            <p>coder, reader</p>
            <nav class="prism-nav"><ul><li><a class="no-link text-uppercase " href="/" target="_self">Home</a></li><li><a class="no-link text-uppercase " href="/archives/" target="_self">Archives</a></li><li><a class="no-link text-uppercase " href="/about/" target="_self">About</a></li><li><a href="#" target="_self" class="search-form-input no-link text-uppercase">Search</a></li></ul></nav>
        </header>
        <div class="prism-wrapper" id="prism__wrapper">
            
<main>
    <section class="prism-section row" id="prism__content">
        <article class="yue col-md-8 offset-md-2">
            <h1 class="prism-post-title">李宏毅Machine Learning 2021 Spring笔记[4]</h1>
            <div class="prism-post-time">
                <time class="text-uppercase">
                    October 15 2021
                </time>
            </div>
            <div class="prism-content-body">
                <h1>Adversarial Attack</h1>
<p>给你一张猫的图片，里面加入少许噪音，以保证肉眼看不出来有噪音的存在：</p><ol>
<li>期望分类器认为它不是猫</li>
<li>期望分类器认为它是一条鱼，一个键盘...</li>
</ol>
<p>比如你想要欺骗垃圾邮件过滤器</p><ul>
<li>找到一个与$x^0$非常近的向量x</li>
<li>网络正常输出y</li>
<li>真值为$\hat y$</li>
<li>$L(x) = -e(y, \hat y)$</li>
<li>$x^* = arg\underset{d(x^0, x) \leq \epsilon}{\rm min}\ L(x)$ 即要找到令损失最大的x<ol>
<li>这里L(x)我们取了反</li>
<li>$\epsilon$越小越好，指的是$x^0$要与x越接近越好（欺骗人眼）</li>
</ol>
</li>
<li>如果还期望它认成是$y^{target}$，那就再加上与其的的损失</li>
<li>$L(x) = -e(y, \hat y) + e(y, y^{target})$</li>
<li>注意两个error是反的，一个要求越远越好(真值），一个要求越近越好（target)</li>
</ul>
<p>怎么计算$d(x^0, x) \leq \epsilon$呢？</p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/52ea0aa0520c35787d94019e0efc61a7.png" alt=""/></figure><p>图上可知，如果都改变一点点，和某一个区域改动相当大，可能在L2-norm的方式计算出来是一样的，但是在L-infinity看来是不一样的（它只关心最大的变动）。</p><p>显然L-infinity更适合人眼的逻辑，全部一起微调人眼不能察觉，单单某一块大调，人眼是肯定可以看出来的。</p><p>而如果是语音的话，可能耳朵对突然某个声音的变化反而不敏感，整体语音风格变了却能立刻认出说话的人声音变了，这就要改变方案了。</p><h2>Attack Approach</h2>
<p>如何得到这个x呢？其实就是上面的损失函数。以前我们是为了train权重，现在train的就是x本身了。</p><ol>
<li>损失达到我们的要求 （有可能这时候与原x相关很远）</li>
<li>与原x的距离达到我们的要求, 怎么做？<ul>
<li>其实就是以$x^0$为中心，边长为$2\epsilon$的矩形才是期望区域</li>
<li>如果update后，$x^t$仍然落在矩形外，那么就在矩形里找一个离它最近的点，当作本轮更新后的$x^t$，进入下一轮迭代</li>
</ul>
</li>
</ol>
<p>Fast Gradient Sign Method(FGSM): <a href="https://arxiv.org/abs/1412.6572">https://arxiv.org/abs/1412.6572</a></p><ul>
<li>相比上面的迭代方法，FGSM只做一次更新</li>
<li>就是根据梯度，判断是正还是负，然后把原x进行一次加减$\epsilon$的操作（其实等于是落在了矩形的四个点上）</li>
<li>也就是说它直接取了四个点之一作为$x^0$</li>
</ul>
<h2>White Box v.s. Black Box</h2>
<p>讲上述方法的时候肯定都在疑惑，分类器是别人的，我怎么可能拿到别人的模型来训练我的攻击器？ -&gt; <strong>White Box Attack</strong></p><p>那么<code>Black Box Attack</code>是怎么实现的呢？</p><ol>
<li>如果我们知道对方的模型是用什么数据训练的话，我们也可以训练一个类似的(proxy network)<ul>
<li>很大概率都是用公开数据集训练的</li>
</ul>
</li>
<li>如果不知道的话呢？就只能尝试地丢一些数据进去，观察（记录）它的输出，然后再用这些测试的输入输出来训练自己的proxy network了。</li>
</ol>
<ul>
<li>one pixel attack<ul>
<li><a href="https://arxiv.org/abs/1710.08864">https://arxiv.org/abs/1710.08864</a></li>
<li><a href="https://youtu.be/tfpKIZIWidA">https://youtu.be/tfpKIZIWidA</a></li>
</ul>
</li>
<li>universal adversarial attack<ul>
<li>万能noise</li>
<li><a href="https://arxiv.org/abs/1610.08401">https://arxiv.org/abs/1610.08401</a></li>
</ul>
</li>
<li>声音</li>
<li>文本</li>
<li>物理世界<ul>
<li>比如欺骗人脸识别系统，去认成另一个人</li>
<li>又比如道路环境，车牌识别等，也可以被攻击</li>
<li>要考虑摄像头能识别的分辨率</li>
<li>要考虑训练时候用的图片颜色与真实世界颜色不一致的问题</li>
</ul>
</li>
<li>Adversarial Reprogramming</li>
<li>Backdoor in Model<ul>
<li>attack happens at the training phase</li>
<li><a href="https://arxiv.org/abs/1804.00792">https://arxiv.org/abs/1804.00792</a></li>
<li>be careful of unknown dataset...</li>
</ul>
</li>
</ul>
<h2>Defence</h2>
<h3>Passive Defense（被动防御）</h3>
<p>进入network前加一层filter</p><ul>
<li>稍微模糊化一点，就去除掉精心设计的noise了<ul>
<li>但是同时也影响了正常的图像</li>
</ul>
</li>
<li>对原图进行压缩</li>
<li>把输入用Generator重新生成一遍</li>
</ul>
<p>如果攻击都知道你怎么做了，其实很好破解，就把你的filter当作network的一部分重新开始设计noise，所以可以选择加入随机选择的一些预处理(让攻击者不可能针对性地训练)：</p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/c5baa988904325835254f3ac7a7ee4e1.png" alt=""/></figure><h3>Proactive Defense（主动防御）</h3>
<p>训练的时候就训练比较不容易被攻破的模型。比如训练过程中加入noise，把生成的结果重新标注回真值。</p><ul>
<li>training model</li>
<li>find the problem</li>
<li>fix it</li>
</ul>
<p>有点类似于<code>Data Augmentation</code></p><p>仍然阻挡不了新的攻击算法，即你对数据进行augment之外的范围。</p><h1>Explainable Machine Learning(可解释性)</h1>
<ul>
<li>correct answers $\neq$ intelligent</li>
<li>很多行业会要求结果必须可解释<ul>
<li>银行，医药，法律，驾驶....</li>
</ul>
</li>
</ul>
<p><strong>Local Explanation</strong></p><p>Why do you thing <strong>this image</strong> is a cat?</p><p><strong>Global Explanation</strong></p><p>What does a &quot;<strong>cat</strong>&quot; look like?</p><ol>
<li>遮挡或改变输入的某些部分，观察对已知输出的影响<ul>
<li>（比如拦到某些部分确实认不出图像是一条狗了）</li>
</ul>
</li>
<li>遮挡或改变输入的某些部分，把两种输出做loss，对比输入变化与loss变化：<ul>
<li>$|\frac{\varDelta e}{\varDelta x}| \rightarrow \frac{\partial e}{\partial x_n}$</li>
</ul>
</li>
</ol>
<p>把上述（任一种）每个部分（像素，单词）的影响结果输出，就是：<code>Saliency Map</code></p><h2>Saliency Map</h2>
<figure  style="flex: 271.9298245614035" ><img width="1240" height="228" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/fdcce76fc7337b0ee787724913f998f0.png" alt=""/></figure><p>图1，2就是为了分辨宝可梦和数码宝贝，人类一般很难区分出来，但机器居然轻松达到了98%的准确率，经过绘制<code>Saliency Map</code>，发现居然就是图片素材（格式）的原因，一个是png，一个是jpg，造成背景一个是透明一个是不透明的。</p><p>也就是说，能发现机器判断的依据不是我们关注的本体（高亮部分就是影响最大的部分，期望是在动物身上）</p><p>第三张图更可笑，机器是如何判断这是一只马的？居然也不是马的本体，而是左下角，标识图片出处的文字，可能是训练过程中同样的logo过多，造成了这个“人为特征”。</p><p>解决方案：</p><h3>Smooth Gradient</h3>
<p>随机给输入图片加入噪点，得到saliency map（们），然后取平均</p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/e672962530cdb4c88d616b3fa82f73f8.png" alt=""/></figure><h3>Integrated gradient(IG)</h3>
<p>一个特征在从无到有的阶段，梯度还是明显的，但是到了一定程度，特征再增强，对gradient影响也不大了，比如从片子来判断大象，到了一定长度，一张图也不会“更像大象”</p><p>一种思路：<a href="https://arxiv.org/abs/1611.02639">https://arxiv.org/abs/1611.02639</a></p><h2>global explaination</h2>
<p><strong>What does a filter detect?</strong></p><p>如果经过某层（训练好的）filter，得到的feature map一些位置的值特别大，那说明这个filter提取的就是这类特征/patten。</p><p>我们去&quot;创造&quot;一张包含了这种patten的图片：$X^* = arg\ \underset{X}{\rm max}\sum_i\sum_j a_{ij}$，即这个图片是“训练/learn“出来的，通过找让X的每个元素($a_{ij}$)在被filter乘加后结果最大的方式。 -&gt; <code>gradient ascent</code></p><p>然后再去观察$X^*$有什么特征，就基本上可以认定这个（训练好的）filter提取的是什么样的patten了。
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/33b78193d5c74e4d146b9efab631c932.png" alt=""/></figure></p><blockquote>
<p><code>adversarial attack</code> 类似的原理，但这是对单filter而言。如果你想用同样的思路去让输出y越大越好，得到X，看X是什么，得到的X大概率都是一堆噪音。如果能生成图像，那是<code>GAN</code>的范畴了。</p></blockquote>
<p>于是，尝试再加一个限制，即不但要让y最大，还要让X看起来最有可能像一个数字：</p><ul>
<li>$R(X)$: how likely X is a digit</li>
<li>$X^* = arg\ \underset{X}{\rm max}y_i + \color{red}{R(X)}$</li>
<li>$R(X) = -\sum_{i,j}|X_{i,j}|$ 比如这个规则，期望每个像素越黑越好</li>
</ul>
<h1>Domain Adaptation</h1>
<p><code>Transfer Learning</code>的一种，在训练数据集和实际使用的数据集不一样的时候。 <a href="https://youtu.be/qD6iD4TFsdQ">https://youtu.be/qD6iD4TFsdQ</a></p><p>需要你对<code>target domain</code>的数据集有一定的了解。</p><p>有一种比较好的情况就是，target domain既有数据，还有标注（但不是太多，如果太多的话就不需要<code>source domain</code>了，直接用target来训练就好了），那就像bert一样，去<code>fine tune</code>结果，要注意的是标本量过小，可能很容易<code>overfitting</code>.</p><p>如果target doamin有<strong>大量</strong>资料，但是没有标注呢？</p><h2>Domain Adversarial Training</h2>
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/7b419953f55daf61a0a4d3f2e77b156d.png" alt=""/></figure><ul>
<li>把source domain的network分为特征提取器（取多少层cnn可以视为超参，并不一定要取所有层cnn）和分类器</li>
<li>然后在特征取层之后跟另一个分类器，用来判断图像来自于source还是target（有点像<code>Discriminator</code></li>
<li>与真值有一个loss，source, target之间也有一个loss，要求找到这样的参数组分别让两个loss最小</li>
<li>loos和也应该最小（图中用的是减，但其实$L_d$的期望是趋近于0，不管是正还是负都是期望越小越好）（不如加个绝对值？）</li>
<li>每一小块都有一组参数，是一起训练的</li>
<li>目的就是既要逼近训练集的真值，还要训练出一个网络能模糊掉source和target数据集的差别</li>
</ul>
<h3>Limit</h3>
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/1c785d92413a44f14ca579564bd3a107.png" alt=""/></figure><p>如果target数据集如上图左，显然结果是会比上图右要差一点的，也就是说尽量要保持同分布。在这里用了另一个角度，就是让数据<strong>离boundary越远越好</strong></p><ul>
<li>Decision-boundary Iterative Refinement Training with a Teacher(<code>DIRT-T</code>)<ul>
<li><a href="https://arxiv.org/abs/1802.08735">https://arxiv.org/abs/1802.08735</a></li>
</ul>
</li>
<li>Maximum Classifier Discrepancy <a href="https://arxiv.org/abs/1712.02560">https://arxiv.org/abs/1712.02560</a></li>
</ul>
<h2>More</h2>
<ul>
<li>如果source 和 target 里的类别不完全一样呢？<ul>
<li>Universal domain adaptation</li>
</ul>
</li>
<li>如果target既没有label，数据量也非常少（比如就一张）呢？<ul>
<li>Test Time Training(TTT) <a href="https://arxiv.org/abs/1909.13231">https://arxiv.org/abs/1909.13231</a></li>
</ul>
</li>
</ul>
<p><strong>Domain Generalization</strong>
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/029818e3d225d57c9b9a866b569983b5.png" alt=""/></figure></p><h1>Deep Reinforcement Learning (RL)</h1>
<ul>
<li><strong>Environment</strong> 给你 <code>Observation</code></li>
<li><strong>Actor</strong> 接收入 <code>Observation</code>, 输出 <code>Action</code></li>
<li><code>Action</code> 反馈给 <strong>Environment</strong>, 计算出 <code>Reward</code> 反馈给 <strong>Actor</strong></li>
<li>要求 <code>Reward</code> 最大</li>
</ul>
<p>与 GAN 的不同之处，不管是生成器还是判别器，都是一个network，而RL里面，Actor和Reward都是黑盒子，你只能看到结果。</p><h2>Policy Gradient</h2>
<p><a href="https://youtu.be/W8XF3ME8G2I">https://youtu.be/W8XF3ME8G2I</a></p><ol>
<li>先是用很类似监督学习的思路，给每一步的最优（或最差）方案一个label，有label就能做loss。先把它变成一个二分类的问题。</li>
<li>打分还可以不仅仅是“好”或“不好”，还可以是一个程度，比如1.5比0.5的“支持”力度要大一些，而-10显然意味着你千万不要这么做，非常拒绝。</li>
<li>比如某一步，可以有三种走法，可以用onehot来表示，其中一种走法可以是[1,0,0]$^T$，表示期望的走法是第一种。</li>
<li>但是也可以是[-1,0,0]$^T$，标识这种走法是不建议的</li>
<li>也可以是[3.5,0,0]$^T$等</li>
<li>后面会用<code>1, -1, 10, 3.5</code>这样的scalar来表示，但要记住其实它们是ont-hot中的那个非零数。</li>
</ol>
<p>现实世界中很多场景不可能执行完一步后就获得reward，或者是全局最佳的reward（比如下围棋）。</p><p><strong>v1</strong></p><p>一种思路是，每一步之后，把游戏/棋局进行完，把当前reward和后续所有步骤的reward加一起做reward -&gt; <code>cumulated reward</code> $\rightarrow G_t = \sum_{n=t}^Nr_n$</p><p><strong>v2</strong></p><p>这种思路仍然有问题，游戏步骤越长，当前步对最终步的影响越小。因此引入一个小于1的权重$\gamma &lt; 1$: $G_1' = r_1 + \gamma r_2 + \gamma^2r_3 + \cdots$</p><p>这样越远的权重越小： $G_t' = \sum_{n=t}^N \color{red}{\gamma^{n-t}} r_n$</p><blockquote>
<p>注意，目前得到的<code>G</code>就是为了给每一次对observation进行的action做loss的对象。</p></blockquote>
<p><strong>v3</strong></p><p>标准化reward。你有10分，是高是低？如果所有人都是20分，那就是低分，所以与G做对比的时候，通常要减去一个合适的值<code>b</code>，让得分的分布有正有负。</p><p><strong>Policy Gradient</strong></p><p>普通的gradient descent是搜集一遍数据，就可以跑for循环了，而PG不行，你每次得到梯度后，要重采一遍样，其实也很好理解，你下了某一步，经过后续50步后，输了，你的下一轮测试应该是下一盘随机的棋，而不是把更新好的参数再用到同一盘棋去。</p><p>还是不怎么好理解，至少要知道，我做参数是不为了训练出这一盘棋是怎么下出来的，而是根据这个（大多是输了的）结果，以及学到的梯度，去下一盘新的棋试试。</p><h2>Actor Critic</h2>
<p><strong>Critic</strong>:</p><ul>
<li>Given <code>actor</code> $\theta$, how good it is when <code>observing</code> s (and taking action a)</li>
</ul>
<p><strong>Value function</strong> $V^\theta(s)$:</p><ul>
<li>使用actor $\theta$的时候，预测会得到多少的<code>cumulated reward</code></li>
<li>分高分低其实还是取决于actor，同样的局面，不同的actor肯定拿的分不同。</li>
</ul>
<h3>Monte-Carlo based approach (MC)</h3>
<p>蒙特卡洛搜索，正常把游戏玩完，得到相应的G.</p><h3>Temporal-difference approach (TD)</h3>
<p>不用玩完整个游戏，就用前后时间段的数据来得到输出。
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/de79fef0ebceeda7cd5011e38ab28c46.png" alt=""/></figure></p><p>关键词：</p><ul>
<li>我们既不知道v(t+1)，也不知道v(t)，但确实能知道<code>v(t+1)-v(t)</code>.</li>
</ul>
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/1fbf32aed94a0b3f47d9b124a466874c.png" alt=""/></figure><p>这个例子没看懂，后面七次游戏为什么没有sa了？</p><p><strong>v3.5</strong></p><p>上文提到的V可以用来作更早提到的b:</p><ul>
<li>${S_t, a_t}\ A_t = G_t' - V^\theta(S_t)$</li>
<li>回顾一下，$V^\theta(S_t)$是看到某个游戏画面时算出来的reward</li>
<li>它包含$S_t$状态下，后续各种步骤的reward的平均值</li>
<li>而$G_t'$则是这一步下的rewared</li>
<li>两个数相减其实就是看你的这一步是比平均水平好还是差</li>
<li>比如你得到了个负值，代表在当前场景下，这个actor执行的步骤是低于平均胜率的，需要换一种走法。</li>
</ul>
<p><strong>v4</strong></p><p>3.5版下，G只有一个样本（一次游戏）的结果，这个版本里，把st再走一步，试难$S_{t+1}$的各种走法下reward的平均值，用它来替换G'，而它的值，就是当前的reward加上t+1时刻的V:</p><ul>
<li>$r_t + V^\theta(S_{t+1}) - V^\theta(S_t)$</li>
</ul>
<p>这就是：</p><h3>Advantage Actor-Critic</h3>
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/a038f8d6237e4d33a943099bdbfdefc3.png" alt=""/></figure><p>就看图而言，感觉就是坚持这一步走完，后续所有可能的rewawrd， 减去， 从这一步开始就试验所有走法的reward</p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/82541f55c2db06d3b1e8339d6e63d140.png" alt=""/></figure><p>More:</p><p>Deep Q Network (DQN)</p><ul>
<li><a href="https://arxiv.org/abs/1710.02298">https://arxiv.org/abs/1710.02298</a></li>
<li><a href="https://youtu.be/o_g9JUMw1Oc">https://youtu.be/o_g9JUMw1Oc</a></li>
<li><a href="https://youtu.be/2-zGCx4iv_k">https://youtu.be/2-zGCx4iv_k</a></li>
</ul>
<h2>Reward Shaping</h2>
<p>前面说过很多场景要得到reward非常困难（时间长，步骤长，或根本不会结束），这样的情况叫<code>sparse reward</code>，人类可以利用一些已知知识去人为设置一些reward以增强或削弱机器的某些行为。</p><p>比如游戏：</p><ol>
<li>原地不动一直慢慢减分</li>
<li>每多活一秒也慢慢减分（迫使你去获得更高的reward, 避免学到根本就不去战斗的方式）</li>
<li>每掉一次血也减分</li>
<li>每杀一个敌人就加分</li>
<li>以此类推，这样就不至于要等到一场比赛结束才有“一个”reward</li>
</ol>
<p>又比如训练机械手把一块有洞的木板套到一根棍子上：</p><ol>
<li>离棍子越近，就有一定的加分</li>
<li>其它有助于套进去的规则</li>
</ol>
<p>还可以给机器加上<strong>好奇心</strong>，让机器看到有用的“新的东西”也加分。</p><h2>No Reward, learn from demostration</h2>
<p>只有游戏场景才会有明确的reward，大多数现实场景都是没有reward的，比如训练自动驾驶的车，或者太过死板的reward既不能适应变化，也容易被打出漏洞，比如机器人三定律里，机器人不能伤害人类，却没有禁止囚禁人类，又比如摆放盘子，却没有给出力度，等盘子摔碎了，再去补一条𢱨碎盘子就负reward的规则，也晚了，由此引入模仿学习：</p><h3>Imitation Learning</h3>
<p>略</p><h1>Life-Long Learning</h1>
<p>持续学习，机器学习到一个模型后，继续学下一个模型（任务）。</p><ol>
<li>为什么不一个任务学一个模型<ul>
<li>不可能去存储所有的模型</li>
<li>一个任务的知识不能转移到另一个任务</li>
</ul>
</li>
<li>为什么不直接用迁移学习（迁移学习只关注迁移后的新任务）</li>
</ol>
<h2>Research Directions</h2>
<h3>Selective Synaptic Plasticity</h3>
<p>选择性的神经突触的可塑性？（Regularization-based Approach）</p><p><strong>Catastrophic Forgetting</strong> 灾难性的遗忘
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/207da40b7ff67f6534e5fc5de208cce9.png" alt=""/></figure></p><p>在任务1上学到的参数，到任务2里接着训练，顺着梯度到了任务2的最优参数，显然不再是任务1的做以参，这叫灾难性的遗忘</p><p>一种思路：</p><p>任务2里梯度要更新未必要往中心，也可以往中下方，这样既在任务2的低loss区域，也没有跑出任务1的低loss区域，实现的方式是找到对之前任务影响比较小的参数，主要去更新那些参数。比如上图中，显然$\theta_1$对任务1的loss影响越小，但是更新它之后会显著影响任务2的loss，而$\theta_2$的改动才是造成任务1loss变大的元凶。</p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/1b27e4cf8478e749d8899d30b75bbb22.png" alt=""/></figure><p>Elastic Weight Consolidation(EWC)</p><ul>
<li><a href="https://arxiv.org/abs/1612.00796">https://arxiv.org/abs/1612.00796</a></li>
</ul>
<p>Synaptic Intelligence(SI)</p><ul>
<li><a href="https://arxiv.org/abs/1703.04200">https://arxiv.org/abs/1703.04200</a></li>
</ul>
<p>Memory Aware Synapses(MAS)</p><ul>
<li><a href="https://arxiv.org/abs/1711.09601">https://arxiv.org/abs/1711.09601</a></li>
</ul>
<p>RWalk</p><ul>
<li><a href="https://arxiv.org/abs/1801.10112">https://arxiv.org/abs/1801.10112</a></li>
</ul>
<p>Sliced Cramer Preservation(SCP)</p><ul>
<li><a href="https://openreview.net/forum?id=BJge3TNKwH">https://openreview.net/forum?id=BJge3TNKwH</a></li>
</ul>
<figure  style="flex: 132.76231263383298" ><img width="1240" height="467" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/474bbac22bf3778191de7bf09d10d69d.png" alt=""/></figure><h3>Memory Reply</h3>
<ol>
<li>在训练task1的时候，同时训练一个相应的generator</li>
<li>训练task2的时候，用task1的generator生成pseudo-data，一起来训练生成新的model</li>
<li>同时也训练出一个task1&amp;2的generator</li>
<li>...</li>
</ol>
<h1>Network Compress</h1>
<h2>pruning (剪枝)</h2>
<p>Networks ar typically over-parameterized (there is significant redundant weights or neurons)</p><ul>
<li>可以看哪些参数通常比较大，或值的变化不影响loss（梯度小）-&gt; 权重，为0的次数少 -&gt; 神经元 等等</li>
<li>剪枝后精度肯定是会下降的</li>
<li>需要接着fine-tune</li>
<li>一次不要prune to much</li>
<li>剪参数和剪神经元效果是不一样的<ul>
<li>剪参数会影响矩阵运算，继而影响GPU加速</li>
</ul>
</li>
</ul>
<p>那么为什么不直接train一个小的network呢？</p><ul>
<li>小的network通常很难train到同样的准确率。 （大乐透假说）</li>
</ul>
<h2>Knowledge Distillation (知识蒸馏)</h2>
<p>老师模型训练出来的结果，用学生模型（小模型）去模拟（即是模拟整个输出，而不是模拟分类结果），让小模型能达到大模型同样的结果。</p><p>一般还会在输出的softmax里面加上温度参数（即平滑输出，不同大小的数除一个大于1的数，显然越大被缩小的倍数也越大，比如100/10=10，少了90，10/10=1, 只少了9，差别也从90变成了9）(或者兴趣个极端的例子，T取无穷大，那么每个输出就基本相等了)</p><h2>Parameter Quantization</h2>
<ol>
<li>Using less bits to represent a value</li>
<li>Weight clustering<ul>
<li>把weights分成预先确定好的簇（或根据分布来确定）</li>
<li>对每簇取均值，用均值代替整个簇里所有的值</li>
</ul>
</li>
<li>represent frequent clusters by less bits, represent rare clusters by more bits<ul>
<li>Huffman encoding</li>
</ul>
</li>
</ol>
<p>极限，<code>Binary Weights</code>，用两个bits来描述整个网络，扩展阅读。</p><h2>Depthwise Separable Convolution</h2>
<p>回顾下CNN的机制，参数量是：</p><ul>
<li>卷积核的大小 x 输入图像的通道数 x 输出的通道数</li>
<li>($k\times k$) x in_channel x out_channel</li>
</ul>
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/769ea8db15464e205b9c674747eb650d.png" alt=""/></figure><p>而<code>Depthwise Separable Convolution</code>由两个卷积组成：</p><ol>
<li>Depthwise Convolution<ul>
<li>很多人对CNN的误解刚好就是Depthwise Convolution的样子，即一个卷积核对应一个输入的channel（事实上是一组卷积核对应所有的输入channel）</li>
<li>因此它的参数个数 k x k x in_channel</li>
</ul>
</li>
<li>PointWise Convolution<ul>
<li>这里是为了补上通道与通道这间的关系</li>
<li>于是用了一个1x1的<code>标准</code>卷积（即每一组卷积核对应输入的所有通道）</li>
<li>输出channel也由这次卷积决定</li>
<li>应用标准卷积参数量：(1x1) x in_channel x out_channel</li>
</ul>
</li>
</ol>
<figure  style="flex: 130.52631578947367" ><img width="1240" height="475" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/e147af5889954aa781498c5977f659f1.png" alt=""/></figure><p>两个参数量做对比, 设<code>in_channel = I</code>, <code>out_channel = O</code></p><ol>
<li>$p_1 = (k\times k) \times I \times O$</li>
<li>$p_2 = (k\times k) \times I + (1\times 1) \times I \times O = (k\times k) \times I + I \times O$</li>
<li>$\frac{p_2}{p_1} = \frac{I\cdot(k^2 + O)}{I\cdot{k^2\cdot O}}</li>
</ol>
<p>= \frac{1}{O} + \frac{1}{k^2} \approx \frac{1}{k^2} 
$</p><p>O代表out_channel，大型网络里256，512比比皆是，所以它可以忽略，那么前后参数量就由$k^2$决定了，如果是大小为3的卷积核，参数量就变成1/9了，已经是压缩得很可观了。</p><h3>Low rank approximation</h3>
<p>上面是应用，原理就是<code>Low rank approximation</code></p><p>以全连接网络举例</p><ol>
<li>如果一个一层的网络，输入<code>N</code>， 输出<code>M</code>，参数为<code>W</code>，那么参数量是<code>MxN</code></li>
<li>中间插入一个线性层<code>K</code>，<ul>
<li>参数变成：<code>V</code>:N-&gt;K, <code>U</code>:K-&gt;M,</li>
<li>参数量：<code>NxK</code> + <code>KxM</code></li>
</ul>
</li>
<li>只要K远小于M和N（比如数量级都不一致），那么参数量是比直接MxN要小很多的</li>
<li>这也限制了能够学习的参数的可能性（毕竟原始参数量怎么取都行）<ul>
<li>所以叫<code>Low rank</code> approximation</li>
</ul>
</li>
</ol>
<p><strong>to learn more</strong></p><p>SqueezeNet</p><ul>
<li><a href="https://arxiv.org/abs/1602.07360">https://arxiv.org/abs/1602.07360</a></li>
</ul>
<p>MobileNet</p><ul>
<li><a href="https://arxiv.org/abs/1704.04861">https://arxiv.org/abs/1704.04861</a></li>
</ul>
<p>ShuffleNet</p><ul>
<li><a href="https://arxiv.org/abs/1707.01083">https://arxiv.org/abs/1707.01083</a></li>
</ul>
<p>Xception</p><ul>
<li><a href="https://arxiv.org/abs/1610.02357">https://arxiv.org/abs/1610.02357</a></li>
</ul>
<p>GhostNet</p><ul>
<li><a href="https://arxiv.org/abs/1911.11907">https://arxiv.org/abs/1911.11907</a></li>
</ul>
<h2>Dynamic Computation</h2>
<ol>
<li>同一个网络，自己来决定计算量，比如是在不同的设备上，又或者是在同设备的不同时期（比如闲时和忙时，比如电量充足和虚电时）</li>
<li>为什么不为不同的场景准备不同的model呢？<ul>
<li>反而需要更大的存储空间，与问题起源（资源瓶颈）冲突了。</li>
</ul>
</li>
</ol>
<h3>Dynamic Depth</h3>
<p>在部分layer之后，每一层都插一个额外的layer，提前做预测和输出，由调用者根据具体情况决定需要多深的depth来产生输出。</p><p>训练的时候既要考虑网络终点的loss，还要考虑所有提前结束的layer的softmax结果，加到一起算个大的Loss</p><p>Multi-Scale Dense Network(MSDNet)</p><ul>
<li><a href="https://arxviv.org/abs/1703.09844">https://arxviv.org/abs/1703.09844</a></li>
</ul>
<h3>Dynamic Width</h3>
<p>训练的时候（同时？）对不同宽度（即神经元个数，或filter个数）进行计算（全部深度），也是把每种宽度最后产生的loss加起来当作总的Loss</p><p>在保留的宽度里，参数是一样的（所以应该就是同一轮训练里的参数了）</p><p>Slimmable Neural Networks</p><ul>
<li><a href="https://arxiv.org/abs/1812.08928">https://arxiv.org/abs/1812.08928</a></li>
</ul>
<h3>Computation based on Sample Difficulty</h3>
<p>上述决定采用什么样的network/model的是人工决定的，那么有没有让机器自己决定采用什么网络的呢？</p><p>比如一张简单的图片，几层或一层网张就能得到结果，而另一张可能前景和或背景更复杂的图片，需要很多层才能最终把特征提取出来，应用同一个模型的话就有点资源浪费了。</p><ul>
<li>SkipNet: Learning Dynamic Routing in Convolutional Networks</li>
<li>Runtime Neural Pruning</li>
<li>BlockDrop: Dynamic Inference Paths in Residual Networks</li>
</ul>
<h1>Meta Learning</h1>
<ul>
<li>学习的学习。</li>
<li>之前的machine learning，输出是明确的任务，比如是一个数字，还是一个分类；而meta-learning，输出是一个model/network，用这个model，可以去做machine learning的任务。</li>
<li>它就相当于一个“返函数的函数”</li>
<li>meta-learning 就是让机器学会去架构一个网络，初始化，学习率等等 $\leftarrow \varPhi$: <code>learnable components</code><ul>
<li>categorize meta learning based on what is learnable</li>
</ul>
</li>
</ul>
<blockquote>
<p>不再深入</p></blockquote>

            </div>
        </article>
        <div class="prism-post-meta col-md-8 offset-md-2">
    <span>walker</span>
    
    <span>/</span>
    <span>
        <a class="category no-link" href="/category/posts/" target="_self">
        posts
        </a>
    </span>
    
    
    <span>/</span>
    
    <span class="prism-tag">
        <a class="no-link" href="/tag/%E6%9D%8E%E5%AE%8F%E6%AF%85/" target="_self">#李宏毅</a>
    </span>
    
    <span class="prism-tag">
        <a class="no-link" href="/tag/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" target="_self">#机器学习</a>
    </span>
    
    <span class="prism-tag">
        <a class="no-link" href="/tag/Adversarial%20Attack/" target="_self">#Adversarial Attack</a>
    </span>
    
    <span class="prism-tag">
        <a class="no-link" href="/tag/Imitation%20Learning/" target="_self">#Imitation Learning</a>
    </span>
    
    <span class="prism-tag">
        <a class="no-link" href="/tag/Life-Long%20Learning/" target="_self">#Life-Long Learning</a>
    </span>
    
    <span class="prism-tag">
        <a class="no-link" href="/tag/Meta%20Learning/" target="_self">#Meta Learning</a>
    </span>
    
    <span class="prism-tag">
        <a class="no-link" href="/tag/Knowledge%20Distillation/" target="_self">#Knowledge Distillation</a>
    </span>
    
    
    
    <span>/</span>
    <span class="leancloud_visitors" id="/archives/%E6%9D%8E%E5%AE%8F%E6%AF%85Machine-Learning-2021-Spring-4/" data-flag-title="李宏毅Machine Learning 2021 Spring笔记[4]"><span class="leancloud-visitors-count"></span> Views</span>
    
</div>
    </section>

    
<section id="prism__pagination" class="prism-pagination" class="col-md-8 offset-md-2">
    <ul>
        
        <li class="next">
            <a class="no-link" href="/archives/%E4%B8%80%E5%BC%A0%E5%9B%BE%E8%AF%B4%E6%B8%85%E5%8C%88%E7%89%99%E5%88%A9%E7%AE%97%E6%B3%95%EF%BC%88Hungarian-Algorithm%EF%BC%89/" target="_self" title="一张图说清匈牙利算法（Hungarian-Algorithm）"><i class="fa fa-chevron-left" aria-hidden="true"></i>Newer</a>
        </li>
        
        
        <li class="prev">
            <a class="no-link" href="/archives/%E6%9D%8E%E5%AE%8F%E6%AF%85Machine-Learning-2021-Spring-3/" target="_self" title="李宏毅Machine Learning 2021 Spring笔记[3]">Older<i class="fa fa-chevron-right" aria-hidden="true"></i></a>
        </li>
        
    </ul>
</section>


    
    <script>
        var initValine = function() {
            new Valine({"enable": true, "el": "#vcomments", "appId": "7tP92LoqK2cggW61DvJmWBo0-gzGzoHsz", "appKey": "iQCtrtlr8eKrQllM03GMESMJ", "visitor": true, "recordIP": true});
        }

    </script>
    <script defer src='https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js' onload="initValine()"></script>
    <div class="prism-comment-section container" id="prism__comment">
        <div class="row">
            <div class="col-md-8 offset-md-2">
                <div id="vcomments"></div>
            </div>
        </div>
    </div>
    

</main>

            <footer id="prism__footer">
                <section>
                    <div>
                        <nav class="social-links">
                            <ul><li><a class="no-link" title="Twitter" href="https://twitter.com/walkerwzy" target="_blank" rel="noopener noreferrer nofollow"><i class="gi gi-twitter"></i></a></li><li><a class="no-link" title="GitHub" href="https://github.com/walkerwzy" target="_blank" rel="noopener noreferrer nofollow"><i class="gi gi-github"></i></a></li><li><a class="no-link" title="Weibo" href="https://weibo.com/1071696872" target="_blank" rel="noopener noreferrer nofollow"><i class="gi gi-weibo"></i></a></li></ul>
                        </nav>
                    </div>

                    <section id="prism__external_links">
                        <ul>
                            
                            <li>
                                <a class="no-link" target="_blank" href="https://github.com/AlanDecode/Maverick" rel="noopener noreferrer nofollow">Maverick</a>：🏄‍ Go My Own Way.
                                <span>|</span>
                            </li>
                            
                            <li>
                                <a class="no-link" target="_blank" href="https://www.imalan.cn" rel="noopener noreferrer nofollow">Triple NULL</a>：Home page for AlanDecode.
                                <span>|</span>
                            </li>
                            
                        </ul>
                    </section>

                    <div class="copyright">
                        <p class="copyright-text">
                            <span class="brand">walker's code blog</span>
                            <span>Copyright © 2022 AlanDecode</span>
                        </p>
                        <p class="copyright-text powered-by">
                            | Powered by <a href="https://github.com/AlanDecode/Maverick" class="no-link" target="_blank" rel="noopener noreferrer nofollow">Maverick</a> | Theme <a href="https://github.com/Reedo0910/Maverick-Theme-Prism" target="_blank" class="no-link" rel="noopener noreferrer nofollow">Prism</a>
                        </p>
                    </div>
                    <div class="footer-addon">
                        
                    </div>
                </section>
                <script>
                    var site_build_date = "2019-12-06T12:00+08:00"

                </script>
                <script src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/prism-efa8685153.js"></script>
            </footer>
        </div>
    </div>
    </div>

    <script src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/ExSearch/jquery.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/ExSearch/ExSearch-493cb9cd89.js"></script>

    <!--katex-->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/katex.min.js"></script>
    <script>
        mathOpts = {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "\\[", right: "\\]", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false }
            ]
        };

    </script>
    <script defer src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/auto-render.min.js" onload="renderMathInElement(document.body, mathOpts);"></script>

    
</body>

</html>