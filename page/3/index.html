<!DOCTYPE HTML>
<html lang="english">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="renderer" content="webkit">
    <meta name="HandheldFriendly" content="true">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Maverick,AlanDecode,Galileo,blog" />
    <meta name="generator" content="Maverick 1.2.1" />
    <meta name="template" content="Prism" />
    <link rel="alternate" type="application/rss+xml" title="walker's code blog &raquo; RSS 2.0" href="/feed/index.xml" />
    <link rel="alternate" type="application/atom+xml" title="walker's code blog &raquo; ATOM 1.0" href="/feed/atom/index.xml" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/prism-b9d78ff38a.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/ExSearch/ExSearch-182e5a8869.css">
    <link href="https://fonts.googleapis.com/css?family=Fira+Code&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css">
    <script>
        var ExSearchConfig = {
            root: "",
            api: "https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/4089f9a071a74d90ba780992e2fc0383.json"
        }

    </script>
    
<title>walker's code blog</title>
<meta name="author" content="AlanDecode" />
<meta name="description" content="coder, reader" />
<meta property="og:title" content="walker's code blog" />
<meta property="og:description" content="coder, reader" />
<meta property="og:site_name" content="walker's code blog" />
<meta property="og:type" content="website" />
<meta property="og:url" content="/page/3/" />
<meta property="og:image" content="walker's code blog" />
<meta name="twitter:title" content="walker's code blog" />
<meta name="twitter:description" content="coder, reader" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/android-chrome-512x512.png" />


    
</head>

<body>
    <div class="container prism-container">
        <header class="prism-header" id="prism__header">
            <h1 class="text-uppercase brand"><a class="no-link" href="/" target="_self">walker's code blog</a></h1>
            <p>coder, reader</p>
            <nav class="prism-nav"><ul><li><a class="no-link text-uppercase " href="/" target="_self">Home</a></li><li><a class="no-link text-uppercase " href="/archives/" target="_self">Archives</a></li><li><a class="no-link text-uppercase " href="/about/" target="_self">About</a></li><li><a href="#" target="_self" class="search-form-input no-link text-uppercase">Search</a></li></ul></nav>
        </header>
        <div class="prism-wrapper" id="prism__wrapper">
            
<main>    
    

<section id="prism__post-list" class="prism-section row">
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/cs193p_2021%E7%AC%94%E8%AE%B0%5B4%5D_Color_Image_Gesture/" target="_self">cs193p_2021笔记[4]_Color_Image_Gesture</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/cs193p_2021%E7%AC%94%E8%AE%B0%5B4%5D_Color_Image_Gesture/" target="_self">
                <time class="text-uppercase">
                    October 24 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><p><a href="https://www.jianshu.com/p/998b0ef4a2cd">cs193p_2021_笔记_1</a>
<a href="https://www.jianshu.com/p/af0ad1bead34">cs193p_2021_笔记_2</a>
<a href="https://www.jianshu.com/p/d103f8d12052">cs193p_2021_笔记_3_Animation_Transition</a>
cs193p_2021_笔记_4_Color_Image_Gesture
<a href="https://www.jianshu.com/p/e3c2ee1628c6">cs193p_2021_笔记_5_Property Wrapper</a>
<a href="https://www.jianshu.com/p/a315274a4fd2">cs193p_2021_笔记_6_Persistence</a>
<a href="https://www.jianshu.com/p/f4ae879eef9c">cs193p_2021_笔记_7_Document Architecture</a>
<a href="https://www.jianshu.com/p/2136bdc2c6f6">cs193p_2021_笔记_8</a></p><p>--</p><h1>Color, UIColor &amp; CGColor</h1>
<p>Color:</p><ul>
<li>Is a color-specifier, e.g., <code>.foregroundColor(Color.green)</code>.</li>
<li>Can also act like a <code>ShapeStyle</code>, e.g., <code>.fill(Color.blue)</code>.</li>
<li>Can also act like a <code>View</code>, e.g., Color.white can appear <code>wherever</code> a View can appear.（可以当作view）</li>
</ul>
<p>UIColor:</p><ul>
<li>Is used to <code>manipulate</code> colors.（主打操控）</li>
<li>Also has many <code>more</code> built-in <code>colors</code> than <code>Color</code>, including “system-related” colors.(颜色更多)</li>
<li>Can be interrogated and can convert between color spaces.</li>
</ul>
<p>For example, you can get the RGBA values from a UIColor.
Once you have desired UIColor, employ <code>Color(uiColor:)</code> to use it in one of the roles above.</p><p>CGColor:</p><ul>
<li>The fundamental color representation in the Core Graphics drawing system</li>
<li><code>color.cgColor</code></li>
</ul>
<h1>Image V.S. UIImage</h1>
<p>Image:</p><ul>
<li>Primarily serves as a View.(主要功能是View)</li>
<li>Is <code>not</code> a type for vars that <code>hold an image</code> (i.e. a jpeg or gif or some such). That’s UIImage.</li>
<li>Access images in your Assets.xcassets (in Xcode) by name using <code>Image(_ name: String)</code>.</li>
<li>Also, many, many system images available via <code>Image(systemName:)</code>.</li>
<li>You can control the size of system images with <code>.imageScale()</code> View modifier.</li>
<li>System images also are affected by the .font modifier.</li>
<li>System images are also very useful <code>as masks</code> (for gradients, for example).</li>
</ul>
<p>UIImage</p><ul>
<li>Is the type for actually <code>creating/manipulating</code> images and <code>storing</code> in vars.</li>
<li>Very powerful representation of an image.</li>
<li>Multiple file formats, transformation primitives, animated images, etc.</li>
<li>Once you have the UIImage you want, use Image(uiImage:) to display it.</li>
</ul>
<h1>Multithreading</h1>
<ul>
<li>多线程其实并不是同时运行，而是前后台非常快速地切换</li>
<li><code>Queue</code>只是有顺序执行的代码，封装了<code>threading</code>的应用</li>
<li>这些“代码”用<code>closure</code>来传递</li>
<li><strong>main queue</strong>唯一能操作UI的线程<ul>
<li>主线程是单线程，所以不能执行异步代码</li>
</ul>
</li>
<li><strong>background queues</strong>执行任意：<em>long-lived, non-UI</em> tasks<ul>
<li>可以并行运行(running in parallel) -&gt; even with main UI queue</li>
<li>可以手动设置优先级，服务质量(<code>QoS</code>)等</li>
<li>优先级永远不可能超过main queue</li>
</ul>
</li>
<li>base API: GCD (<code>Grand Central Dispatch</code>)<ol>
<li>getting access to a queue</li>
<li>plopping a block of code on a queue</li>
</ol>
</li>
</ul>
<p>A: Creating a Queue</p><p>There are numerous ways to create a queue, but we’re only going to look at two ...</p><div class="highlight"><pre><span></span><span class="n">DispatchQueue</span><span class="p">.</span><span class="n">main</span> <span class="c1">// the queue where all UI code must be posted</span>
<span class="n">DispatchQueue</span><span class="p">.</span><span class="n">global</span><span class="p">(</span><span class="n">qos</span><span class="p">:</span> <span class="n">QoS</span><span class="p">)</span> <span class="c1">// a non-UI queue with a certain quality of service qos (quality of service) is one of the following ...</span>
    <span class="p">.</span><span class="n">userInteractive</span>    <span class="c1">// do this fast, the UI depends on it!</span>
    <span class="p">.</span><span class="n">userInitiated</span>  <span class="c1">// the user just asked to do this, so do it now</span>
    <span class="p">.</span><span class="n">utility</span>    <span class="c1">// this needs to happen, but the user didn’t just ask for it</span>
    <span class="p">.</span><span class="n">background</span> <span class="c1">// maintenance tasks (cleanups, etc.)</span>
</pre></div>
<p>B: Plopping a Closure onto a Queue</p><p>There are two basic ways to add a closure to a queue ...</p><div class="highlight"><pre><span></span><span class="kd">let</span> <span class="nv">queue</span> <span class="p">=</span> <span class="n">DispatchQueue</span><span class="p">.</span><span class="n">main</span> <span class="c1">//or</span>
<span class="kd">let</span> <span class="nv">queue</span> <span class="p">=</span> <span class="n">DispatchQueue</span><span class="p">.</span><span class="n">global</span><span class="p">(</span><span class="n">qos</span><span class="p">:)</span> 
<span class="n">queue</span><span class="p">.</span><span class="k">async</span> <span class="p">{</span> <span class="cm">/* code to execute on queue */</span> <span class="p">}</span>
<span class="n">queue</span><span class="p">.</span><span class="n">sync</span> <span class="p">{</span> <span class="cm">/* code to execute on queue */</span> <span class="p">}</span>
</pre></div>
<p>主线程里永远不要<code>.sync</code>, 那样会阻塞UI</p><div class="highlight"><pre><span></span><span class="n">DispatchQueue</span><span class="p">(</span><span class="n">global</span><span class="p">:</span> <span class="p">.</span><span class="n">userInitiated</span><span class="p">).</span><span class="k">async</span> <span class="p">{</span>
    <span class="c1">// 耗时代码</span>
    <span class="c1">// 不阻塞UI，也不能更新UI</span>
    <span class="c1">// 到主线程去更新UI</span>
    <span class="n">DispatchQueue</span><span class="p">.</span><span class="n">main</span><span class="p">.</span><span class="k">async</span> <span class="p">{</span>
        <span class="c1">// UI code can go here! we’re on the main queue! </span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
<h1>Gestures</h1>
<p>手势是iOS里的一等公民</p><div class="highlight"><pre><span></span><span class="c1">// recognize</span>
<span class="n">myView</span><span class="p">.</span><span class="n">gesture</span><span class="p">(</span><span class="n">theGesture</span><span class="p">)</span> <span class="c1">// theGesture must implement the Gesture protocol</span>

<span class="c1">// create</span>
<span class="kd">var</span> <span class="nv">theGesture</span><span class="p">:</span> <span class="n">some</span> <span class="n">Gesture</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">TapGesture</span><span class="p">(</span><span class="bp">count</span><span class="p">:</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1">// double tap</span>
<span class="p">}</span>

<span class="c1">// discrete gestures</span>
<span class="kd">var</span> <span class="nv">theGesture</span><span class="p">:</span> <span class="n">some</span> <span class="n">Gesture</span> <span class="p">{</span>
      <span class="k">return</span> <span class="n">TapGesture</span><span class="p">(</span><span class="bp">count</span><span class="p">:</span> <span class="mi">2</span><span class="p">)</span>
        <span class="p">.</span><span class="n">onEnded</span> <span class="p">{</span> <span class="cm">/* do something */</span> <span class="p">}</span>
<span class="p">}</span>

<span class="c1">// 其实就是：</span>
<span class="kd">func</span> <span class="nf">theGesture</span><span class="p">()</span> <span class="p">-&gt;</span> <span class="n">some</span> <span class="n">Gesture</span> <span class="p">{</span>
    <span class="n">tapGesture</span><span class="p">(</span><span class="bp">count</span><span class="p">:</span> <span class="mi">2</span><span class="p">)</span>
<span class="p">}</span>

<span class="c1">// “convenience versions”</span>
<span class="n">myView</span><span class="p">.</span><span class="n">onTapGesture</span><span class="p">(</span><span class="bp">count</span><span class="p">:</span> <span class="nb">Int</span><span class="p">)</span> <span class="p">{</span> <span class="cm">/* do something */</span> <span class="p">}</span> 
<span class="n">myView</span><span class="p">.</span><span class="n">onLongPressGesture</span><span class="p">(...)</span> <span class="p">{</span> <span class="cm">/* do something */</span> <span class="p">}</span>

<span class="c1">// non-discrete gestures</span>

<span class="kd">var</span> <span class="nv">theGesture</span><span class="p">:</span> <span class="n">some</span> <span class="n">Gesture</span> <span class="p">{</span>
      <span class="n">DragGesture</span><span class="p">(...)</span>
<span class="p">.</span><span class="n">onEnded</span> <span class="p">{</span> <span class="n">value</span> <span class="k">in</span> <span class="cm">/* do something */</span> <span class="p">}</span>
</pre></div>
<p>non-discrete手势里传递的<code>value</code>是一个state:</p><ul>
<li>For a <code>DragGesture</code>, it’s a struct with things like the <code>start and end location</code> of the fingers.</li>
<li>For a <code>MagnificationGesture</code> it’s the <code>scale</code> of the magnification (how far the fingers spread out).</li>
<li>For a <code>RotationGesture</code> it’s the <code>Angle</code> of the rotation (like the fingers were turning a dial).</li>
<li>还可以跟踪一个state: <code>@GestureState var myGestureState: MyGestureStateType = &lt;starting value&gt;</code></li>
</ul>
<p>唯一可以更新这个<code>myGestureState</code>的机会：</p><div class="highlight"><pre><span></span><span class="kd">var</span> <span class="nv">theGesture</span><span class="p">:</span> <span class="n">some</span> <span class="n">Gesture</span> <span class="p">{</span>
     <span class="n">DragGesture</span><span class="p">(...)</span>
        <span class="p">.</span><span class="n">updating</span><span class="p">(</span><span class="err">$</span><span class="n">myGestureState</span><span class="p">)</span> <span class="p">{</span> <span class="n">value</span><span class="p">,</span> <span class="n">myGestureState</span><span class="p">,</span> <span class="n">transaction</span> <span class="k">in</span> 
            <span class="n">myGestureState</span> <span class="p">=</span> <span class="cm">/* usually something related to value */</span>
        <span class="p">}</span>
        <span class="p">.</span><span class="n">onEnded</span> <span class="p">{</span> <span class="n">value</span> <span class="k">in</span> <span class="cm">/* do something */</span> <span class="p">}</span>
 <span class="p">}</span>
</pre></div>
<p>注意<code>$</code>的用法</p><p>如果不需要去计算一个<code>gestureState</code>传出去的话，有个<code>updating</code>用简版：</p><div class="highlight"><pre><span></span><span class="p">.</span><span class="n">onChanged</span> <span class="p">{</span> <span class="n">value</span> <span class="k">in</span>
<span class="cm">/* do something with value (which is the state of the fingers) */</span>
<span class="p">}</span>
</pre></div>
<p>事实上，目前来看<code>gestureState</code>只做了两件事：</p><ol>
<li>把实时手势对应的值保存起来</li>
<li>在手势结束时复原（对于缩放，变为1，对于移动，变为0）</li>
<li>同时，它是只读的，只在<code>.updating</code>方法里有更新的机会</li>
</ol>
<p>所以，如果你的UI和动画逻辑，用到了手势结束时的值（即需要它复原），那么你也可以直接在<code>.onEnded</code>方法里手动把它设回去，等同于你也实现了你的<code>gestureState</code>，并且没有它那些限制。</p><h2>Drag and Drop</h2>
<h3>Item Provider</h3>
<ul>
<li>The heart of drag nad drop is the <code>NSItemProvider</code> class.</li>
<li>It facilitates the transfer of data between processes (via drag and drop, for example)</li>
<li>It facilitates the transfer of a number of data types in iOS, for example:<ul>
<li>NSAttributedString and NSString</li>
<li>NSURL</li>
<li>UIImage and UIColor</li>
</ul>
</li>
<li>pre-Swift，所以需要bridging，比如：<code>String as NSString</code></li>
</ul>
<p>结合几个要点，一句话就能让你的元素能被拖动(drag)：</p><div class="highlight"><pre><span></span><span class="n">Text</span><span class="p">(</span><span class="n">emoji</span><span class="p">).</span><span class="n">onDrag</span><span class="p">{</span> <span class="bp">NSItemProvider</span><span class="p">(</span><span class="n">object</span><span class="p">:</span> <span class="n">emoji</span> <span class="k">as</span> <span class="bp">NSString</span><span class="p">)}</span>
</pre></div>
<p>而接收(drop)则要复杂很多：</p><div class="highlight"><pre><span></span><span class="n">otherView</span><span class="p">.</span><span class="n">onDrop</span><span class="p">(</span><span class="n">of</span><span class="p">:</span> <span class="p">[.</span><span class="n">plainText</span><span class="p">],</span> <span class="n">isTarget</span><span class="p">:</span> <span class="kc">nil</span><span class="p">)</span> <span class="p">{</span><span class="n">providers</span><span class="p">,</span> <span class="n">location</span> <span class="k">in</span> <span class="k">return</span> <span class="kc">false</span> <span class="p">}</span>
</pre></div>
<ul>
<li>参接收的类型由<code>of</code>参数指定，这里假定是文本</li>
<li>方法里最终要返回一个bool值，表示成功接收与否，我返了个false，意思是你能让物体拖动，但是一松开手指就复原了</li>
</ul>
<p>从<code>itemprovider</code>里加载对象有模板代码：</p><div class="highlight"><pre><span></span><span class="kd">extension</span> <span class="nc">Array</span> <span class="k">where</span> <span class="n">Element</span> <span class="p">==</span> <span class="bp">NSItemProvider</span> <span class="p">{</span>
  <span class="kd">func</span> <span class="nf">loadObjects</span><span class="p">&lt;</span><span class="n">T</span><span class="p">&gt;(</span><span class="n">ofType</span> <span class="n">theType</span><span class="p">:</span> <span class="n">T</span><span class="p">.</span><span class="kr">Type</span><span class="p">,</span> <span class="n">firstOnly</span><span class="p">:</span> <span class="nb">Bool</span> <span class="p">=</span> <span class="kc">false</span><span class="p">,</span> <span class="n">using</span> <span class="n">load</span><span class="p">:</span> <span class="p">@</span><span class="n">escaping</span> <span class="p">(</span><span class="n">T</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="nb">Void</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="nb">Bool</span> <span class="k">where</span> <span class="n">T</span><span class="p">:</span> <span class="bp">NSItemProviderReading</span> <span class="p">{</span>
    <span class="k">if</span> <span class="kd">let</span> <span class="nv">provider</span> <span class="p">=</span> <span class="bp">first</span><span class="p">(</span><span class="k">where</span><span class="p">:</span> <span class="p">{</span> <span class="nv">$0</span><span class="p">.</span><span class="n">canLoadObject</span><span class="p">(</span><span class="n">ofClass</span><span class="p">:</span> <span class="n">theType</span><span class="p">)})</span> <span class="p">{</span>
      <span class="n">provider</span><span class="p">.</span><span class="n">loadObject</span><span class="p">(</span><span class="n">ofClass</span><span class="p">:</span> <span class="n">theType</span><span class="p">)</span> <span class="p">{</span> <span class="n">object</span><span class="p">,</span> <span class="n">error</span> <span class="k">in</span>
        <span class="k">if</span> <span class="kd">let</span> <span class="nv">value</span> <span class="p">=</span> <span class="n">object</span> <span class="k">as</span><span class="p">?</span> <span class="n">T</span> <span class="p">{</span>
          <span class="n">DispatchQueue</span><span class="p">.</span><span class="n">main</span><span class="p">.</span><span class="k">async</span> <span class="p">{</span>
              <span class="n">load</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
          <span class="p">}</span>
        <span class="p">}</span>
      <span class="p">}</span>
      <span class="k">return</span> <span class="kc">true</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="kc">false</span>
  <span class="p">}</span>

<span class="c1">// and</span>
<span class="c1">// where T: _ObjectiveCBridgeable, T._ObjectiveCType: NSItemProviderReading</span>
</pre></div>
<ol>
<li>提供了两段代码，可以看到其实就是对要加载的对象的约束不同，提供了对OC的兼容</li>
<li>模板代码演示了</li>
</ol>
<p>稳健地从拖拽对象加载内容（canload -&gt; load)
3. 真正的业务逻辑其实就是为拖进来的这个view选择一个位置存放（或读取它携带的数据）
4. <code>T.Type</code>传的是类别的<code>.self</code>，比如<code>String.self</code></p></div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/cs193p_2021%E7%AC%94%E8%AE%B0%5B4%5D_Color_Image_Gesture/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/cs193p_2021%E7%AC%94%E8%AE%B0%5B3%5D_Animation_Transition/" target="_self">cs193p_2021笔记[3]_Animation_Transition</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/cs193p_2021%E7%AC%94%E8%AE%B0%5B3%5D_Animation_Transition/" target="_self">
                <time class="text-uppercase">
                    October 24 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><p><a href="https://www.jianshu.com/p/998b0ef4a2cd">cs193p_2021_笔记_1</a>
<a href="https://www.jianshu.com/p/af0ad1bead34">cs193p_2021_笔记_2</a>
cs193p_2021_笔记_3_Animation_Transition
<a href="https://www.jianshu.com/p/41e7309c7f55">cs193p_2021_笔记_4_Color_Image_Gesture</a>
<a href="https://www.jianshu.com/p/e3c2ee1628c6">cs193p_2021_笔记_5_Property Wrapper</a>
<a href="https://www.jianshu.com/p/a315274a4fd2">cs193p_2021_笔记_6_Persistence</a>
<a href="https://www.jianshu.com/p/f4ae879eef9c">cs193p_2021_笔记_7_Document Architecture</a>
<a href="https://www.jianshu.com/p/2136bdc2c6f6">cs193p_2021_笔记_8</a></p><hr />
<h1>Animation</h1>
<ul>
<li>One way to do animation is by animating a Shape.</li>
<li>The other way to do animation is to animate Views via their <code>ViewModifiers</code>.</li>
<li>Only <code>changes</code> can be animated<ul>
<li>ViewModifier arguments (not all, i.e. fonts)</li>
<li>Shapes</li>
<li>the <em>existance</em> of a View in the UI<ul>
<li>比如if-else和ForEach</li>
</ul>
</li>
</ul>
</li>
<li>You can only animate changes to Views in <em>containers that are already on screen</em> (<code>CTAAOS</code>).</li>
</ul>
<p>两个golden rule:</p><ol>
<li>要有view modifier的属性变化</li>
<li>要在屏幕上</li>
</ol>
<p>才会触发动画（其实就是上面的最后两条）</p><ul>
<li>课程的动画例子里，用了if-else来生成view，这样导致了新生成的view不会触发动画</li>
<li>比如点开两张牌，新点开的那张牌由于之前牌的内容并没有出现在屏幕上，导致动画没有触发</li>
<li>所以把view的结构由if-else的生成和销毁机制，变成了透明度切换机制<ul>
<li>即正面和反面都在屏幕上，只不过透明度相反，以在视觉上要么是正面要么是反面</li>
<li>本以为透明度为0就会销毁视图(UIKit？)，看样子并不是这样的，大胆用opacity就好了</li>
</ul>
</li>
</ul>
<h2>隐式调用</h2>
<div class="highlight"><pre><span></span><span class="n">Text</span><span class="p">(</span><span class="err">“👻</span> <span class="err">”</span><span class="p">)</span>
    <span class="p">.</span><span class="n">opacity</span><span class="p">(</span><span class="n">scary</span> <span class="p">?</span> <span class="mi">1</span> <span class="p">:</span> <span class="mi">0</span><span class="p">)</span>                             <span class="c1">// 普通modifier, 即如果没有动画，也需要的状态（即代码也不会删）</span>
    <span class="p">.</span><span class="n">rotationEffect</span><span class="p">(</span><span class="n">Angle</span><span class="p">.</span><span class="n">degrees</span><span class="p">(</span><span class="n">upsideDown</span> <span class="p">?</span> <span class="mi">180</span> <span class="p">:</span> <span class="mi">0</span><span class="p">))</span>    <span class="c1">// 动画modifier，即定制的动画效果，不需要动画的时候，就不需要这一行</span>
    <span class="p">.</span><span class="n">animation</span><span class="p">(</span><span class="n">Animation</span><span class="p">.</span><span class="n">easeInOut</span><span class="p">)</span>                         <span class="c1">// 触发</span>
</pre></div>
<ul>
<li>上述所有<code>ViewModifier</code>都会被动画<ul>
<li><code>scary, upsideDown</code>等值改变时也会触发动画</li>
</ul>
</li>
<li>隐式调用会冒泡（所以不要对一个container view做<code>.animation</code>，还有定位的问题)</li>
<li>animation的参数就是一个struct： duration, delay, repeat, curve...</li>
</ul>
<p>对于不能动画的modifier，看一下这个实例（上为修改前，下为修改后）
<figure  style="flex: 51.452991452991455" ><img width="1204" height="1170" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/4804373fb223c0a4333d720331093521.png" alt=""/></figure></p><ol>
<li>把font设为常量，把缩放变成一个geometric effect</li>
<li>同时也说明<code>.animation()</code>不止作用于它前面的</li>
</ol>
<h2>显式调用</h2>
<div class="highlight"><pre><span></span><span class="n">withAnimation</span><span class="p">(.</span><span class="n">linear</span><span class="p">(</span><span class="n">duration</span><span class="p">:</span> <span class="mi">2</span><span class="p">))</span> <span class="p">{</span>
    <span class="c1">// do something that will cause ViewModifier/Shape arguments to </span>
<span class="n">change</span> <span class="n">somewhere</span> <span class="p">}</span>
</pre></div>
<ul>
<li>It will appear in closures like <code>.onTapGesture</code>.</li>
<li>显式动画不会覆盖掉隐式动画</li>
<li>很少有处理用户手势而不包<code>.withAnimation</code>的</li>
</ul>
<h1>Transition</h1>
<ul>
<li>转场，主要用于view的出现和消失</li>
<li>一对<code>ViewModifier</code>，一个<code>before</code>, 一个<code>after</code></li>
</ul>
<div class="highlight"><pre><span></span><span class="n">ZStack</span> <span class="p">{</span>
    <span class="k">if</span> <span class="n">isFaceUp</span> <span class="p">{</span>
        <span class="n">RoundedRectangle</span><span class="p">()</span> <span class="c1">// default .transition is .opacity </span>
        <span class="n">Text</span><span class="p">(</span><span class="err">“👻</span> <span class="err">”</span><span class="p">).</span><span class="n">transition</span><span class="p">(.</span><span class="n">scale</span><span class="p">)</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="n">RoundedRectangle</span><span class="p">(</span><span class="n">cornerRadius</span><span class="p">:</span> <span class="mi">10</span><span class="p">).</span><span class="n">transition</span><span class="p">(.</span><span class="n">identity</span><span class="p">)</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
<p>Unlike .animation(), .transition() does not get redistributed to a container’s content Views. So putting .transition() on the ZStack above only works if the entire ZStack came/went.</p><p>(Group and ForEach do distribute .transition() to their content Views, however.)</p><p>意思是<code>.transition</code>并不会向下传递，如果对<code>ZStack</code>做转场，只会把整个容器进行转场而不是里面的view（见实例二）</p><ul>
<li>转场只是一个声明，并没有触发动画（其实就是设置了<code>ViewModifier</code>）</li>
<li>所以转场没有隐式调用</li>
<li>只对CTAAOS有用</li>
</ul>
<p><code>.onAppear</code>或<code>.onDisappear</code>时，container必然是在屏幕上的，所以这是一个写<code>.transition</code>的好地方（记得要<code>withAnimation</code>)</p><p>built-in transitions:</p><ul>
<li>AnyTransition.opacity: 通过<code>.opacity</code> modifier来实现淡入淡出</li>
<li>AnyTransition.scale: 通过<code>.frame</code> modifier来实现缩放</li>
<li>AnyTransition.offset(CGSize): 通过<code>.offset</code>来实现移动</li>
<li>AnyTransition.modifier(active:identity:): 你提供两个<code>ViewModifier</code></li>
</ul>
<p>通过<code>AnyTransition.animation</code>(Animation`)来定制动画细节：</p><div class="highlight"><pre><span></span><span class="p">.</span><span class="n">transition</span><span class="p">(.</span><span class="n">opacity</span><span class="p">.</span><span class="n">animation</span><span class="p">(.</span><span class="n">linear</span><span class="p">(</span><span class="n">duration</span><span class="p">:</span> <span class="mi">20</span><span class="p">)))</span>
</pre></div>
<h1>动画机制</h1>
<p>其实就是给出一系列的数据点，系统会根据这些数据点把时间切分，你给的数据点越多，切的时间块也就越多，而且系统会根据你的线性函数来决定是平均还是怎样去切分这些时间块：</p><ul>
<li>the animation system divides the animation duration up into little pieces.</li>
<li>The animation system then tells the Shape/ViewModifier the current piece it should show.</li>
<li>And the Shape/ViewModifier makes sure that its code always reflects that.</li>
</ul>
<p>系统通知变量当前的值，UI根据这个值实时绘制当前的View，不断销毁重建，就是动画的过程。</p><p>系统是用一个变量来通知这个进度的：<code>Animatable</code> protocol的唯一成员变量：<code>animatableData</code>:</p><div class="highlight"><pre><span></span><span class="kd">var</span> <span class="nv">animatableData</span><span class="p">:</span> <span class="kr">Type</span>
</pre></div>
<ul>
<li>Type只需要满足<code>VectorArithmetic</code>协议，其实就是一个可以被细分的值，基本上是Float, Double, CGFloat，以及<code>AnimatablePair</code>(其实就是两个<code>VectorArithmetic</code>)</li>
<li>想要支持动画的<code>Shape</code>, <code>ViewModifier</code>，只需要实现<code>Animatable</code>协议即可（即提供一个<code>animatableData</code>属性）</li>
</ul>
<p>Because it’s communicating both ways, this animatableData is a <code>read-write</code> var.</p><ul>
<li>The <code>setting</code> of this var is the animation system telling the Shape/VM which piece to draw.</li>
<li>The <code>getting</code> of this var is the animation system getting the <code>start/end</code> points of an animation.</li>
</ul>
<p><strong>实例一</strong></p><figure  style="flex: 73.625" ><img width="1178" height="800" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/10d0bb779b10383a36d23d581f2fadb9.png" alt=""/></figure><ul>
<li>view modifier里面有一个变量<code>rotation</code>（ZStack, content, rotation3DEffect)</li>
<li>那么外层在<code>withAnimation{}</code>的时候，我们是期望rotation的值能动起来的<ul>
<li>内置的viewmodifier当然会自己动，如<code>opacity</code>等</li>
</ul>
</li>
<li>那么我们首先就要让<code>Cardify</code> conform to <code>Animatable</code>（例子中的AnimatableModifer = Animatable + ViewModifer)</li>
<li>然后我们就要实现<code>animatableData</code>, 因为系统事实上就是不断去更新这个data值</li>
<li>教材里把它进行了封装（当然你也可以直接用它），这只是思维方式上的区别</li>
<li><code>animatedData</code>会随时间变化，自然会不断invalidate view，然后rebuild view，动画就产生了。</li>
</ul>
<p><strong>实例二</strong></p><p>课程里有这么个需求：卡片由<code>LazyVGrid</code>提供布局，且卡片出现和消失的时候都要有动画。</p><p>出现和消失？那当然就是<code>Transition</code>的事了:</p><div class="highlight"><pre><span></span><span class="n">Card</span><span class="p">()</span>
  <span class="p">.</span><span class="n">transition</span><span class="p">(</span><span class="n">AnyTransition</span><span class="p">.</span><span class="n">asymmetric</span><span class="p">(</span><span class="n">insertion</span><span class="p">:</span> <span class="p">.</span><span class="n">scale</span><span class="p">,</span> 
                                         <span class="n">removal</span><span class="p">:</span> <span class="p">.</span><span class="n">opacity</span><span class="p">)))</span>
</pre></div>
<p>运行时发现消失的时候有动画，出现的动画却没有。原因是<code>transition</code>只会在<em>出现和消失</em>时触发，而我们的卡片是包在grid容器里的，所以grid出现在屏幕上的时候，就带着卡片一起出现了，transition并不会向下传递（前文也已经说过了，这里刚好印证）。</p><ol>
<li>所以解决方法当然可以“延迟”呈现这些卡片</li>
<li>课程里用了另一种方法，机制当然也是延迟，但不是那么地直白：</li>
</ol>
<figure  style="flex: 82.95774647887323" ><img width="1178" height="710" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/e7e3ef2ac16296fabf73162c1fc678fa.png" alt=""/></figure><ul>
<li>就是利用了<code>.onAppear</code>来阻断容器和卡片的连续生成，而改用容器呈现后，再逐个“添加”的方式，让每一张卡片都有一个单独出现的机会</li>
<li>同时也必须利用<code>@State</code>, 让每添加一张卡片都会invalidate view一次</li>
<li>也能看出，animate能animate的就是属性和transition</li>
</ul>
<blockquote>
<p>当然，课程最后改成了“发牌”的机制，手动添加卡片，彻底阻断了卡片和容器一起出现的场景。</p></blockquote>
<p>这就带我们来到了实例三，同一个view在不同容器间的动画，怎么计算各自尺度下同一个view的位置：<code>matchedGeometryEffect</code></p><p><strong>实例三</strong></p><figure  style="flex: 73.45679012345678" ><img width="1190" height="810" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/08935bf9006d072875c203895aff17b7.png" alt=""/></figure><ul>
<li>想要有牌一张张发出去的效果，自然会想到添加延时</li>
<li>实现成了同时做动画，只不过越到后面的牌，延时越长（动作越慢），而不是我们想象的先后触发</li>
</ul>
<p>为了让不同的牌发出去时有立体效果，还以index为依据设置了<code>zIndex</code>，最终效果：</p><figure class="vertical-figure" style="flex: 35.714285714285715" ><img width="220" height="308" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/72b4116eb1e49a3b1eaa9097a2890b60.jpg" alt=""/></figure></div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/cs193p_2021%E7%AC%94%E8%AE%B0%5B3%5D_Animation_Transition/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/cs193p_2021_%E7%AC%94%E8%AE%B0%5B2%5D/" target="_self">cs193p_2021_笔记[2]</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/cs193p_2021_%E7%AC%94%E8%AE%B0%5B2%5D/" target="_self">
                <time class="text-uppercase">
                    October 24 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><p><a href="https://www.jianshu.com/p/998b0ef4a2cd">cs193p_2021_笔记_1</a>
cs193p_2021_笔记_2
<a href="https://www.jianshu.com/p/d103f8d12052">cs193p_2021_笔记_3_Animation_Transition</a>
<a href="https://www.jianshu.com/p/41e7309c7f55">cs193p_2021_笔记_4_Color_Image_Gesture</a>
<a href="https://www.jianshu.com/p/e3c2ee1628c6">cs193p_2021_笔记_5_Property Wrapper</a>
<a href="https://www.jianshu.com/p/a315274a4fd2">cs193p_2021_笔记_6_Persistence</a>
<a href="https://www.jianshu.com/p/f4ae879eef9c">cs193p_2021_笔记_7_Document Architecture</a>
<a href="https://www.jianshu.com/p/2136bdc2c6f6">cs193p_2021_笔记_8</a></p><p>本文涉及内容：<code>ViewModifier, Property Observers, Layout</code></p><hr />
<h1>ViewModifier</h1>
<p><code>.aspectRatio(2/3)</code> is likely something like <code>.modifier(AspectModifier(2/3))</code> AspectModifier can be <code>anything</code> that conforms to the <code>ViewModifier</code> protocol ...</p><p>它只有一个body方法：</p><div class="highlight"><pre><span></span><span class="kd">protocol</span> <span class="nc">ViewModifier</span> <span class="p">{</span>
    <span class="kd">associatedtype</span> <span class="n">Content</span> <span class="c1">// this is a protocol’s version of a“don’t care” </span>
    <span class="kd">func</span> <span class="nf">body</span><span class="p">(</span><span class="n">content</span><span class="p">:</span> <span class="n">Content</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="n">some</span> <span class="n">View</span> <span class="p">{</span>
        <span class="k">return</span> <span class="n">some</span> <span class="n">View</span> <span class="n">that</span> <span class="n">represents</span> <span class="n">a</span> <span class="n">modification</span> <span class="n">of</span> <span class="n">content</span> <span class="p">}</span>
<span class="p">}</span>
</pre></div>
<ul>
<li>对一个view调用<code>.modifier</code>就是把这个view传成了上述body方法的content</li>
<li>而从<code>.modifer</code>变成<code>.cardify</code>，不过是用了<code>extension</code>：</li>
</ul>
<div class="highlight"><pre><span></span><span class="kd">extension</span> <span class="nc">View</span> <span class="p">{</span>
    <span class="kd">func</span> <span class="nf">cardify</span><span class="p">(</span><span class="n">isFaceUp</span><span class="p">:</span> <span class="nb">Bool</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="n">some</span> <span class="n">View</span> <span class="p">{</span>
        <span class="k">return</span> <span class="kc">self</span><span class="p">.</span><span class="n">modifier</span><span class="p">(</span><span class="n">Cardify</span><span class="p">(</span><span class="n">isFaceUp</span><span class="p">:</span> <span class="n">isFaceUp</span><span class="p">))</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
<h1>Property Observers</h1>
<ul>
<li>语法长得像<code>computed var</code>, 但完全不是一回事 （get, set之于willSet, didSet）</li>
<li>willSet, didSet，对应newValue, oldValue</li>
</ul>
<h2>@State</h2>
<p>your view is <strong>Read Only</strong>,</p><p>为什么？</p><blockquote>
<p>因为view的生命周期足够短，基本上是不断地生成和销毁，根本不需要”被改变“</p></blockquote>
<ul>
<li>所以永远用<code>let</code></li>
<li>所以是<code>stateles</code>的</li>
</ul>
<p>这样的结构很简单，任何view的变化其实就是重绘。</p><p>仍然有些时候需要状态：</p><ul>
<li>编辑表单</li>
<li>模态窗口或通知窗口等临时窗口</li>
<li>动画需要追踪动画进度</li>
</ul>
<p>声明：</p><div class="highlight"><pre><span></span><span class="p">@</span><span class="n">State</span> <span class="kd">private</span> <span class="kd">var</span> <span class="nv">somethingTemporary</span><span class="p">:</span> <span class="n">SomeType</span> <span class="c1">// this can be of any type</span>
</pre></div>
<ul>
<li>private 表示别人访问不到</li>
<li>@State的的变化会在<strong>必要时</strong>引起重绘 （相当于一个<code>@ObservedObject</code>）</li>
<li>view会不断销毁和重建 -&gt; 指针会永远指向新的内存地址</li>
<li>而state是在堆上分配的空间</li>
<li>所以销毁和重建view并不会丢失state</li>
<li>后文<code>property wrapper</code>详述</li>
</ul>
<h1>Layout</h1>
<ol>
<li><code>Container</code>提供空间</li>
<li><code>Views</code>确定自身的大小</li>
<li><code>Container</code>提供<code>View</code>的位置</li>
<li><code>Container</code>确定自身大小（等同于#2)</li>
</ol>
<h2>HStack and VStack</h2>
<p>横/纵向排列元素(View)，并提供“尽可能小”的空间，根据元素性质，有三种场景：</p><ol>
<li><code>inflexble</code> view: <code>Image</code>，fixed size</li>
<li>slightly more flexible view: <code>Text</code>，适应文字的合适大小</li>
<li>very flexible view: <code>RoundedRectangle</code>: 占满空间 -&gt; 基本上<code>Shape</code>都会有多少空间占多少</li>
</ol>
<ul>
<li>一旦元素确定了size，多余的空间就会给下一个元素，最后<code>very flexible view</code>平均分配剩下的空间</li>
<li>所有元素大小确定，容器大小也就确定了，如果有<code>very flexible</code>的，那么容易本身也是<code>very flexible</code>的</li>
</ul>
<p>remark：</p><ul>
<li><code>Spacer(minLength: CGFloat)</code> 空格, draw nothing, 占尽可能多的空间</li>
<li><code>Divider()</code> 画条分隔线，占尽可能小的空间</li>
<li><code>.layoutPriority(100)</code> 用优先级来表示分配空间的顺序，默认值为0。后分配者如果没有空间了会用省略号表示</li>
<li><code>HStack(alignment: .leading)</code>用来控制元素的对齐</li>
</ul>
<blockquote>
<p>List, Form, OutlineGroup 其实就是 <code>really smart VStacks</code>，即本质上就是一个纵向排列的布局。</p></blockquote>
<h2>LazyHStack and LazyVStack</h2>
<ul>
<li><em>Lazy</em>的意思是如果元素对应的位置没有出现在屏幕上，就不会构建View.</li>
<li>they also size themselves to fit their views</li>
<li>前两条加一起，得出这个容器不会尽可能多的占用空间，即使含有very flexible的view -&gt; 尽可能小的空间</li>
<li>显然，它最多出现在<code>ScrollView</code>里（只有在有限窗口里滚动，才有可见不可见的差别）</li>
</ul>
<h2>Scrollview</h2>
<ul>
<li>给多少空间占多少空间</li>
</ul>
<h2>LazyHGrid and LazyVGrid</h2>
<ul>
<li>一个方向view数量固定，另一个方向动态增减（scroll）的H/V stack，以竖向的<code>LazyVGrid</code>为例：</li>
<li>确定每行元素个数，多少行由元素总数决定</li>
<li>或者确定元素大小，在行方向铺满后，再往下一行铺</li>
<li>HGrid方向则是先纵向铺满，再水平铺</li>
</ul>
<h2>ZStack</h2>
<ul>
<li>sizes itself to fit its children</li>
<li>can be very flexible (if one children is)</li>
</ul>
<p>两个modifier其实也是用的ZStack:</p><ul>
<li><code>.background</code>，插入一个view在底层，stack起来: <code>Text(&quot;hello&quot;).background(Rectangle().foregroundColor(.red))</code></li>
<li><code>.overlay</code>，覆盖到表层的zstack: <code>Circle().overlay(Text(&quot;hello&quot;), alignment:.center)</code></li>
</ul>
<p>More：</p><ul>
<li>一个view是可以选择任意size的，哪怕比给它的空间更大(产生裁剪)</li>
<li><code>.aspectRatio(2/3, contentMode: .fit)</code>如果是在HStack里，<ul>
<li>则是把元素横向排列后得到宽度，根据宽度计算出高度，得到元素大小</li>
<li><code>.fit</code>表示完整显示图片（就长边），短边部分补成黑色，<code>.fill</code>应该是就短边，长边部分就裁剪了</li>
</ul>
</li>
</ul>
<div class="highlight"><pre><span></span><span class="n">HStack</span> <span class="p">{</span>
    <span class="n">ForEach</span><span class="p">(</span><span class="n">cards</span><span class="p">)</span> <span class="p">{</span> <span class="n">card</span> <span class="k">in</span>
        <span class="n">CardView</span><span class="p">(</span><span class="n">card</span><span class="p">).</span><span class="n">aspectRatio</span><span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="n">contentMode</span><span class="p">:</span> <span class="p">.</span><span class="n">fit</span><span class="p">)</span>
    <span class="p">}</span>
<span class="p">}</span>
    <span class="p">.</span><span class="n">foregroundColor</span><span class="p">(.</span><span class="n">orange</span><span class="p">)</span>
    <span class="p">.</span><span class="n">padding</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
<ol>
<li>在能够分配的空间里，四边各减10 -&gt; padding(10)</li>
<li>减10后的空间里，根据aspectRation确定一个size</li>
<li>这个size应用给CardView</li>
<li>组合成HStack的size</li>
</ol>
<p>总大小就是HStack的size四边各加10</p><p>而View们如何知道能占多少空间？-&gt; <code>GeometryReader</code></p><h2>GeometryReader</h2>
<div class="highlight"><pre><span></span><span class="kd">var</span> <span class="nv">body</span><span class="p">:</span> <span class="n">View</span> <span class="p">{</span>
    <span class="n">GeometryReader</span> <span class="p">{</span> <span class="n">geometry</span> <span class="k">in</span>
        <span class="p">...</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
<p>参数<code>geometry</code>是一个<code>GeometryProxy</code>:</p><div class="highlight"><pre><span></span><span class="kd">struct</span> <span class="nc">GeometryProxy</span> <span class="p">{</span>
    <span class="kd">var</span> <span class="nv">size</span><span class="p">:</span> <span class="n">CGSize</span>
    <span class="kd">var</span> <span class="nv">safeAreaInsets</span><span class="err">：</span> <span class="n">EdgeInsets</span>
    <span class="kd">func</span> <span class="nf">frame</span><span class="p">(</span><span class="k">in</span><span class="p">:</span> <span class="n">CoordinateSpace</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="n">CGRect</span>
<span class="p">}</span>
</pre></div>
<ul>
<li><code>size</code>表示被提供了多少的空间（by its container)</li>
<li>并且不包含safe area（如刘海）</li>
<li>如果需要绘制到safe area里去: <code>ZStack{...}.edgesIgnoringSafeArea([.top])</code></li>
</ul>
<figure  style="flex: 127.6086956521739" ><img width="1174" height="460" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/aa2a99b80008d002cfc2521af182c43d.png" alt=""/></figure><p>图中演示的是设置卡片字体的大小，希望尽可能地填充卡片，<code>geometry.size</code>能给出运行时数据，而无需硬编码。</p></div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/cs193p_2021_%E7%AC%94%E8%AE%B0%5B2%5D/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/cs193p_2021_%E7%AC%94%E8%AE%B0%5B1%5D/" target="_self">cs193p_2021_笔记[1]</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/cs193p_2021_%E7%AC%94%E8%AE%B0%5B1%5D/" target="_self">
                <time class="text-uppercase">
                    October 24 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><p>2020年看了一遍，后来学深度学习去了，然后发现2021也出来了，仍然是视频授课（对我们没区别），看完后整理了两年课程的笔记。</p><p>本文涉及内容：<code>struct, enum, optional, protocol, viewbuilder, shape</code></p><p><a href="https://www.jianshu.com/p/af0ad1bead34">cs193p_2021_笔记_2</a>
<a href="https://www.jianshu.com/p/d103f8d12052">cs193p_2021_笔记_3_Animation_Transition</a>
<a href="https://www.jianshu.com/p/41e7309c7f55">cs193p_2021_笔记_4_Color_Image_Gesture</a>
<a href="https://www.jianshu.com/p/e3c2ee1628c6">cs193p_2021_笔记_5_Property Wrapper</a>
<a href="https://www.jianshu.com/p/a315274a4fd2">cs193p_2021_笔记_6_Persistence</a>
<a href="https://www.jianshu.com/p/f4ae879eef9c">cs193p_2021_笔记_7_Document Architecture</a>
<a href="https://www.jianshu.com/p/2136bdc2c6f6">cs193p_2021_笔记_8</a></p><hr />
<h1>struct and class</h1>
<p>拥有差不多的结构</p><ul>
<li>stored vars</li>
<li>computed vars</li>
<li>constant lets</li>
<li>functions</li>
<li>initializers</li>
</ul>
<p>differents:
struct | class
-------|------
Value type | Reference type
Copied when passed or assigned | Passed around via pointers 
Copy on write | Automatically reference counted 
Functional programming | Object-oriented programming 
No inheritance | Inheritance (single) 
“Free”（缺省） init initializes ALL vars | “Free” init initializes NO vars 
Mutability must be explicitly stated | Always mutable (即使用let, 只表示不会改变指针)
Your “go to” data structure | Used in specific circumstances
Everything you’ve seen so far is a struct (except View which is a protocol) | The ViewModel in MVVM is always a class (also, UIKit (old style iOS) is class-based)</p><h1>泛型，函数类型,闭包</h1>
<ul>
<li>允许未知类型，但swift是强类型，所以用类型占位符，用作参数时参考.net的泛型</li>
<li>函数也是一种类型，可以当作变量，参数，出现在变量，参数的位置</li>
<li>in-line风格的函数叫<code>closure</code>(闭包)</li>
</ul>
<h1>enum</h1>
<ul>
<li>枚举是值类型</li>
<li>枚举的每个state都可以有<code>associated data</code>（等于是把每个state看成一个class/struct，associated data就可以理解为<strong>属性</strong>)</li>
</ul>
<div class="highlight"><pre><span></span><span class="kd">enum</span> <span class="nc">FastFoodMenuItem</span> <span class="p">{</span>
    <span class="k">case</span> <span class="n">hamburger</span><span class="p">(</span><span class="n">numberOfPatties</span><span class="p">:</span> <span class="nb">Int</span><span class="p">)</span>
    <span class="k">case</span> <span class="n">fries</span><span class="p">(</span><span class="n">size</span><span class="p">:</span> <span class="n">FryOrderSize</span><span class="p">)</span>
    <span class="k">case</span> <span class="n">drink</span><span class="p">(</span><span class="nb">String</span><span class="p">,</span> <span class="n">ounces</span><span class="p">:</span> <span class="nb">Int</span><span class="p">)</span> <span class="c1">// the unnamed String is the brand, e.g. “Coke”</span>
    <span class="k">case</span> <span class="n">cookie</span> <span class="p">}</span>

<span class="kd">enum</span> <span class="nc">FryOrderSize</span> <span class="p">{</span>
    <span class="k">case</span> <span class="n">large</span>
    <span class="k">case</span> <span class="n">small</span> <span class="p">}</span>

<span class="kd">let</span> <span class="nv">menuItem</span><span class="p">:</span> <span class="n">FastFoodMenuItem</span> <span class="p">=</span> <span class="n">FastFoodMenuItem</span><span class="p">.</span><span class="n">hamburger</span><span class="p">(</span><span class="n">patties</span><span class="p">:</span> <span class="mi">2</span><span class="p">)</span>
<span class="kd">var</span> <span class="nv">otherItem</span><span class="p">:</span> <span class="n">FastFoodMenuItem</span> <span class="p">=</span> <span class="n">FastFoodMenuItem</span><span class="p">.</span><span class="n">cookie</span>
<span class="kd">var</span> <span class="nv">yetAnotherItem</span> <span class="p">=</span> <span class="p">.</span><span class="n">cookie</span> <span class="c1">// Swift can’t figure this out</span>
</pre></div>
<ol>
<li>FryOrderSize同时又是一个枚举</li>
<li>状态drink拥有两个“属性”，而且其中一个还<strong>未命名</strong></li>
</ol>
<h2>break and fall through/defaults</h2>
<div class="highlight"><pre><span></span><span class="kd">var</span> <span class="nv">menuItem</span> <span class="p">=</span> <span class="n">FastFoodMenuItem</span><span class="p">.</span><span class="n">cookie</span>
<span class="k">switch</span> <span class="n">menuItem</span> <span class="p">{</span>
    <span class="k">case</span> <span class="p">.</span><span class="n">hamburger</span><span class="p">:</span> <span class="k">break</span>  <span class="c1">// break</span>
    <span class="k">case</span> <span class="p">.</span><span class="n">fries</span><span class="p">:</span> <span class="bp">print</span><span class="p">(</span><span class="err">“</span><span class="n">fries</span><span class="err">”</span><span class="p">)</span>
    <span class="k">default</span><span class="p">:</span> <span class="bp">print</span><span class="p">(</span><span class="err">“</span><span class="n">other</span><span class="err">”</span><span class="p">)</span> <span class="c1">// default</span>
<span class="p">}</span>
</pre></div>
<ol>
<li>如果把drink写上，但没有方法体，则叫<code>fall through</code>，只会往后面一个state fall through</li>
<li>如果漏写了drink，则会匹配到default项（cookie同理）</li>
</ol>
<h2>with associated data</h2>
<div class="highlight"><pre><span></span><span class="kd">var</span> <span class="nv">menuItem</span> <span class="p">=</span> <span class="n">FastFoodMenuItem</span><span class="p">.</span><span class="n">drink</span><span class="p">(</span><span class="err">“</span><span class="n">Coke</span><span class="err">”</span><span class="p">,</span> <span class="n">ounces</span><span class="p">:</span> <span class="mi">32</span><span class="p">)</span>
  <span class="k">switch</span> <span class="n">menuItem</span> <span class="p">{</span>
      <span class="k">case</span> <span class="p">.</span><span class="n">hamburger</span><span class="p">(</span><span class="kd">let</span> <span class="nv">pattyCount</span><span class="p">):</span> <span class="bp">print</span><span class="p">(</span><span class="err">“</span><span class="n">a</span> <span class="n">burger</span> <span class="n">with</span> <span class="err">\</span><span class="p">(</span><span class="n">pattyCount</span><span class="p">)</span> <span class="n">patties</span><span class="p">!</span><span class="err">”</span><span class="p">)</span>
      <span class="k">case</span> <span class="p">.</span><span class="n">fries</span><span class="p">(</span><span class="kd">let</span> <span class="nv">size</span><span class="p">):</span> <span class="bp">print</span><span class="p">(</span><span class="err">“</span><span class="n">a</span> <span class="err">\</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="n">order</span> <span class="n">of</span> <span class="n">fries</span><span class="p">!</span><span class="err">”</span><span class="p">)</span>
      <span class="k">case</span> <span class="p">.</span><span class="n">drink</span><span class="p">(</span><span class="kd">let</span> <span class="nv">brand</span><span class="p">,</span> <span class="kd">let</span> <span class="nv">ounces</span><span class="p">):</span> <span class="bp">print</span><span class="p">(</span><span class="err">“</span><span class="n">a</span> <span class="err">\</span><span class="p">(</span><span class="n">ounces</span><span class="p">)</span><span class="n">oz</span> <span class="err">\</span><span class="p">(</span><span class="n">brand</span><span class="p">)</span><span class="err">”</span><span class="p">)</span>
      <span class="k">case</span> <span class="p">.</span><span class="n">cookie</span><span class="p">:</span> <span class="bp">print</span><span class="p">(</span><span class="err">“</span><span class="n">a</span> <span class="n">cookie</span><span class="p">!</span><span class="err">”</span><span class="p">)</span>
 <span class="p">}</span>
</pre></div>
<h2>可以拥有方法</h2>
<p>这就可以扩展出computed vars</p><div class="highlight"><pre><span></span><span class="kd">enum</span> <span class="nc">FastFoodMenuItem</span> <span class="p">{</span> <span class="p">...</span>
      <span class="kd">func</span> <span class="nf">isIncludedInSpecialOrder</span><span class="p">(</span><span class="n">number</span><span class="p">:</span> <span class="nb">Int</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="nb">Bool</span> <span class="p">{</span>
          <span class="k">switch</span> <span class="kc">self</span> <span class="p">{</span>
            <span class="k">case</span> <span class="p">.</span><span class="n">hamburger</span><span class="p">(</span><span class="kd">let</span> <span class="nv">pattyCount</span><span class="p">):</span> <span class="k">return</span> <span class="n">pattyCount</span> <span class="p">==</span> <span class="n">number</span>
            <span class="k">case</span> <span class="p">.</span><span class="n">fries</span><span class="p">,</span> <span class="p">.</span><span class="n">cookie</span><span class="p">:</span> <span class="k">return</span> <span class="kc">true</span> <span class="c1">// a drink and cookie in every special order </span>
            <span class="k">case</span> <span class="p">.</span><span class="n">drink</span><span class="p">(</span><span class="kc">_</span><span class="p">,</span> <span class="kd">let</span> <span class="nv">ounces</span><span class="p">):</span> <span class="k">return</span> <span class="n">ounces</span> <span class="p">==</span> <span class="mi">16</span> <span class="c1">// &amp; 16oz drink of any kind</span>
 <span class="p">}</span> <span class="p">}</span>
<span class="p">}</span>
</pre></div>
<h2>Iterable</h2>
<p>conform <code>CaseIterable</code>协议就能被遍历，因为增加了一个<code>allCases</code>的静态变量：</p><div class="highlight"><pre><span></span><span class="kd">enum</span> <span class="nc">TeslaModel</span><span class="p">:</span> <span class="n">CaseIterable</span> <span class="p">{</span>
      <span class="k">case</span> <span class="n">X</span>
      <span class="k">case</span> <span class="n">S</span>
      <span class="k">case</span> <span class="n">Three</span>
      <span class="k">case</span> <span class="n">Y</span>
<span class="p">}</span>
<span class="k">for</span> <span class="n">model</span> <span class="k">in</span> <span class="n">TeslaModel</span><span class="p">.</span><span class="n">allCases</span> <span class="p">{</span>
    <span class="n">reportSalesNumbers</span><span class="p">(</span><span class="k">for</span><span class="p">:</span> <span class="n">model</span><span class="p">)</span>
<span class="p">}</span>
<span class="kd">func</span> <span class="nf">reportSalesNumbers</span><span class="p">(</span><span class="k">for</span> <span class="n">model</span><span class="p">:</span> <span class="n">TeslaModel</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">switch</span> <span class="n">model</span> <span class="p">{</span> <span class="p">...</span> <span class="p">}</span>
<span class="p">}</span>
</pre></div>
<p>SwiftUI实例， <code>LazyVGrid</code>中：</p><div class="highlight"><pre><span></span><span class="kd">struct</span> <span class="nc">GridItem</span> <span class="p">{</span>
    <span class="p">...</span>
    <span class="kd">enum</span> <span class="nc">Size</span> <span class="p">{</span>
        <span class="k">case</span> <span class="n">adaptive</span><span class="p">(</span><span class="n">minimum</span><span class="p">:</span> <span class="n">CGFloat</span><span class="p">,</span> <span class="n">maximum</span><span class="p">:</span> <span class="n">CGFloat</span> <span class="p">=</span> <span class="p">.</span><span class="n">infinity</span><span class="p">)</span>
        <span class="k">case</span> <span class="n">fixed</span><span class="p">(</span><span class="n">CGFloat</span><span class="p">)</span>
        <span class="k">case</span> <span class="n">flexible</span><span class="p">(</span><span class="n">minimum</span><span class="p">:</span> <span class="n">CGFloat</span> <span class="p">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">maximum</span><span class="p">:</span> <span class="n">CGFloat</span> <span class="p">=</span> <span class="p">.</span><span class="n">infinity</span><span class="p">)</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
<ol>
<li><code>associated data</code>还能带默认值</li>
<li>核心作用是告诉系统griditem的size是采用哪种方案（枚举），顺便设置了这种方案下的参数。所以这种场景在swift下完全可以用枚举做到</li>
</ol>
<h1>Optionals</h1>
<p>可靠类型其实就是一个<code>Enum</code></p><div class="highlight"><pre><span></span><span class="kd">enum</span> <span class="nc">Optional</span><span class="p">&lt;</span><span class="n">T</span><span class="p">&gt;</span> <span class="p">{</span> <span class="c1">// a generic type, like Array&lt;Element&gt; or MemoryGame&lt;CardContent&gt; </span>
    <span class="k">case</span> <span class="kr">none</span>
    <span class="k">case</span> <span class="n">some</span><span class="p">(</span><span class="n">T</span><span class="p">)</span> <span class="c1">// the some case has associated value of type T }</span>
</pre></div>
<p>它只有两个状态，要么是none，要么就是is set的状态，具体的值其实是绑定到了<code>associate data</code>里去了</p><p>所以你现在知道了有一种取法其实就是从<code>some</code>里面来取了。</p><h2>语法糖</h2>
<div class="highlight"><pre><span></span><span class="kd">var</span> <span class="nv">hello</span><span class="p">:</span> <span class="nb">String</span><span class="p">?</span>
<span class="kd">var</span> <span class="nv">hello</span><span class="p">:</span> <span class="nb">String</span><span class="p">?</span> <span class="p">=</span> <span class="err">“</span><span class="n">hello</span><span class="err">”</span>
<span class="kd">var</span> <span class="nv">hello</span><span class="p">:</span> <span class="nb">String</span><span class="p">?</span> <span class="p">=</span> <span class="kc">nil</span>
<span class="c1">// 其实是：</span>
<span class="kd">var</span> <span class="nv">hello</span><span class="p">:</span> <span class="nb">Optional</span><span class="p">&lt;</span><span class="nb">String</span><span class="p">&gt;</span> <span class="p">=</span> <span class="p">.</span><span class="kr">none</span>
<span class="kd">var</span> <span class="nv">hello</span><span class="p">:</span> <span class="nb">Optional</span><span class="p">&lt;</span><span class="nb">String</span><span class="p">&gt;</span> <span class="p">=</span> <span class="p">.</span><span class="n">some</span><span class="p">(</span><span class="err">“</span><span class="n">hello</span><span class="err">”</span><span class="p">)</span>
<span class="kd">var</span> <span class="nv">hello</span><span class="p">:</span> <span class="nb">Optional</span><span class="p">&lt;</span><span class="nb">String</span><span class="p">&gt;</span> <span class="p">=</span> <span class="p">.</span><span class="kr">none</span>
</pre></div>
<p>使用：</p><div class="highlight"><pre><span></span><span class="kd">let</span> <span class="nv">hello</span><span class="p">:</span> <span class="nb">String</span><span class="p">?</span> <span class="p">=</span> <span class="p">...</span>
<span class="bp">print</span><span class="p">(</span><span class="n">hello</span><span class="p">!)</span> 
<span class="c1">// 其实是：</span>
<span class="k">switch</span> <span class="n">hello</span> <span class="p">{</span>
    <span class="k">case</span> <span class="p">.</span><span class="kr">none</span><span class="p">:</span> <span class="c1">// raise an exception (crash) </span>
    <span class="k">case</span> <span class="p">.</span><span class="n">some</span><span class="p">(</span><span class="kd">let</span> <span class="nv">data</span><span class="p">):</span> <span class="bp">print</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="p">}</span>

<span class="k">if</span> <span class="kd">let</span> <span class="nv">safehello</span> <span class="p">=</span> <span class="n">hello</span> <span class="p">{</span>
    <span class="bp">print</span><span class="p">(</span><span class="n">safehello</span><span class="p">)</span>
<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="c1">// do something else</span>
<span class="p">}</span>
<span class="c1">// 其实是：</span>
<span class="k">switch</span> <span class="n">hello</span> <span class="p">{</span>
    <span class="k">case</span> <span class="p">.</span><span class="kr">none</span><span class="p">:</span> <span class="p">{</span> <span class="c1">// do something else } </span>
    <span class="k">case</span> <span class="p">.</span><span class="n">some</span><span class="p">(</span><span class="kd">let</span> <span class="nv">data</span><span class="p">):</span> <span class="bp">print</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="p">}</span>

<span class="c1">// 还有一种：</span>

<span class="kd">let</span> <span class="nv">x</span><span class="p">:</span> <span class="nb">String</span><span class="p">?</span> <span class="p">=</span> <span class="p">...</span>
<span class="kd">let</span> <span class="nv">y</span> <span class="p">=</span> <span class="n">x</span> <span class="p">??</span> <span class="err">“</span><span class="n">foo</span><span class="err">”</span>
<span class="c1">// 其实是：</span>
<span class="k">switch</span> <span class="n">x</span> <span class="p">{</span>
    <span class="k">case</span> <span class="p">.</span><span class="kr">none</span><span class="p">:</span> <span class="n">y</span> <span class="p">=</span> <span class="err">“</span><span class="n">foo</span><span class="err">”</span>
    <span class="k">case</span> <span class="p">.</span><span class="n">some</span><span class="p">(</span><span class="kd">let</span> <span class="nv">data</span><span class="p">):</span> <span class="n">y</span> <span class="p">=</span> <span class="n">data</span>
<span class="p">}</span>
</pre></div>
<ol>
<li>所以用<code>!</code>来解包是会报错的原理在此</li>
<li><code>guard</code>的原理同样是<code>switch</code></li>
<li>默认值的原理你应该也能猜到了</li>
<li>三个语法糖，对应的底层就是一句switch，其实就是<code>.none</code>时的三种处理方案</li>
</ol>
<p>当然，还可以<code>chain</code>起来
let x: String? = ...
let y = x?foo()?bar?.z</p><p>// 尝试还原一下：</p><div class="highlight"><pre><span></span><span class="k">switch</span> <span class="n">x</span> <span class="p">{</span>
    <span class="k">case</span> <span class="p">.</span><span class="kr">none</span><span class="p">:</span> <span class="n">y</span> <span class="p">=</span> <span class="kc">nil</span>
    <span class="k">case</span> <span class="p">.</span><span class="n">some</span><span class="p">(</span><span class="kd">let</span> <span class="nv">xval</span><span class="p">)::</span>
        <span class="k">switch</span> <span class="n">xval</span><span class="p">.</span><span class="n">foo</span><span class="p">()</span> <span class="p">{</span>
            <span class="k">case</span> <span class="p">.</span><span class="kr">none</span><span class="p">:</span> <span class="n">y</span> <span class="p">=</span> <span class="kc">nil</span>
            <span class="k">case</span> <span class="p">.</span><span class="n">some</span><span class="p">(</span><span class="kd">let</span> <span class="nv">xfooval</span><span class="p">):</span>
                <span class="k">switch</span> <span class="n">xfooval</span><span class="p">.</span><span class="n">bar</span> <span class="p">{</span>
                    <span class="k">case</span> <span class="p">.</span><span class="kr">none</span><span class="p">:</span> <span class="n">y</span> <span class="p">=</span> <span class="kc">nil</span>
                    <span class="k">case</span> <span class="p">.</span><span class="n">some</span><span class="p">(</span><span class="kd">let</span> <span class="nv">xfbarval</span><span class="p">):</span>
                        <span class="n">y</span> <span class="p">=</span> <span class="n">xfbarval</span><span class="p">.</span><span class="n">z</span>
                <span class="p">}</span>
        <span class="p">}</span>
<span class="p">}</span>
</pre></div>
<p>记住每一个句号对应一个switch，然后在<code>.none</code>的状态下安全退出就是<code>?</code>的用法了。</p><h1>@ViewBuilder</h1>
<ol>
<li>任意<code>func</code>或<code>只读的计算属性</code>都可以标识为<code>@ViewBuilder</code>，一旦标识，它里面的内容将会被解析为<code>a list of Views</code>（也仅仅是这个，最多再加上if-else来选择是“哪些view”，不能再定义变量和写其它代码了）<ul>
<li>一个典型例子就是View里面扣出来的代码(比如子view)做成方法，这个方法是需要加上@ViewBuilder的</li>
<li>或者改语法</li>
<li>或者只有一个View，就不会产生语法歧义，也是可以不加@ViewBuilder的</li>
</ul>
</li>
<li>所以不需要return，而如果你不打标，也是可以通过return来构建view的<ul>
<li>但是就不支持默认返回list或通过if-else返view list的语法了</li>
</ul>
</li>
<li><code>@ViewBuilder</code>也可以标识为方法的参数，表示需要接受一个返回views的函数</li>
</ol>
<div class="highlight"><pre><span></span><span class="kd">init</span><span class="p">(</span><span class="n">items</span><span class="p">:</span> <span class="p">[</span><span class="n">Item</span><span class="p">],</span> <span class="p">@</span><span class="n">ViewBuilder</span> <span class="n">content</span><span class="p">:</span> <span class="p">@</span><span class="n">escaping</span> <span class="p">(</span><span class="n">Item</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="n">ItemView</span><span class="p">)</span> <span class="p">{...}</span>
</pre></div>
<p>同时也注意一下<code>@escaping</code>，凡是函数返回后才可能被调用的闭包（逃逸闭包）就需要，而我们的view是在需要的时候才创建，或反复移除并重建（重绘）的，显然符合逃逸闭包的特征。</p><blockquote>
<p>viewbuilder支持的控制流程代码指的是<code>if-else</code>和<code>ForEach</code>, 所以<code>for...in...</code>是不行的。</p></blockquote>
<h1>Protocol</h1>
<p>接口，协议，约束... 使用场景：</p><ul>
<li>用作类型(Type):<ul>
<li>func travelAround(using moveable: Moveable)</li>
<li>let foo = [Moveable]</li>
</ul>
</li>
<li>用作接口:<ul>
<li>struct cardView: View</li>
<li>class myGame: ObservableObject</li>
<li>behaviors: Identifiable, Hashable, ... Animatable</li>
</ul>
</li>
<li>用作约束：
  struct Game<Content> <code>where</code> Content: Equtable   // 类
  extension Array <code>where</code> Element: Hashable {...}  // 扩展
  init(data: Data) <code>where</code> Data: Collection, Data.Element: Identifiable // 方法</li>
<li>OC里的delegate</li>
<li>code sharing (by <code>extension</code>)<ul>
<li><code>extension</code> to a protocol</li>
<li>this is how Views get forecolor, font and all their other modifiers</li>
<li>also `firstIndex(where:) get implemented</li>
<li>an <code>extension</code> can add <em>default implementation</em> for a func or a var<ul>
<li>that's how <code>objectWillChange</code> comes from</li>
</ul>
</li>
<li><code>extension</code>可以作用到所有服从同一协议的对象<ul>
<li>func filter(_ isIncluded: (Element) -&gt; Bool) -&gt; Array<Element></li>
<li>只为<code>Sequence</code> protocol写了一份filter的扩展代码，但能作用于Array, Range, String, Dictionary</li>
<li>等一切conform to the <code>Sequence</code> protocol的类</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>SwiftUI的<code>View</code> protocol非常简单，conform 一个返回<code>some view</code>的<code>body</code>方法就行了，但是又为它写了无数<code>extension</code>，比如<code>foregroundColor</code>, <code>padding</code>, etc. 示意图：</p><figure  style="flex: 146.20253164556962" ><img width="924" height="316" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/9bfb49031e316c1d1d1bc4a5d6b57427.png" alt=""/></figure><h2>Generics(泛型)</h2>
<p>举例：</p><div class="highlight"><pre><span></span><span class="kd">protocol</span> <span class="nc">Identifiable</span> <span class="p">{</span>
    <span class="kd">associatedtype</span> <span class="n">ID</span><span class="p">:</span> <span class="nb">Hashable</span>
    <span class="kd">var</span> <span class="nv">id</span><span class="p">:</span> <span class="n">ID</span> <span class="p">{</span> <span class="kr">get</span> <span class="p">}</span>
<span class="p">}</span>
</pre></div>
<ol>
<li>不像struct，protocol并不是用<code>Identifiable&lt;ID&gt;</code>来表示泛型，而是在作用域内定义</li>
<li>上例中，ID既定义了类别别名，还规范了约束</li>
</ol>
<ul>
<li>所以你Identifiable的类, 是需要有一个Hashable的ID的</li>
<li>而Hashable的对象，又是需要Equatable的(因为hash会碰撞出相同的结果，需要提供检查相等的方法)</li>
<li>-&gt; <code>protocol inheritancee</code></li>
</ul>
<h1>Shape</h1>
<ul>
<li>Shape is a <code>protocol</code> that inherits from <code>View</code>.</li>
<li>In other words, all Shapes are also Views.</li>
<li>Examples of Shapes already in SwiftUI: RoundedRectangle, Circle, Capsule, etc.</li>
<li>by default, Shapes draw themselfs by <code>filling</code> with the current foreground color.</li>
</ul>
<div class="highlight"><pre><span></span><span class="kd">func</span> <span class="nf">fill</span><span class="p">&lt;</span><span class="n">S</span><span class="p">&gt;(</span><span class="kc">_</span> <span class="n">whatToFillWith</span><span class="p">:</span> <span class="n">S</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="n">some</span> <span class="n">view</span> <span class="k">where</span> <span class="n">S</span><span class="p">:</span> <span class="n">ShapeStyle</span>
</pre></div>
<p><code>ShapeStyle</code> protocol turns a <code>Shape</code> into a <code>View</code>: Color, ImagePaint, AngularGradinet, LinearGradient</p><p>自定义shape最好用path(系统的已经通过extension实现好了view的body)：</p><div class="highlight"><pre><span></span><span class="kd">func</span> <span class="nf">path</span><span class="p">(</span><span class="k">in</span> <span class="n">rect</span><span class="p">:</span> <span class="n">CGRect</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="n">Path</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">a</span> <span class="n">Path</span> 
<span class="p">}</span>
</pre></div>
</div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/cs193p_2021_%E7%AC%94%E8%AE%B0%5B1%5D/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/%E6%9D%8E%E5%AE%8F%E6%AF%85MACHINE-LEARNING-2021-SPRING%E7%AC%94%E8%AE%B0/" target="_self">李宏毅MACHINE LEARNING 2021 SPRING笔记</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/%E6%9D%8E%E5%AE%8F%E6%AF%85MACHINE-LEARNING-2021-SPRING%E7%AC%94%E8%AE%B0/" target="_self">
                <time class="text-uppercase">
                    October 03 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><blockquote>
<p>纯听课时一些思路和笔记，没有教程作用。
这个课程后面就比较水了，大量的全是介绍性的东西，也罗列了大量的既往课程和论文，如果你在工作过研究中碰到了它提过的场景或问题，倒是可以把它作索引用。</p></blockquote>
<h1>Linear Model</h1>
<h2>Piecewise Linear</h2>
<p>线性模型永远只有一条直线，那么对于折线（曲线），能怎样更好地建模呢？这里考虑一种方法，</p><ol>
<li>用一个<code>常数</code>加一个<code>整流函数</code><ul>
<li>即左右两个阈值外y值不随x值变化，阈值内才是线性变化（天生就拥有两个折角）。</li>
</ul>
</li>
<li>每一个转折点加一个新的整流函数</li>
</ol>
<p>如下：
<figure  style="flex: 66.625" ><img width="1066" height="800" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/bc1ab298834013a9304c0c6939d2406b.png" alt=""/></figure></p><p>如果是曲线，也可以近似地理解为数个折线构成的（取决于近似的精度），而蓝色的整流函数不好表示，事实上有sigmoid函数与它非常接近（它是曲线），所以蓝线又可以叫：<code>Hard Sigmoid</code></p><p>所以，最终成了一个常数(<code>bias</code>)和数个<code>sigmoid</code>函数来逼近真实的曲线。同时，每一个转折点<code>i</code>上s函数的具体形状（比如有多斜多高），就由一个新的线性变换来控制：$b_i + w_ix_n$，把<code>i</code>上<strong>累积的线性变换</strong>累加，就得到与$x_n$最可能逼近的曲线。</p><p>下图演示了3个转折点的情况：</p><figure  style="flex: 66.625" ><img width="1066" height="800" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/1d785e5ce2165596c3ebeb09fc8fc48b.png" alt=""/></figure><p>至此，一个简单的对b,w依赖的函数变成了对（$w_i, b_i, c_i$)和, x, b的依赖，即多了很多变量。</p><ul>
<li>$y = b + wx_1$</li>
<li>$y = b + \sum_i c_i sigmoid(b_i + w_i x_{\color{red} 1})$</li>
</ul>
<p>注意这个$x_1$，即只转了一个x就要堆一个<code>sum</code>，而目前也只是演示了只有一个特征的情况。</p><p>如果更复杂一点的模型，每次不是看一个x，而看n个x，（比如利用前7天的观看数据来预测第8天的，那么建模的时候就是每一个数都要与前7天的数据建立w和b的关系）：</p><blockquote>
<p>其实就是由一个feature变成了n个feature了，一般的教材会用不同的feature来讲解（比如影响房价的除了时间，还有面积，地段等等），而这里只是增加了天数，可能会让人没有立刻弄清楚两者其实是同一个东西。其实就是x1, x2, x3...不管它们对应的是同一<strong>类</strong>特征，而是完全不同的多个<strong>角度</strong>的特征。</p></blockquote>
<p>现在就有一堆$wx$了</p><ul>
<li>$y = b + \sum_j w_j x_j$</li>
<li>现在就变成了(注意，其实就是把加号右边完整代入）：</li>
<li>$y = b + \sum_i c_i sigmoid(b_i + \color{red}{\sum_j w_{ij} x_j})$</li>
</ul>
<p>展开计算，再根据特征，又可以看回矩阵了（而不是从矩阵出发来思考）：</p><figure class="vertical-figure" style="flex: 33.29129886506936" ><img width="1056" height="1586" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/a84ecec9786db279d55ff5ecbb1a8124.png" alt=""/></figure><p>矩阵运算结果为(r)，再sigmoid后，设结果为a:</p><ul>
<li>$a_i = c_i \sigma(r_i)$</li>
<li>$y = b + \sum_i a_i$ c 和 a要乘加，仍然可以矩阵化（其实是向量化）：</li>
<li>$y = b + c^T a$， 把上面的展开回去：</li>
<li>$y = b + c^T \sigma(\bold b + W x)$<ul>
<li>前后两个b是不同的，一个是数值，一个是向量</li>
</ul>
</li>
</ul>
<p>这里，我们把目前所有的“未知数”全部拉平拼成了一个向量 $\theta$：
<figure  style="flex: 66.625" ><img width="1066" height="800" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/961de3efa8dcf472170d57c5e60f2fc4.png" alt=""/></figure></p><p>这里，如果把$c^T$写成<code>W'</code>你会发现，我们已经推导出了一个2层的神经网络：一个隐层，一个输出层：</p><ul>
<li>b+wx 是第一层 得到<code>a</code></li>
<li>对<code>a</code>进行一次sigmoid（别的教材里会说是激活）得到<code>a'</code></li>
<li>把<code>a'</code>当作输入，再进行一次 b+wx (这就是隐层了)</li>
<li>得到的输出就是网络的输出<code>o</code></li>
</ul>
<blockquote>
<p>这里在用另一个角度来尝试解释神经网络，激活函数等，但要注意，sigmoid的引入原本是去”对着折线描“的，也就是说是人为选择的，而这里仍然变成了机器去”学习“，即没有告诉它哪些地方是转折点。也就是说有点陷入了用机器学习解释机器学习的情况。</p></blockquote>
<blockquote>
<p>但是如果是纯曲线，那么其实是可以无数个sigmoid来组合的，就不存在要去拟合某些“特定的点”，那样只要找到最合适“数量”的sigmoig就行了（因为任何一个点都可以算是折点）</p></blockquote>
<h2>Loss</h2>
<p>loss 没什么变化，仍旧是一堆$\theta$代入后求的值与y的差，求和。并期望找到使loss最小化的$\theta$：</p><p>$\bold \theta = arg\ \underset{\theta}{min}\ L$</p><h1>Optimization</h1>
<p>真实世界训练样本会很大，</p><ul>
<li>我们往往不会把整个所有数据直接算一次loss，来迭代梯度，</li>
<li>而是分成很多小份(mini-batch)每一小份计算一次loss（然后迭代梯度）</li>
<li>下一个小batch认前一次迭代的结果</li>
<li>也就是说，其实这是一个不严谨的迭代，用别人数据的结果来当成本轮数据的前提<ul>
<li>最准确的当然是所有数据计算梯度和迭代。</li>
<li>一定要找补的话，可以这么认为：<ul>
<li>即使一个小batch，也是可以训练到合理的参数的</li>
<li>所以前一个batch训练出来的数据，是一定程度上合理的</li>
<li>现在换了新的数据，但保持上一轮的参数，反而可以防止<code>过拟合</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<figure  style="flex: 66.625" ><img width="1066" height="800" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/a5c3447aeea455b8aea110482cbcd750.png" alt=""/></figure><p>minibatch还有一个极端就是batchsize=1，即每次看完一条数据就与真值做loss，这当然是可以的，而且它非常快。但是：</p><ol>
<li>小batch虽然快，但是它非常noisy（及每一笔数据都有可能是个例，没有其它数据来抵消它的影响）</li>
<li>因为有gpu平行运算的原因，只要不是batch非常大（比如10000以上），其实mini-batch并不慢</li>
<li>如果是小样本，mini-batch反而更快，因为它一来可以平行运算，在计算gradient的时候不比小batch慢，但是它比小batch要小几个数量级的update.</li>
</ol>
<p>仍然有个但是：实验证明小的batch size会有更高的准确率。
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/5ff5b42e644a35896a5ff9419e005465.png" alt=""/></figure></p><p>两个local minimal，右边那个认为是不好的，因为它只要有一点偏差，与真值就会有巨大的差异。但是没懂为什么大的batch会更容易落在右边。</p><p>这是什么问题？其实是optimization的问题，后面会用一些方法来解决。</p><h2>Sigmoid -&gt; RelU</h2>
<p>前面我们用了soft的折线来模拟折线，其实还可以叠加两个真的折线(<code>ReLU</code>)，这才是我一直说的<code>整流函数</code>的名字的由来。</p><figure  style="flex: 66.625" ><img width="1066" height="800" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/dad7e4a61c95964819eb135b85290edf.png" alt=""/></figure><p>仔细看图，c和c'在第二个转折的右边，一个是向无穷大变，一个是向无穷小变，只要找到合理的斜率，就能抵消掉两个趋势，变成一条直线。</p><p>如果要用ReLU，那么简单替换一下：</p><ul>
<li>$y = b + \sum_i {\color{ccdd00}{c_i}} sigmoid(\color{green}{b_i} + \sum_j \color{blue}{w_{ij}} x_j)$</li>
<li>$y = b + \sum_{\color{red}2i} {\color{ccdd00}{c_i}} \color{red}{max}(\color{red}0,\ \color{green}{b_i} + \sum_j \color{blue}{w_{ij}} x_j)$</li>
</ul>
<p>红色的即为改动的部分，也呼应了2个relu才构成一个sigmoid的铺垫。</p><p>把每一个a当成之前的x，我们可以继续套上新的w,b,c等，生成新的a-&gt;a'
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/fe56169175a358925eb1493eef7511a9.png" alt=""/></figure></p><figure  style="flex: 52.01793721973094" ><img width="928" height="892" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/565aa95b1ba87a098f82bb221977d8f1.png" alt=""/></figure><p>而如果再叠一层，在课程里的资料里，在训练集上loss仍然能下降（到0.1），但是在测试集里，loss反而上升了（0.44)，这意味着开始过拟合了。</p><p>这就是反向介绍神经元和神经网络。先介绍数学上的动机，组成网络后再告诉你这是什么，而不是一上来就给你扯什么是神经元什么是神经网络，再来解释每一个神经元干了什么。</p><p>而传统的神经网络课程里，sigmoid是在逻辑回归里才引入的，是为了把输出限定在1和0之间。显然这里的目的不是这样的，是为了用足够多的sigmoid或relu来逼近真实的曲线（折线）</p><h2>Framework of ML</h2>
<h3>通用步骤：</h3>
<ol>
<li>设定一个函数来描述问题$y = f_\theta(x)$, 其中$\theta$就是所有未知数（参数）</li>
<li>设定一个损失函数$L(\theta)$</li>
<li>求让损失函数尽可能小的$\theta^* = arg\ \underset{\theta}{\rm min}L(\theta)$</li>
</ol>
<h3>拟合不了的原因：</h3>
<ol>
<li>过大的loss通常“暗示”了模型不合适（<strong>model bias</strong>），比如上面的用前1天数据预测后一天，可以尝试改成前7天，前30天等。<ul>
<li>大海里捞针，针其实不在海里</li>
</ul>
</li>
<li>优化问题，梯度下降不到目标值<ul>
<li>针在大海里，我却没有办法把它找出来</li>
</ul>
</li>
</ol>
<h3>如何判断是loss optimization没做好？</h3>
<p>用不同模型来比较（更简单的，更浅的）
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/4a4478c4160c3e330f1d7bdc4ac4b2bb.png" alt=""/></figure></p><p>上图中，为什么56层的表现还不如20层呢？是<code>overfitting</code>吗？<strong>不一定</strong>。</p><p>我们看一下在训练集里的表现，56层居然也不如20层，这合理吗？ <strong>不合理</strong></p><blockquote>
<p>但凡20层能做到的，多出的36层可以直接全部identity（即复制前一层的输出），也不可能比20层更差（神经网络总可以学到的）</p></blockquote>
<p>这时，就是你的loss optimization有问题了。</p><h3>如何解决overfitting</h3>
<ol>
<li>增加数据量<ul>
<li>增加数据量的绝对数量</li>
<li>data augmentation数据增强（比如反复随机从训练集里取，或者对图像进行旋转缩放位移和裁剪等）</li>
</ul>
</li>
<li>缩减模型弹性<ul>
<li>（低次啊，更少的参数「特征」啊）</li>
<li>更少的神经元，层数啊</li>
<li>考虑共用参数</li>
<li>early stopping</li>
<li>regularization<ul>
<li>让损失函数与每个特征系数直接挂勾，就变成了惩罚项</li>
<li>因为它的值越大，会让损失函数越大，这样可以“惩罚”过大的权重</li>
</ul>
</li>
<li>dropout<ul>
<li>随机丢弃一些计算结果</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2>Missmatch</h2>
<p>课上一个测试，预测2/26的观看人数（周五，历史数据都是观看量低），但因为公开了这个测试，引起很多人疯狂点击，结果造成了这一天的预测结果非常差。</p><p>这个不叫overfitting，而是<code>mismatch</code>，表示的是<strong>训练集和测试集的分布是不一样的</strong></p><p>mismatch的问题，再怎么增加数据也是不可能解决的。</p><h2>optimization problems</h2>
<p>到目前为止，有两个问题没有得到解决：</p><ol>
<li>loss optimization有问题怎么解决<ul>
<li>其实就是判断是不是saddle point（鞍点）</li>
</ul>
</li>
<li>mismatch怎么解决</li>
</ol>
<h3>saddle point</h3>
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/71ae2a0cf3a214a3c0660ad34a18874e.png" alt=""/></figure><p>hessian矩阵是二次微分，当一次微分为0的时候，二次微分并不一定为0。这是题眼。</p><p>对于红杠内的部分，设$\theta - \theta^T = v$，有：</p><ul>
<li>for all v: $v^T H v &gt; 0 \rightarrow \theta'$附近的$\theta$都要更大<ul>
<li>-&gt; 确实是在<code>local minima</code></li>
</ul>
</li>
<li>for all v: $v^T H v &lt; 0 \rightarrow \theta'$附近的$\theta$都要更小<ul>
<li>-&gt; 确实是在<code>local maxima</code></li>
</ul>
</li>
<li>而时大时小，说明是在<code>saddle point</code></li>
</ul>
<p>事实上我们不可能去检查<code>所有的v</code>，这里用Hessian matrix来判断：</p><ul>
<li>$\rm H$ is <code>positive definite</code> $\rightarrow$ all eigen values are positive $\rightarrow$ local minimal</li>
<li>$\rm H$ is <code>negative definite</code> $\rightarrow$ all eigen values are negative $\rightarrow$ local maximal</li>
</ul>
<p>用一个很垃圾的网络举例，输入是1，输出是1，有w1, w2两层网络参数，因为函数简单，两次微分得到的hessian矩阵还是比较简单直观的：
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/149c13520fe91f6bd35b4512d3feb892.png" alt=""/></figure></p><p>由于特征值有正有负，我们判断在些(0, 0)这个<code>critical point</code>，它是一个<code>saddle point</code>.</p><p>如果你判断出当前的参数确实卡在了鞍点，它同时也指明了<code>update direction</code>!</p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/81aa9722408cf52b4a133a54a7435dd9.png" alt=""/></figure><p>图中，</p><ol>
<li>先构建出了一个小于0的结果，以便找到可以让$L(\theta)$收敛的目标</li>
<li>这个结果依赖于找到这样一个u<ul>
<li>这个u是$\theta, \theta'$相减的结果</li>
<li>它还是$H$的<code>eigen vector</code></li>
<li>它的<code>eigen value</code>$\rightarrow \lambda$ 还要小于0</li>
</ul>
</li>
</ol>
<p>实际上，<code>eigen value</code>是可以直接求出来的（上例已经求出来了），由它可以推出<code>eigen vector</code>，比如[1, 1]$^T$（自行补相关课程），往往会一对多，应该都是合理的，我们顺着共中一个u去更新$\theta$，就可以继续收敛loss。</p><blockquote>
<p>实际不会真的去计算hessian matrix?</p></blockquote>
<h3>Momentum</h3>
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/14f0976b60bb909a3ec3b147594b49bb.png" alt=""/></figure><p>不管是较为平坦的面，还是saddle point，如果小球以图示的方式滚下去，真实的物理世界是不可能停留在那个gradient为0或接近于0的位置的，因为它有“动量”，即惯性，甚至还可能滚过local minima，这恰好是我们需要的特性。
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/f2711cc9c420552051336483c760c347.png" alt=""/></figure>
不但考虑当前梯度，还考虑之前累积的值（动量），这个之前，是之前所有的动量，而不是前一步的：
$$
\begin{aligned}
m^0 &amp;= 0 \
m^1 &amp;= -\eta g^0 \
m^2 &amp;= -\lambda \eta g^0 - \eta g^1 \
&amp;\vdots
\end{aligned}
$$</p><h3>adaptive learning rate</h3>
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/632ad7d3ca2cd8bae9138dfdcd311805.png" alt=""/></figure><p>不是什么时候loss卡住了就说明到了极点(最小值，鞍点，平坦的点)</p><p>看下面这个error surface，两个参数，一个变动非常平缓，一个非常剧烈，如果应用相同的<code>learning rate</code>，要么反复横跳（过大），要么就再也挪不动步（太小）：</p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/78118fcfe7c7a300c0fb3c063101642e.png" alt=""/></figure><h3>Adagrad (Root Mean Square)</h3>
<p>于是有了下面的优化方法，思路与<code>l2正则化</code>差不多，利用不同参数本身gradient的大小来“惩罚”它起到的作用。</p><ol>
<li>这里用的是相除，因为我的梯度越小，步伐就可以跨得更大了。</li>
<li>并且采用的是梯度的平方和(<code>Root Mean Square</code>)</li>
</ol>
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/5cad715ddff837d3280d5f38270b27c6.png" alt=""/></figure><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/e6eb54b116d3c2b0889449550c4ab5a1.png" alt=""/></figure><p>图中可以看出平缓的$\theta_1$就可以应用大的学习率，反之亦然。这个方法就是<code>Adagrad</code>的由来。不同的参数用不同的步伐来迭代，这是一种思路。</p><p>这就解决问题了吗？看下面这个新月形的error surface，不卖关子了，这个以前接触的更多，即梯度随时间的变化而不同，</p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/2faed0f3097f4189d91a745c25ee50ca.png" alt=""/></figure><h3>RMSProp</h3>
<p>这个方法是找不到论文的。核心思想是在<code>Adagrad</code>做平方和的时候，给了一个$\alpha$作为当前这个梯度的权重(0,1)，而把前面产生的$\sigma$直接应用$(1-\alpha)$：</p><ul>
<li>$\theta_i^{t+1} \leftarrow \theta_i^t - \frac{\eta}{\color{red}{\sigma_i^t}} g_i^t$</li>
<li>$\sigma_i^t = \sqrt{\alpha(\theta_i^{t-1})^2 + (1-\alpha)(g_i^t)^2}$</li>
</ul>
<figure  style="flex: 118.22660098522168" ><img width="960" height="406" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/e2c29123c5ddf87908d3328295e5bb79.png" alt=""/></figure><h3>Adam: (RMSProp + Momentum)</h3>
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/399b29c70ef02dadc3f57a47ddca3a2f.png" alt=""/></figure><h3>Learning Rate Scheduling</h3>
<p>终于来到了最直观的lr scheduling部分，也是最容易理解的，随着时间的变化（如果你拟合有效的话），越接近local minima，lr越小。</p><p>而RMSProp一节里说的lr随时间变化并不是这一节里的随时间变化，而是设定一个权重，始终让<strong>当前</strong>的梯度拥有最高权重，注重的是当前与过往，而schedule则考量的是有计划的减小。</p><p>下图中，应用了adam优化后，由于长久以来横向移动累积的小梯度会突然爆发，形成了图中的局面，应用了scheduling后，人为在越靠近极值学习率越低，很明显直接就解决了这个问题。
<figure  style="flex: 50.7399577167019" ><img width="960" height="946" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/3d5a17833ffc7c40d1b452b9e7bd6771.png" alt=""/></figure></p><p>而<code>warm up</code>没有在原理或直观上讲解更多，了解一下吧，实操上是很可行的，很多知名的网络都用了它：</p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/4357f7df834313bc33800b14e2268252.png" alt=""/></figure><p>要强行解释的话，就是adam的$\theta$是一个基于统计的结果，所以要在看了足够多的数据之后才有意义，因此采用了一开始小步伐再增加到大步伐这样一个过度，拿到足够的数据之后，才开始一个正常的不断减小的schedule的过程。</p><p>更多可参考：<code>RAdam</code>: <a href="https://arxiv.org/abs/1908.03265">https://arxiv.org/abs/1908.03265</a></p><h3>Summary of Optimization</h3>
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/fd50408d5cc7b3a7fc6a1b7e8542a01d.png" alt=""/></figure><p>回顾下<code>Momentum</code>，它就是不但考虑当前的梯度，还考虑之前所有的梯度（加起来），通过数学计算，当然是能算出它的”动量“的。</p><p>那么同样是累计过往的梯度，一个在分母（$\theta$)，一个在分子（momentum)，那不是抵消了吗？</p><ol>
<li>momentum是相加，保留了方向</li>
<li>$\sigma$是平方和，只保留了大小</li>
</ol>
<h2>Batch Normalization</h2>
<p>沿着cost surface找到最低点有一个思路，就是能不能把山“铲平”？即把地貌由崎岖变得平滑点？ <code>batch normalization</code>就是其中一种把山铲平的方法。
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/495d3f5866aebabc0cc9e30a73ac917c.png" alt=""/></figure></p><p>其实就是人为控制了error的范围，让它在各个feature上面的“数量级”基本一致（均值0，方差1），这样产生的error surface不会出现某参数影响相当小，某些影响又相当大，而纯粹是因为input本身量级不同的原因（比如房价动以百万计，而年份是一年一年增的）</p><p>error surface可以想象成每一个特征拥有一个轴（课程用二到三维演示），BN让每条轴上的ticks拥有差不多的度量。</p><p>然后，你把它丢到深层网络里去，你的输出的分布又是不可控的，要接下一个网络的话，你的输出又成了下一个网络的输入。虽然你在输出前nomalization过了，但是可能被极大和极小的权重w又给变了了数量级不同的输出</p><p>再然后，不像第一层，输入的数据来自于训练资料，下一层的输入是要在上一层的输出进行sigmoid之后的</p><p>再然后，你去看看sigmoid函数的形状，它在大于一定值或小于一定值之后，对x的变化是非常不敏感了，这样非常容易了出现梯度消失的现象。</p><p>于是，出于以下两个原因，我们都会考虑在输出后也接一次batch normalization::</p><ol>
<li>归一化（$\mu=0, \delta=1$)</li>
<li>把输入压缩到一个（sigmoid梯度较大的）小区间内</li>
</ol>
<p>照这个思路，我们是需要在sigmoid之前进行一次BN的，而有的教材会告诉你之前之后做都没关系，那么之后去做就丧失了以上第二条的好处。</p><p><strong>副作用</strong></p><ul>
<li>以前$x_1 \rightarrow z_1 \rightarrow a_1$</li>
<li>现在$\tilde z_1$是用所有$z_i$算出来的，不再是独立的了</li>
</ul>
<p><strong>后记1</strong></p><p>最后，实际还会把$\tilde z_i$再这么处理一次：</p><ul>
<li>$\hat z_i = \gamma \odot \tilde z_i + \beta$</li>
</ul>
<p>不要担心又把量级和偏移都做回去了，会以1和0为初始值慢慢learn的。</p><p><strong>后记2</strong></p><p>推理的时候，如果batch size不够，甚至只有一条时，怎么去算$\mu, \sigma$呢？</p><p>pytorch在训练的时候会计算<code>moving average</code>of $\mu$ and $\sigma$ of the batches.(每次把当前批次的均值和历史均值来计算一个新的历史均值$\bar \mu$)</p><ul>
<li>$\bar \mu \leftarrow p \bar \mu + (1-p)\mu_t$</li>
</ul>
<p>推理的时候用$\bar \mu, \bar \sigma$。</p><p>最后，用了BN，平滑了error surface，学习率就可以设大一点了，加速收敛。</p><h1>Classification</h1>
<p>用数字来表示class，就会存在认为1跟2比较近与3比较远的可能（从数学运算来看也确实是的，毕竟神经网络就是不断地乘加和与真值减做对比），所以引入了one-hot，它的特征就是class之间无关联。</p><p>恰恰是这个特性，使得用one-hot来表示词向量的时候成了一个要克服的缺点。预测单词确实是一个分类问题，然后词与词之间却并不是无关的，恰恰是有距离远近的概念的，而把它还原回数字也解决不了问题，因为单个数字与前后的数字确实近了，但是空间上还是可以和很多数字接近的，所以向量还是必要的，于是又继续打补丁，才有了稠密矩阵embedding的诞生。</p><h2>softmax</h2>
<p>softmax的一个简单的解释就是你的真值是0和1的组合(one-hot)，但你的预测值可以是任何数，因为你需要把它normalize到(0,1)的区间。</p><p>当class只有两个时，用softmax和用sigmoid是一样的。</p><h2>loss</h2>
<p>可以继续用MeanSquare Error(MSE) $ e = \sum_i(\hat y_i - y'_i)^2$，但更常用的是：</p><h3>Cross-entropy</h3>
<p>$e = - \sum_i \hat y_i lny'_i$</p><blockquote>
<p><code>Minimizing cross-entropy</code> is equivalent to <code>maximizing likelihood</code></p></blockquote>
<figure  style="flex: 67.24511930585683" ><img width="1240" height="922" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/57db1539a499b753d283f44fd02b1476.png" alt=""/></figure><p>linear regression是想从真值与预测值的差来入手找到最合适的参数，而logistic regression是想找到一个符合真值分布的的预测分布。</p><p>在吴恩达的课程里，这个损失函数是”找出来的“：</p><figure class="vertical-figure" style="flex: 42.29195088676671" ><img width="1240" height="1466" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/962ad32822f268909e640f943fb27eaa.png" alt=""/></figure><ol>
<li>首先，$\theta x$后的值可以是任意值，所以再sigmoid一下，以下记为hx</li>
<li>hx的意思就是<code>y为1的概率</code></li>
<li>我需要一个损失函数，希望当真值是0时，预测y为1的概率的误差应该为无穷大<ul>
<li>也就是说hx=0时，损失函数的结果应该是无穷大</li>
<li>而hx=1时, 损失应该为0</li>
</ul>
</li>
<li>同理，当y为1时，hx=0时损失应该是无穷大，hx=1时损失为0</li>
<li>这时候才告诉你，log函数<strong>刚好长这样</strong>，请回看上面的两张图</li>
</ol>
<p>而别的地方是告诉你log是为了把概率连乘变成连加，方便计算。李宏毅这里干脆就直接告诉你公式长这样了。。。</p><p>这里绕两个弯就好了：</p><ol>
<li>y=1时，预测y为1的概率为1， y=0时，应预测y=1的概率为0</li>
<li>而这里是做损失函数，所以预测对了损失为0，错了损失无穷大</li>
<li>预测为1的概率就是hx，横轴也是hx</li>
</ol>
<blockquote>
<p>课程里说softmax和cross entorpy紧密到pytorch里直接就把两者结合到一起了，应用cross entropy的时候把softmax加到了你的network的最后一层（也就是说你没必要手写）。这里说的只是pytorch是这么处理的吗？</p><p>----是的</p></blockquote>
<h3>CE v.s. MSE</h3>
<p>数学证明：<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/MLDS_2015_2/Lecture/Deep%20More%20(v2).ecm.mp4/index.html">http://speech.ee.ntu.edu.tw/~tlkagk/courses/MLDS_2015_2/Lecture/Deep%20More%20(v2).ecm.mp4/index.html</a></p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/97f9a28a6fef03ce00f321f79903a216.png" alt=""/></figure><p>单看实验结果，初始位置同为loss较大的左上角，因为CE有明显的梯度，很容易找到右下角的极值，但是MSE即使loss巨大，但是却没有梯度。因此对于逻辑回归，选择交叉熵从实验来看是合理的，数学推导请看上面的链接。</p><h1>CNN</h1>
<ol>
<li><strong>Receptive field</strong></li>
</ol>
<p>不管是计算机，还是人脑，去认一个物，都是去判断特定的patten（所以就会有错认的图片产生），这也说明，如果神经网络要去辨识物体，是不需要每个神经元都把整张图片看一次的，只需要关注一些特征区域就好了。（感受野, <code>Receptive field</code>)</p><p>如果你一直用3x3，会不会看不到大的patten呢？$\rightarrow$ 会也不会。</p><p>首先，小的filter当然是不可能看到它的感受野以外的部分，但是，神经网络是多层架构，你这层的输出再被卷一次，这时候每一个数字代表的就是之前的9个像素计算的结果，这一轮的9个数字就是上一层的81个像素（因为stride的原因，大部分是重复的）的计算结果，换言之，感受野大大增强了，也就是说，你只需要增加层数，就可以在小的filter上得到大的patten.</p><ol start="2">
<li><strong>filter &amp; feature map</strong></li>
</ol>
<p>从神经元角度和全连接角度出发的话，每个框其实可以有自己的参数的（即你用了64步把整个图片扫描完的话，就有64组参数），而事实上为了简化模型，可以让某些框对应同样的参数（<strong>参数共享</strong>），原因就是同一特征可能出现在多个位置，比如人有两只脚。</p><p>再然后，实际上每一次都是用一组参数扫完全图的，意思是在每个角落都只搜索这<strong>一个特征</strong>。</p><p>我们把这种机制叫<code>filter</code>，一个filter只找一种特征，乘加出来的结果叫<code>feature map</code>，即这个filter提取出来的特征图。</p><p>因此，</p><ul>
<li>你想提取多少个特征，就得有多少个filter</li>
<li>表现出来就成了你这一层输出有多少个channel</li>
<li>这就是为什么你的图片进来是3channel，出来就是N个channel了，取决于你设计了多少个filter</li>
</ul>
<ol start="3">
<li><strong>Pooling &amp; subsampling</strong></li>
</ol>
<p>由于图像的视觉特征，你把它放大或缩小都能被人眼认出来，因此就产生了pooling这种机制，可以降低样本的大小，这主要是为了减小运算量吧（硬件性能足够就可以不考虑它）。</p><ol start="4">
<li><strong>Data Augmentation</strong></li>
</ol>
<p>CNN并不能识别缩放、旋转、裁切、翻转过的图片，因此训练数据的增强也是必要的。</p><h2>AlphaGo</h2>
<p><strong>layer 1</strong></p><ol>
<li>能被影像化的问题就可以尝试CNN，围棋可以看成是一张19x19的图片</li>
<li>每一个位置被总结出了48种可能的情况(超参1)</li>
<li>所以输入就是19x19x48</li>
<li>用0来padding成23x23</li>
<li>很多patten、定式也是影像化的，可以被filter扫出来</li>
<li>总结出5x5大小的filter就够用了（超参2）</li>
<li>就用了192个fitler（即每一次output有48层channel)（超参3）</li>
<li>stride = 1</li>
<li>ReLU</li>
</ol>
<p><strong>layer 2-12</strong></p><ol>
<li>padding成 21x21</li>
<li>192个 3x3 filter with stride = 1</li>
<li>ReLU</li>
</ol>
<p><strong>layer 13</strong></p><ol>
<li>1x1 filter stride = 1</li>
<li>bias</li>
<li>softmax</li>
</ol>
<p>其中192(个filter)这个超参对比了128，256，384等，也就是说人类并不理解它每一次都提取了什么特征。</p><blockquote>
<p>subsampling对围棋也有用吗？ 上面的结构看出并没有用，事实上，围棋你抽掉一行一列影响是很大的。</p></blockquote>
<h1>Self-Attention</h1>
<p>前面说的都是输入为一个向量（总会拉平成一维向量），如果是多个向量呢？有这样的场景吗？</p><ul>
<li>一段文字，每一个文字都用one-hot或word-embedding来表示<ul>
<li>不但是多个向量，而且还长短不齐</li>
</ul>
</li>
<li>一段语音，每25ms采样形成一个向量，步长为每10ms重复采样，形成向量序列<ul>
<li>400 sample points (16khz)</li>
<li>39-dim MFCC</li>
<li>80-dim filter bank output</li>
<li>参考人类语言处理课程</li>
</ul>
</li>
<li>一个Graph组向量（比如social network)<ul>
<li>每个节点（每个人的profile）就是一个向量</li>
</ul>
</li>
<li>一个分子结构<ul>
<li>每个原子就是一个one-hot</li>
</ul>
</li>
</ul>
<p><strong>输出是什么样的？</strong></p><ol>
<li>一个向量对应一个输出<ul>
<li>文字 -&gt; POS tagging</li>
<li>语音 -&gt; a, a, b, b(怎么去重也参考<a href="https://speech.ee.ntu.edu.tw/~hylee/dlhlp/2020-spring.html">人类语言处理</a>课程)</li>
<li>graph -&gt; 每个节点输出特性（比如每个人的购买决策）</li>
</ul>
</li>
<li>只有一个输出<ul>
<li>文字 -&gt; 情绪分析，舆情分析</li>
<li>语音 -&gt; 判断是谁说的</li>
<li>graph -&gt; 输出整个graph的特性，比如亲水性如何</li>
</ul>
</li>
<li>不定输出（由network自己决定）<ul>
<li>这就叫seq2seq</li>
<li>文字 -&gt; 翻译</li>
<li>语音 -&gt; 真正的语音识别</li>
</ul>
</li>
</ol>
<p>self-attention</p><p>稍稍回顾一下self attention里最重要的q, k, v的部分：</p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/97e9b2e0e7ac0f115036ca6bd0c71849.png" alt=""/></figure><p>图示的是q2与所有的k相乘，再分别与对应的v相乘，然后相加，得到q2对应的输出：b2的过程。</p><p>下图则是矩阵化后的结论：
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/429312b4d209231715578ba2ba3a80dc.png" alt=""/></figure>
具体细节看专题</p><p>真正要学的，就是图中的$W^q, W^k, W^v$</p><h2>Multi-head Self-attention</h2>
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/4ac2f963c12f8046cf93824b2a2c5f9f.png" alt=""/></figure><p>CNN是Self-attention的特例</p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/9c914c0fa757448a64dfa2c88ae110e5.png" alt=""/></figure><h2>Self-attention for Graph</h2>
<p>了解更多：<a href="https://youtu.be/eybCCtNKwzA">https://youtu.be/eybCCtNKwzA</a></p><h1>Transformer</h1>
<p>Transformer是一个seq2seq的model</p><p>以下场景，不管看上去像不像是seq2seq的特征，都可以尝试用seq2seq（trnasformer）来“硬train一发”</p><ul>
<li>QA类的问题，送进去question + context，输出answer<ul>
<li>翻译，摘要，差别，情感分析，只要训练能套上上面的格式，就有可能</li>
</ul>
</li>
<li>文法剖析，送入是句子，输出是树状的语法结构<ul>
<li>把树状结构摊平（其实就是多层括号）</li>
<li>然后就用这个对应关系来当成翻译来训练（即把语法当成翻译）</li>
</ul>
</li>
<li>multi-label classification<ul>
<li>你不能在做multi-class classification的时候取top-k,因为有的属于一个类，有的属于三个类，k不定</li>
<li>所以你把每个输入和N个输出也丢到seq2seq里去硬train一发，网络会自己学到每个文章属于哪“些”类别（不定个数，也像翻译一样）</li>
</ul>
</li>
<li>object dectection<ul>
<li>这个更匪夷所思，感兴趣看论文：<a href="https://arxiv.org/abs/2005.12872(End-to-End">https://arxiv.org/abs/2005.12872(End-to-End</a> Object Detection with Transformers)</li>
</ul>
</li>
</ul>
<h2>Encoder</h2>
<p>Q, K, V(relavant/similarity), zero padding mask, layer normalization, residual等, 具体看<code>self-attention</code>一节。</p><h2>Decoder</h2>
<h3>AT v.s. NAT</h3>
<p>我们之前用的decoder都是一个一个字地预测（输出的）</p><ul>
<li>所以才有position-mask（用来屏蔽当前位置后面的字）</li>
</ul>
<p>这种叫<code>Auto Regressive</code>，简称<code>AT</code>,<code>NAT</code>即<code>Non Auto Regressive</code></p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/a4e144ed0e5f9ad95a029f3224fc46e3.png" alt=""/></figure><p>它一次生成输出的句子。</p><p>至于seq2seq的输出是不定长的，它是怎么在一次输出里面确定长度的，上图已经给出了几种做法：</p><ol>
<li>另做一个predictor来输出一个数字，表示应该输出的长度</li>
<li>直接用一个足够长的<bos>做输入（比如300个），那输出也就有300个，取到第一个<eos>为止</li>
</ol>
<p>因为不是一个一个生成了，好处</p><ol>
<li>可以平行运算。</li>
<li>输出的长度更可控</li>
</ol>
<blockquote>
<p>NAT通常表现不如AT好 (why? <strong>Multi-mmodality</strong>)</p></blockquote>
<p>detail: <a href="https://youtu.be/jvyKmU4OM3c">https://youtu.be/jvyKmU4OM3c</a> (Non-Autoregressive Sequence Generation)</p><h3>AT</h3>
<p>在decoder里最初有让人看不懂的三个箭头从encode的输出里指出来:</p><figure class="vertical-figure" style="flex: 41.08681245858184" ><img width="1240" height="1509" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/04bb017a6b7821b1f8e1f676a4e89c13.png" alt=""/></figure><p>其实这就是<code>cross attention</code></p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/943872fa064ed57098cf57694c851ace.png" alt=""/></figure><p>它就是把自己第一层(self-attention后)的输出乘一个$W^q$得到的<code>q</code>，去跟encoder的输出分别乘$W^k, W^v$得到的k和v运算($\sum q \times k \times v$)得到当前位置的输出的过程。</p><p>而且研究者也尝试过各种<code>cross attention</code>的方法，而不仅仅是本文中的无论哪一层都用<code>encoder</code>最后一层的输出做q和v这一种方案：</p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/d1a0d2db75519d1eb4cd58bc9a0c65a5.png" alt=""/></figure><h2>Training Tips</h2>
<h3>复制机制</h3>
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/637e1eace14b2504906690435792aa40.png" alt=""/></figure><p>一些场景，训练的时候没必要去“生成”阅读材料里提到的一些概念，只需要把它“复制”出来即可，比如上述的人名，专有名字，概念等，以及对文章做摘要等。</p><ul>
<li>Pointer Network: <a href="https://youtu.be/VdOyqNQ9aww">https://youtu.be/VdOyqNQ9aww</a></li>
<li>Copying Mechanism in Seq2Seq <a href="https://arxiv.org/abs/1603.06393">https://arxiv.org/abs/1603.06393</a></li>
</ul>
<h3>Guided Attention</h3>
<p>像语音这种连续性的，需要强制指定(guide)它的attention顺序，相对而言，文字跳跃感可以更大，语音一旦不连续就失去了可听性了，一些关键字：</p><ul>
<li>Monotonic Attention</li>
<li>Location-aware attention</li>
</ul>
<h3>Beam Search</h3>
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/0068ee933dbbe3ab1f2fdc59431bc72b.png" alt=""/></figure><h3>Optimizing Evaluation Metrics / BLEU</h3>
<ul>
<li><p>训练的时候loss用的是cross entropy，要求loss越小越好，</p></li>
<li><p>而在evaluation的时候，我们用的是预测值与真值的<code>BLEU score</code>，要求score越大越好</p></li>
<li><p>那么越小的cross entropy loss真的能产生越高的BLEU score吗？ 未必</p></li>
<li><p>那么能不能在训练的时候也用BLEU score呢？ 不行，它太复杂没法微分，就没法bp做梯度了。</p></li>
</ul>
<h3>Exposure bias</h3>
<p>训练时候应用了<code>Teaching force</code>，用了全部或部分真值当作预测结果来训练（或防止一错到底），而eval的时候确实就是一错到底的模式了。</p><h1>Self-supervised Learning</h1>
<ul>
<li>芝麻街家庭：elmo, bert, erine...</li>
<li>bert就是transformer的encoder</li>
</ul>
<h2>Bert</h2>
<h3>GLUE</h3>
<p>GLUE: General Language Understanding Evaluation</p><p>基本上就是看以下这九个模型的得分：</p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/bc43a4b9f8009e33413a2f5917c37fa2.png" alt=""/></figure><p>训练：</p><ol>
<li>预测mask掉的词(masked token prediction)<ul>
<li>为训练数据集添加部分掩码，预测可能的输出</li>
<li>类似word2vec的C-Bow</li>
</ul>
</li>
<li>预测下一个句子（分类，比如是否相关）(next sentence prediction)<ul>
<li>在句首添加<cls>用来接分类结果</li>
<li>用<sep>来表示句子分隔</li>
</ul>
</li>
</ol>
<p>下游任务（Downstream Task） &lt;- Fine Tune:</p><ol>
<li>sequence -&gt; class: sentiment analysis<ul>
<li>这是需要有label的</li>
<li><cls>节点对的linear部分是随机初始化</li>
<li>bert部分是pre-train的</li>
</ul>
</li>
<li>sequence -&gt; sequence(等长): POS tagging</li>
<li>2 sequences -&gt; class: NLI(从句子A能否推出句子B)(Natural Language Inferencee)<ul>
<li>也比如文章下面的留言的立场分析</li>
<li>用<cls>输出分类结果，用<sep>分隔句子</li>
</ul>
</li>
<li>Extraction-based Question Answering: 基于已有文本的问答系统<ul>
<li>答案一定是出现在文章里面的</li>
<li>输入文章和问题的向量</li>
<li>输出两个数字(start, end)，表示答案在文章中的索引</li>
</ul>
</li>
</ol>
<p>QA输出：</p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/af1aa0bae691c8837e101c1a4d97c30f.png" alt=""/></figure><p>思路：</p><ol>
<li>用<cls>input<sep>document 的格式把输入摆好</li>
<li>用pre-trained的bert模型输出同样个数的向量</li>
<li>准备两个与bert模型等长的向量（比如768维）a, b（random initialized)</li>
<li>a与document的每个向量相乘(inner product)</li>
<li>softmax后，找到最大值，对应的位置(argmax)即为start index</li>
<li>同样的事b再做一遍，得到end index</li>
</ol>
<figure  style="flex: 131.07822410147992" ><img width="1240" height="473" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/5fb69b6f706227e3cc81eaa740c28606.png" alt=""/></figure><h3>Bert train seq2seq</h3>
<p>也是可能的。就是你把输入“弄坏”，比如去掉一些字词，打乱词序，倒转，替换等任意方式，让一个decoder把它还原。 -&gt; <strong>BART</strong></p><h3>附加知识</h3>
<p>有研究人员用bert去分类DNA，蛋白质，音乐。以DNA为例，元素为A,C,G,T,分别对应4个随机词汇，再用bert去分类（用一个英文的pre-trained model），同样的例子用在了蛋白质和音乐上，居然发现效果全部要好于“纯随机”。</p><p>如果之前的实验说明了bert看懂了我们的文章，那么这个荒诞的实验（用完全无关的随意的英文单词代替另一学科里面的类别）似乎证明了事情没有那么简单。</p><h3>More</h3>
<ol>
<li><a href="https://youtu.be/1_gRK9EIQpc">https://youtu.be/1_gRK9EIQpc</a></li>
<li><a href="https://youtu.be/Bywo7m6ySlk">https://youtu.be/Bywo7m6ySlk</a></li>
</ol>
<h2>Multi-lingual Bert</h2>
<p>略</p><h2>GPT-3</h2>
<p>训练是predict next token...so it can do generation(能做生成)</p><blockquote>
<p>Language Model 都能做generation</p></blockquote>
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/a4687a3585d0e42cada14edb4d3d0890.png" alt=""/></figure><p><a href="https://youtu.be/DOG1L9lvsDY">https://youtu.be/DOG1L9lvsDY</a></p><p>别的模型是pre-train后，再fine-tune， GPT-3是想实现zero-shot，</p><h3>Image</h3>
<p><strong>SimCLR</strong></p><ul>
<li><a href="https://arxiv.org/abs/2002.05709">https://arxiv.org/abs/2002.05709</a></li>
<li><a href="https://github.com/google-research/simclr">https://github.com/google-research/simclr</a></li>
</ul>
<p><strong>BYOL</strong></p><ul>
<li><strong>B</strong>ootstrap <strong>y</strong>our <strong>o</strong>own <strong>l</strong>atent</li>
<li><a href="https://arxiv.org/abs/2006.07733">https://arxiv.org/abs/2006.07733</a></li>
</ul>
<h3>Speech</h3>
<p>在bert上有九个任务(GLUE)来差别效果好不好，在speech领域还缺乏这样的数据库。</p><h2>Auto Encoder</h2>
<p>也是一种<code>self-supervised</code> Learning Framework -&gt; 也叫 pre-train, 回顾：
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/6d2689336625e12011479fa23633aa94.png" alt=""/></figure></p><p>在这个之前，其实有个更古老的任务，它就是<code>Auto Encoder</code></p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/b5667c49d1673c62d5fdbb794fd32053.png" alt=""/></figure><ul>
<li>用图像为例，通过一个网络encode成一个向量后，再通过一个网络解码(reconstrucion)回这张图像（哪怕有信息缺失）</li>
<li>中间生成的那个向量可以理解为对原图进行的压缩</li>
<li>或者说一种降维</li>
</ul>
<p>降维的课程：</p><ul>
<li>PCA: <a href="https://youtu.be/iwh5o_M4BNU">https://youtu.be/iwh5o_M4BNU</a></li>
<li>t-SNE: <a href="https://youtu.be/GBUEjkpoxXc">https://youtu.be/GBUEjkpoxXc</a></li>
</ul>
<p>有一个de-noising的Auto-encoder, 给入的是加了噪音的数据，经过encode-decode之后还原的是没有加噪音的数据</p><p>这就像加了噪音去训练bert</p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/db0ed507503f41304ead7808d285a7f8.png" alt=""/></figure><h3>Feature Disentangle</h3>
<p>去解释auto-encoder压成的向量就叫<code>Feature Disentagle</code>，比如一段音频，哪些是内容，哪些是人物；一段文字，哪些表示语义，哪些是语法；一张图片，哪些表示物体，哪些表示纹理，等。</p><p>应用： voice conversion -&gt; 变声器</p><p>传统的做法应该是每一个语句，都有两种语音的资料，N种语言/语音的话，就需要N份。有Feature Disentangle的话，只要有两种语音的encoder，就能知道哪些是语音特征，哪些是内容特征，拼起来，就能用A的语音去读B的内容。所以<strong>前提</strong>就是能分析压缩出来的向量。</p><h3>Discrete Latent Representation</h3>
<p>如果压缩成的向量不是实数，而是一个binary或one-hot</p><ul>
<li>binary: 每一个维度几乎都有它的含义，我们只需要看它是0还是1</li>
<li>one-hot: 直接变分类了。-&gt; <code>unsupervised classification</code></li>
</ul>
<p><strong>VQVAE</strong></p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/7d4061daabfdfcf415de004a69a1636b.png" alt=""/></figure><ul>
<li>Vector Quantized Variational Auot-encoder <a href="https://arxiv.org/abs/1711.00937">https://arxiv.org/abs/1711.00937</a></li>
</ul>
<h3>Text as Representation</h3>
<ul>
<li><a href="https://arxiv.org/abs/1810.02851">https://arxiv.org/abs/1810.02851</a></li>
</ul>
<p>如果压缩成的不是一个向量，而也是一段<code>word sequence</code>，那么是不是就成了<code>summary</code>的任务？ 只要encoder和decoder都是seq2seq的model</p><p>-&gt; seq2seq2seq auto-encoder -&gt; <code>unsupervised summarization</code></p><p>事实上训练的时候encoder和decoder可能产生强关联，这个时候就引入一个额外的<code>discriminator</code>来作判别:
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/e470d48770f1cee066eca5b76e49e983.png" alt=""/></figure></p><p>有点像cycle GAN，一个generator接一个discriminator，再接另一个generator</p><h3>abnormal detection</h3>
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/d8a6da742b367ace4528d833cb8623fa.png" alt=""/></figure><ul>
<li>Part 1: <a href="https://youtu.be/gDp2LXGnVLQ">https://youtu.be/gDp2LXGnVLQ</a></li>
<li>Part 2: <a href="https://youtu.be/cYrNjLxkoXs">https://youtu.be/cYrNjLxkoXs</a></li>
<li>Part 3: <a href="https://youtu.be/ueDlm2FkCnw">https://youtu.be/ueDlm2FkCnw</a></li>
<li>Part 4: <a href="https://youtu.be/XwkHOUPbc0Q">https://youtu.be/XwkHOUPbc0Q</a></li>
<li>Part 5: <a href="https://youtu.be/Fh1xFBktRLQ">https://youtu.be/Fh1xFBktRLQ</a></li>
<li>Part 6: <a href="https://youtu.be/LmFWzmn2rFY">https://youtu.be/LmFWzmn2rFY</a></li>
<li>Part 7: <a href="https://youtu.be/6W8FqUGYyDo">https://youtu.be/6W8FqUGYyDo</a></li>
</ul>
<h1>Adversarial Attack</h1>
<p>给你一张猫的图片，里面加入少许噪音，以保证肉眼看不出来有噪音的存在：</p><ol>
<li>期望分类器认为它不是猫</li>
<li>期望分类器认为它是一条鱼，一个键盘...</li>
</ol>
<p>比如你想要欺骗垃圾邮件过滤器</p><ul>
<li>找到一个与$x^0$非常近的向量x</li>
<li>网络正常输出y</li>
<li>真值为$\hat y$</li>
<li>$L(x) = -e(y, \hat y)$</li>
<li>$x^* = arg\underset{d(x^0, x) \leq \epsilon}{\rm min}\ L(x)$ 即要找到令损失最大的x<ol>
<li>这里L(x)我们取了反</li>
<li>$\epsilon$越小越好，指的是$x^0$要与x越接近越好（欺骗人眼）</li>
</ol>
</li>
<li>如果还期望它认成是$y^{target}$，那就再加上与其的的损失</li>
<li>$L(x) = -e(y, \hat y) + e(y, y^{target})$</li>
<li>注意两个error是反的，一个要求越远越好(真值），一个要求越近越好（target)</li>
</ul>
<p>怎么计算$d(x^0, x) \leq \epsilon$呢？</p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/52ea0aa0520c35787d94019e0efc61a7.png" alt=""/></figure><p>图上可知，如果都改变一点点，和某一个区域改动相当大，可能在L2-norm的方式计算出来是一样的，但是在L-infinity看来是不一样的（它只关心最大的变动）。</p><p>显然L-infinity更适合人眼的逻辑，全部一起微调人眼不能察觉，单单某一块大调，人眼是肯定可以看出来的。</p><p>而如果是语音的话，可能耳朵对突然某个声音的变化反而不敏感，整体语音风格变了却能立刻认出说话的人声音变了，这就要改变方案了。</p><h2>Attack Approach</h2>
<p>如何得到这个x呢？其实就是上面的损失函数。以前我们是为了train权重，现在train的就是x本身了。</p><ol>
<li>损失达到我们的要求 （有可能这时候与原x相关很远）</li>
<li>与原x的距离达到我们的要求, 怎么做？<ul>
<li>其实就是以$x^0$为中心，边长为$2\epsilon$的矩形才是期望区域</li>
<li>如果update后，$x^t$仍然落在矩形外，那么就在矩形里找一个离它最近的点，当作本轮更新后的$x^t$，进入下一轮迭代</li>
</ul>
</li>
</ol>
<p>Fast Gradient Sign Method(FGSM): <a href="https://arxiv.org/abs/1412.6572">https://arxiv.org/abs/1412.6572</a></p><ul>
<li>相比上面的迭代方法，FGSM只做一次更新</li>
<li>就是根据梯度，判断是正还是负，然后把原x进行一次加减$\epsilon$的操作（其实等于是落在了矩形的四个点上）</li>
<li>也就是说它直接取了四个点之一作为$x^0$</li>
</ul>
<h2>White Box v.s. Black Box</h2>
<p>讲上述方法的时候肯定都在疑惑，分类器是别人的，我怎么可能拿到别人的模型来训练我的攻击器？ -&gt; <strong>White Box Attack</strong></p><p>那么<code>Black Box Attack</code>是怎么实现的呢？</p><ol>
<li>如果我们知道对方的模型是用什么数据训练的话，我们也可以训练一个类似的(proxy network)<ul>
<li>很大概率都是用公开数据集训练的</li>
</ul>
</li>
<li>如果不知道的话呢？就只能尝试地丢一些数据进去，观察（记录）它的输出，然后再用这些测试的输入输出来训练自己的proxy network了。</li>
</ol>
<ul>
<li>one pixel attack<ul>
<li><a href="https://arxiv.org/abs/1710.08864">https://arxiv.org/abs/1710.08864</a></li>
<li><a href="https://youtu.be/tfpKIZIWidA">https://youtu.be/tfpKIZIWidA</a></li>
</ul>
</li>
<li>universal adversarial attack<ul>
<li>万能noise</li>
<li><a href="https://arxiv.org/abs/1610.08401">https://arxiv.org/abs/1610.08401</a></li>
</ul>
</li>
<li>声音</li>
<li>文本</li>
<li>物理世界<ul>
<li>比如欺骗人脸识别系统，去认成另一个人</li>
<li>又比如道路环境，车牌识别等，也可以被攻击</li>
<li>要考虑摄像头能识别的分辨率</li>
<li>要考虑训练时候用的图片颜色与真实世界颜色不一致的问题</li>
</ul>
</li>
<li>Adversarial Reprogramming</li>
<li>Backdoor in Model<ul>
<li>attack happens at the training phase</li>
<li><a href="https://arxiv.org/abs/1804.00792">https://arxiv.org/abs/1804.00792</a></li>
<li>be careful of unknown dataset...</li>
</ul>
</li>
</ul>
<h2>Defence</h2>
<h3>Passive Defense（被动防御）</h3>
<p>进入network前加一层filter</p><ul>
<li>稍微模糊化一点，就去除掉精心设计的noise了<ul>
<li>但是同时也影响了正常的图像</li>
</ul>
</li>
<li>对原图进行压缩</li>
<li>把输入用Generator重新生成一遍</li>
</ul>
<p>如果攻击都知道你怎么做了，其实很好破解，就把你的filter当作network的一部分重新开始设计noise，所以可以选择加入随机选择的一些预处理(让攻击者不可能针对性地训练)：</p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/c5baa988904325835254f3ac7a7ee4e1.png" alt=""/></figure><h3>Proactive Defense（主动防御）</h3>
<p>训练的时候就训练比较不容易被攻破的模型。比如训练过程中加入noise，把生成的结果重新标注回真值。</p><ul>
<li>training model</li>
<li>find the problem</li>
<li>fix it</li>
</ul>
<p>有点类似于<code>Data Augmentation</code></p><p>仍然阻挡不了新的攻击算法，即你对数据进行augment之外的范围。</p><h1>Explainable Machine Learning(可解释性)</h1>
<ul>
<li>correct answers $\neq$ intelligent</li>
<li>很多行业会要求结果必须可解释<ul>
<li>银行，医药，法律，驾驶....</li>
</ul>
</li>
</ul>
<p><strong>Local Explanation</strong></p><p>Why do you thing <strong>this image</strong> is a cat?</p><p><strong>Global Explanation</strong></p><p>What does a &quot;<strong>cat</strong>&quot; look like?</p><ol>
<li>遮挡或改变输入的某些部分，观察对已知输出的影响<ul>
<li>（比如拦到某些部分确实认不出图像是一条狗了）</li>
</ul>
</li>
<li>遮挡或改变输入的某些部分，把两种输出做loss，对比输入变化与loss变化：<ul>
<li>$|\frac{\varDelta e}{\varDelta x}| \rightarrow \frac{\partial e}{\partial x_n}$</li>
</ul>
</li>
</ol>
<p>把上述（任一种）每个部分（像素，单词）的影响结果输出，就是：<code>Saliency Map</code></p><h2>Saliency Map</h2>
<figure  style="flex: 271.9298245614035" ><img width="1240" height="228" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/fdcce76fc7337b0ee787724913f998f0.png" alt=""/></figure><p>图1，2就是为了分辨宝可梦和数码宝贝，人类一般很难区分出来，但机器居然轻松达到了98%的准确率，经过绘制<code>Saliency Map</code>，发现居然就是图片素材（格式）的原因，一个是png，一个是jpg，造成背景一个是透明一个是不透明的。</p><p>也就是说，能发现机器判断的依据不是我们关注的本体（高亮部分就是影响最大的部分，期望是在动物身上）</p><p>第三张图更可笑，机器是如何判断这是一只马的？居然也不是马的本体，而是左下角，标识图片出处的文字，可能是训练过程中同样的logo过多，造成了这个“人为特征”。</p><p>解决方案：</p><h3>Smooth Gradient</h3>
<p>随机给输入图片加入噪点，得到saliency map（们），然后取平均</p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/e672962530cdb4c88d616b3fa82f73f8.png" alt=""/></figure><h3>Integrated gradient(IG)</h3>
<p>一个特征在从无到有的阶段，梯度还是明显的，但是到了一定程度，特征再增强，对gradient影响也不大了，比如从片子来判断大象，到了一定长度，一张图也不会“更像大象”</p><p>一种思路：<a href="https://arxiv.org/abs/1611.02639">https://arxiv.org/abs/1611.02639</a></p><h2>global explaination</h2>
<p><strong>What does a filter detect?</strong></p><p>如果经过某层（训练好的）filter，得到的feature map一些位置的值特别大，那说明这个filter提取的就是这类特征/patten。</p><p>我们去&quot;创造&quot;一张包含了这种patten的图片：$X^* = arg\ \underset{X}{\rm max}\sum_i\sum_j a_{ij}$，即这个图片是“训练/learn“出来的，通过找让X的每个元素($a_{ij}$)在被filter乘加后结果最大的方式。 -&gt; <code>gradient ascent</code></p><p>然后再去观察$X^*$有什么特征，就基本上可以认定这个（训练好的）filter提取的是什么样的patten了。
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/33b78193d5c74e4d146b9efab631c932.png" alt=""/></figure></p><blockquote>
<p><code>adversarial attack</code> 类似的原理，但这是对单filter而言。如果你想用同样的思路去让输出y越大越好，得到X，看X是什么，得到的X大概率都是一堆噪音。如果能生成图像，那是<code>GAN</code>的范畴了。</p></blockquote>
<p>于是，尝试再加一个限制，即不但要让y最大，还要让X看起来最有可能像一个数字：</p><ul>
<li>$R(X)$: how likely X is a digit</li>
<li>$X^* = arg\ \underset{X}{\rm max}y_i + \color{red}{R(X)}$</li>
<li>$R(X) = -\sum_{i,j}|X_{i,j}|$ 比如这个规则，期望每个像素越黑越好</li>
</ul>
<h1>Domain Adaptation</h1>
<p><code>Transfer Learning</code>的一种，在训练数据集和实际使用的数据集不一样的时候。 <a href="https://youtu.be/qD6iD4TFsdQ">https://youtu.be/qD6iD4TFsdQ</a></p><p>需要你对<code>target domain</code>的数据集有一定的了解。</p><p>有一种比较好的情况就是，target domain既有数据，还有标注（但不是太多，如果太多的话就不需要<code>source domain</code>了，直接用target来训练就好了），那就像bert一样，去<code>fine tune</code>结果，要注意的是标本量过小，可能很容易<code>overfitting</code>.</p><p>如果target doamin有<strong>大量</strong>资料，但是没有标注呢？</p><h2>Domain Adversarial Training</h2>
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/7b419953f55daf61a0a4d3f2e77b156d.png" alt=""/></figure><ul>
<li>把source domain的network分为特征提取器（取多少层cnn可以视为超参，并不一定要取所有层cnn）和分类器</li>
<li>然后在特征取层之后跟另一个分类器，用来判断图像来自于source还是target（有点像<code>Discriminator</code></li>
<li>与真值有一个loss，source, target之间也有一个loss，要求找到这样的参数组分别让两个loss最小</li>
<li>loos和也应该最小（图中用的是减，但其实$L_d$的期望是趋近于0，不管是正还是负都是期望越小越好）（不如加个绝对值？）</li>
<li>每一小块都有一组参数，是一起训练的</li>
<li>目的就是既要逼近训练集的真值，还要训练出一个网络能模糊掉source和target数据集的差别</li>
</ul>
<h3>Limit</h3>
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/1c785d92413a44f14ca579564bd3a107.png" alt=""/></figure><p>如果target数据集如上图左，显然结果是会比上图右要差一点的，也就是说尽量要保持同分布。在这里用了另一个角度，就是让数据<strong>离boundary越远越好</strong></p><ul>
<li>Decision-boundary Iterative Refinement Training with a Teacher(<code>DIRT-T</code>)<ul>
<li><a href="https://arxiv.org/abs/1802.08735">https://arxiv.org/abs/1802.08735</a></li>
</ul>
</li>
<li>Maximum Classifier Discrepancy <a href="https://arxiv.org/abs/1712.02560">https://arxiv.org/abs/1712.02560</a></li>
</ul>
<h2>More</h2>
<ul>
<li>如果source 和 target 里的类别不完全一样呢？<ul>
<li>Universal domain adaptation</li>
</ul>
</li>
<li>如果target既没有label，数据量也非常少（比如就一张）呢？<ul>
<li>Test Time Training(TTT) <a href="https://arxiv.org/abs/1909.13231">https://arxiv.org/abs/1909.13231</a></li>
</ul>
</li>
</ul>
<p><strong>Domain Generalization</strong>
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/029818e3d225d57c9b9a866b569983b5.png" alt=""/></figure></p><h1>Deep Reinforcement Learning (RL)</h1>
<ul>
<li><strong>Environment</strong> 给你 <code>Observation</code></li>
<li><strong>Actor</strong> 接收入 <code>Observation</code>, 输出 <code>Action</code></li>
<li><code>Action</code> 反馈给 <strong>Environment</strong>, 计算出 <code>Reward</code> 反馈给 <strong>Actor</strong></li>
<li>要求 <code>Reward</code> 最大</li>
</ul>
<p>与 GAN 的不同之处，不管是生成器还是判别器，都是一个network，而RL里面，Actor和Reward都是黑盒子，你只能看到结果。</p><h2>Policy Gradient</h2>
<p><a href="https://youtu.be/W8XF3ME8G2I">https://youtu.be/W8XF3ME8G2I</a></p><ol>
<li>先是用很类似监督学习的思路，给每一步的最优（或最差）方案一个label，有label就能做loss。先把它变成一个二分类的问题。</li>
<li>打分还可以不仅仅是“好”或“不好”，还可以是一个程度，比如1.5比0.5的“支持”力度要大一些，而-10显然意味着你千万不要这么做，非常拒绝。</li>
<li>比如某一步，可以有三种走法，可以用onehot来表示，其中一种走法可以是[1,0,0]$^T$，表示期望的走法是第一种。</li>
<li>但是也可以是[-1,0,0]$^T$，标识这种走法是不建议的</li>
<li>也可以是[3.5,0,0]$^T$等</li>
<li>后面会用<code>1, -1, 10, 3.5</code>这样的scalar来表示，但要记住其实它们是ont-hot中的那个非零数。</li>
</ol>
<p>现实世界中很多场景不可能执行完一步后就获得reward，或者是全局最佳的reward（比如下围棋）。</p><p><strong>v1</strong></p><p>一种思路是，每一步之后，把游戏/棋局进行完，把当前reward和后续所有步骤的reward加一起做reward -&gt; <code>cumulated reward</code> $\rightarrow G_t = \sum_{n=t}^Nr_n$</p><p><strong>v2</strong></p><p>这种思路仍然有问题，游戏步骤越长，当前步对最终步的影响越小。因此引入一个小于1的权重$\gamma &lt; 1$: $G_1' = r_1 + \gamma r_2 + \gamma^2r_3 + \cdots$</p><p>这样越远的权重越小： $G_t' = \sum_{n=t}^N \color{red}{\gamma^{n-t}} r_n$</p><blockquote>
<p>注意，目前得到的<code>G</code>就是为了给每一次对observation进行的action做loss的对象。</p></blockquote>
<p><strong>v3</strong></p><p>标准化reward。你有10分，是高是低？如果所有人都是20分，那就是低分，所以与G做对比的时候，通常要减去一个合适的值<code>b</code>，让得分的分布有正有负。</p><p><strong>Policy Gradient</strong></p><p>普通的gradient descent是搜集一遍数据，就可以跑for循环了，而PG不行，你每次得到梯度后，要重采一遍样，其实也很好理解，你下了某一步，经过后续50步后，输了，你的下一轮测试应该是下一盘随机的棋，而不是把更新好的参数再用到同一盘棋去。</p><p>还是不怎么好理解，至少要知道，我做参数是不为了训练出这一盘棋是怎么下出来的，而是根据这个（大多是输了的）结果，以及学到的梯度，去下一盘新的棋试试。</p><h2>Actor Critic</h2>
<p><strong>Critic</strong>:</p><ul>
<li>Given <code>actor</code> $\theta$, how good it is when <code>observing</code> s (and taking action a)</li>
</ul>
<p><strong>Value function</strong> $V^\theta(s)$:</p><ul>
<li>使用actor $\theta$的时候，预测会得到多少的<code>cumulated reward</code></li>
<li>分高分低其实还是取决于actor，同样的局面，不同的actor肯定拿的分不同。</li>
</ul>
<h3>Monte-Carlo based approach (MC)</h3>
<p>蒙特卡洛搜索，正常把游戏玩完，得到相应的G.</p><h3>Temporal-difference approach (TD)</h3>
<p>不用玩完整个游戏，就用前后时间段的数据来得到输出。
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/de79fef0ebceeda7cd5011e38ab28c46.png" alt=""/></figure></p><p>关键词：</p><ul>
<li>我们既不知道v(t+1)，也不知道v(t)，但确实能知道<code>v(t+1)-v(t)</code>.</li>
</ul>
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/1fbf32aed94a0b3f47d9b124a466874c.png" alt=""/></figure><p>这个例子没看懂，后面七次游戏为什么没有sa了？</p><p><strong>v3.5</strong></p><p>上文提到的V可以用来作更早提到的b:</p><ul>
<li>${S_t, a_t}\ A_t = G_t' - V^\theta(S_t)$</li>
<li>回顾一下，$V^\theta(S_t)$是看到某个游戏画面时算出来的reward</li>
<li>它包含$S_t$状态下，后续各种步骤的reward的平均值</li>
<li>而$G_t'$则是这一步下的rewared</li>
<li>两个数相减其实就是看你的这一步是比平均水平好还是差</li>
<li>比如你得到了个负值，代表在当前场景下，这个actor执行的步骤是低于平均胜率的，需要换一种走法。</li>
</ul>
<p><strong>v4</strong></p><p>3.5版下，G只有一个样本（一次游戏）的结果，这个版本里，把st再走一步，试难$S_{t+1}$的各种走法下reward的平均值，用它来替换G'，而它的值，就是当前的reward加上t+1时刻的V:</p><ul>
<li>$r_t + V^\theta(S_{t+1}) - V^\theta(S_t)$</li>
</ul>
<p>这就是：</p><h3>Advantage Actor-Critic</h3>
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/a038f8d6237e4d33a943099bdbfdefc3.png" alt=""/></figure><p>就看图而言，感觉就是坚持这一步走完，后续所有可能的rewawrd， 减去， 从这一步开始就试验所有走法的reward</p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/82541f55c2db06d3b1e8339d6e63d140.png" alt=""/></figure><p>More:</p><p>Deep Q Network (DQN)</p><ul>
<li><a href="https://arxiv.org/abs/1710.02298">https://arxiv.org/abs/1710.02298</a></li>
<li><a href="https://youtu.be/o_g9JUMw1Oc">https://youtu.be/o_g9JUMw1Oc</a></li>
<li><a href="https://youtu.be/2-zGCx4iv_k">https://youtu.be/2-zGCx4iv_k</a></li>
</ul>
<h2>Reward Shaping</h2>
<p>前面说过很多场景要得到reward非常困难（时间长，步骤长，或根本不会结束），这样的情况叫<code>sparse reward</code>，人类可以利用一些已知知识去人为设置一些reward以增强或削弱机器的某些行为。</p><p>比如游戏：</p><ol>
<li>原地不动一直慢慢减分</li>
<li>每多活一秒也慢慢减分（迫使你去获得更高的reward, 避免学到根本就不去战斗的方式）</li>
<li>每掉一次血也减分</li>
<li>每杀一个敌人就加分</li>
<li>以此类推，这样就不至于要等到一场比赛结束才有“一个”reward</li>
</ol>
<p>又比如训练机械手把一块有洞的木板套到一根棍子上：</p><ol>
<li>离棍子越近，就有一定的加分</li>
<li>其它有助于套进去的规则</li>
</ol>
<p>还可以给机器加上<strong>好奇心</strong>，让机器看到有用的“新的东西”也加分。</p><h2>No Reward, learn from demostration</h2>
<p>只有游戏场景才会有明确的reward，大多数现实场景都是没有reward的，比如训练自动驾驶的车，或者太过死板的reward既不能适应变化，也容易被打出漏洞，比如机器人三定律里，机器人不能伤害人类，却没有禁止囚禁人类，又比如摆放盘子，却没有给出力度，等盘子摔碎了，再去补一条𢱨碎盘子就负reward的规则，也晚了，由此引入模仿学习：</p><h3>Imitation Learning</h3>
<p>略</p><h1>Life-Long Learning</h1>
<p>持续学习，机器学习到一个模型后，继续学下一个模型（任务）。</p><ol>
<li>为什么不一个任务学一个模型<ul>
<li>不可能去存储所有的模型</li>
<li>一个任务的知识不能转移到另一个任务</li>
</ul>
</li>
<li>为什么不直接用迁移学习（迁移学习只关注迁移后的新任务）</li>
</ol>
<h2>Research Directions</h2>
<h3>Selective Synaptic Plasticity</h3>
<p>选择性的神经突触的可塑性？（Regularization-based Approach）</p><p><strong>Catastrophic Forgetting</strong> 灾难性的遗忘
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/207da40b7ff67f6534e5fc5de208cce9.png" alt=""/></figure></p><p>在任务1上学到的参数，到任务2里接着训练，顺着梯度到了任务2的最优参数，显然不再是任务1的做以参，这叫灾难性的遗忘</p><p>一种思路：</p><p>任务2里梯度要更新未必要往中心，也可以往中下方，这样既在任务2的低loss区域，也没有跑出任务1的低loss区域，实现的方式是找到对之前任务影响比较小的参数，主要去更新那些参数。比如上图中，显然$\theta_1$对任务1的loss影响越小，但是更新它之后会显著影响任务2的loss，而$\theta_2$的改动才是造成任务1loss变大的元凶。</p><figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/1b27e4cf8478e749d8899d30b75bbb22.png" alt=""/></figure><p>Elastic Weight Consolidation(EWC)</p><ul>
<li><a href="https://arxiv.org/abs/1612.00796">https://arxiv.org/abs/1612.00796</a></li>
</ul>
<p>Synaptic Intelligence(SI)</p><ul>
<li><a href="https://arxiv.org/abs/1703.04200">https://arxiv.org/abs/1703.04200</a></li>
</ul>
<p>Memory Aware Synapses(MAS)</p><ul>
<li><a href="https://arxiv.org/abs/1711.09601">https://arxiv.org/abs/1711.09601</a></li>
</ul>
<p>RWalk</p><ul>
<li><a href="https://arxiv.org/abs/1801.10112">https://arxiv.org/abs/1801.10112</a></li>
</ul>
<p>Sliced Cramer Preservation(SCP)</p><ul>
<li><a href="https://openreview.net/forum?id=BJge3TNKwH">https://openreview.net/forum?id=BJge3TNKwH</a></li>
</ul>
<figure  style="flex: 132.76231263383298" ><img width="1240" height="467" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/474bbac22bf3778191de7bf09d10d69d.png" alt=""/></figure><h3>Memory Reply</h3>
<ol>
<li>在训练task1的时候，同时训练一个相应的generator</li>
<li>训练task2的时候，用task1的generator生成pseudo-data，一起来训练生成新的model</li>
<li>同时也训练出一个task1&amp;2的generator</li>
<li>...</li>
</ol>
<h1>Network Compress</h1>
<h2>pruning (剪枝)</h2>
<p>Networks ar typically over-parameterized (there is significant redundant weights or neurons)</p><ul>
<li>可以看哪些参数通常比较大，或值的变化不影响loss（梯度小）-&gt; 权重，为0的次数少 -&gt; 神经元 等等</li>
<li>剪枝后精度肯定是会下降的</li>
<li>需要接着fine-tune</li>
<li>一次不要prune to much</li>
<li>剪参数和剪神经元效果是不一样的<ul>
<li>剪参数会影响矩阵运算，继而影响GPU加速</li>
</ul>
</li>
</ul>
<p>那么为什么不直接train一个小的network呢？</p><ul>
<li>小的network通常很难train到同样的准确率。 （大乐透假说）</li>
</ul>
<h2>Knowledge Distillation (知识蒸馏)</h2>
<p>老师模型训练出来的结果，用学生模型（小模型）去模拟（即是模拟整个输出，而不是模拟分类结果），让小模型能达到大模型同样的结果。</p><p>一般还会在输出的softmax里面加上温度参数（即平滑输出，不同大小的数除一个大于1的数，显然越大被缩小的倍数也越大，比如100/10=10，少了90，10/10=1, 只少了9，差别也从90变成了9）(或者兴趣个极端的例子，T取无穷大，那么每个输出就基本相等了)</p><h2>Parameter Quantization</h2>
<ol>
<li>Using less bits to represent a value</li>
<li>Weight clustering<ul>
<li>把weights分成预先确定好的簇（或根据分布来确定）</li>
<li>对每簇取均值，用均值代替整个簇里所有的值</li>
</ul>
</li>
<li>represent frequent clusters by less bits, represent rare clusters by more bits<ul>
<li>Huffman encoding</li>
</ul>
</li>
</ol>
<p>极限，<code>Binary Weights</code>，用两个bits来描述整个网络，扩展阅读。</p><h2>Depthwise Separable Convolution</h2>
<p>回顾下CNN的机制，参数量是：</p><ul>
<li>卷积核的大小 x 输入图像的通道数 x 输出的通道数</li>
<li>($k\times k$) x in_channel x out_channel</li>
</ul>
<figure  style="flex: 66.66666666666667" ><img width="960" height="720" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/769ea8db15464e205b9c674747eb650d.png" alt=""/></figure><p>而<code>Depthwise Separable Convolution</code>由两个卷积组成：</p><ol>
<li>Depthwise Convolution<ul>
<li>很多人对CNN的误解刚好就是Depthwise Convolution的样子，即一个卷积核对应一个输入的channel（事实上是一组卷积核对应所有的输入channel）</li>
<li>因此它的参数个数 k x k x in_channel</li>
</ul>
</li>
<li>PointWise Convolution<ul>
<li>这里是为了补上通道与通道这间的关系</li>
<li>于是用了一个1x1的<code>标准</code>卷积（即每一组卷积核对应输入的所有通道）</li>
<li>输出channel也由这次卷积决定</li>
<li>应用标准卷积参数量：(1x1) x in_channel x out_channel</li>
</ul>
</li>
</ol>
<figure  style="flex: 130.52631578947367" ><img width="1240" height="475" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/e147af5889954aa781498c5977f659f1.png" alt=""/></figure><p>两个参数量做对比, 设<code>in_channel = I</code>, <code>out_channel = O</code></p><ol>
<li>$p_1 = (k\times k) \times I \times O$</li>
<li>$p_2 = (k\times k) \times I + (1\times 1) \times I \times O = (k\times k) \times I + I \times O$</li>
<li>$\frac{p_2}{p_1} = \frac{I\cdot(k^2 + O)}{I\cdot{k^2\cdot O}}</li>
</ol>
<p>= \frac{1}{O} + \frac{1}{k^2} \approx \frac{1}{k^2} 
$</p><p>O代表out_channel，大型网络里256，512比比皆是，所以它可以忽略，那么前后参数量就由$k^2$决定了，如果是大小为3的卷积核，参数量就变成1/9了，已经是压缩得很可观了。</p><h3>Low rank approximation</h3>
<p>上面是应用，原理就是<code>Low rank approximation</code></p><p>以全连接网络举例</p><ol>
<li>如果一个一层的网络，输入<code>N</code>， 输出<code>M</code>，参数为<code>W</code>，那么参数量是<code>MxN</code></li>
<li>中间插入一个线性层<code>K</code>，<ul>
<li>参数变成：<code>V</code>:N-&gt;K, <code>U</code>:K-&gt;M,</li>
<li>参数量：<code>NxK</code> + <code>KxM</code></li>
</ul>
</li>
<li>只要K远小于M和N（比如数量级都不一致），那么参数量是比直接MxN要小很多的</li>
<li>这也限制了能够学习的参数的可能性（毕竟原始参数量怎么取都行）<ul>
<li>所以叫<code>Low rank</code> approximation</li>
</ul>
</li>
</ol>
<p><strong>to learn more</strong></p><p>SqueezeNet</p><ul>
<li><a href="https://arxiv.org/abs/1602.07360">https://arxiv.org/abs/1602.07360</a></li>
</ul>
<p>MobileNet</p><ul>
<li><a href="https://arxiv.org/abs/1704.04861">https://arxiv.org/abs/1704.04861</a></li>
</ul>
<p>ShuffleNet</p><ul>
<li><a href="https://arxiv.org/abs/1707.01083">https://arxiv.org/abs/1707.01083</a></li>
</ul>
<p>Xception</p><ul>
<li><a href="https://arxiv.org/abs/1610.02357">https://arxiv.org/abs/1610.02357</a></li>
</ul>
<p>GhostNet</p><ul>
<li><a href="https://arxiv.org/abs/1911.11907">https://arxiv.org/abs/1911.11907</a></li>
</ul>
<h2>Dynamic Computation</h2>
<ol>
<li>同一个网络，自己来决定计算量，比如是在不同的设备上，又或者是在同设备的不同时期（比如闲时和忙时，比如电量充足和虚电时）</li>
<li>为什么不为不同的场景准备不同的model呢？<ul>
<li>反而需要更大的存储空间，与问题起源（资源瓶颈）冲突了。</li>
</ul>
</li>
</ol>
<h3>Dynamic Depth</h3>
<p>在部分layer之后，每一层都插一个额外的layer，提前做预测和输出，由调用者根据具体情况决定需要多深的depth来产生输出。</p><p>训练的时候既要考虑网络终点的loss，还要考虑所有提前结束的layer的softmax结果，加到一起算个大的Loss</p><p>Multi-Scale Dense Network(MSDNet)</p><ul>
<li><a href="https://arxviv.org/abs/1703.09844">https://arxviv.org/abs/1703.09844</a></li>
</ul>
<h3>Dynamic Width</h3>
<p>训练的时候（同时？）对不同宽度（即神经元个数，或filter个数）进行计算（全部深度），也是把每种宽度最后产生的loss加起来当作总的Loss</p><p>在保留的宽度里，参数是一样的（所以应该就是同一轮训练里的参数了）</p><p>Slimmable Neural Networks</p><ul>
<li><a href="https://arxiv.org/abs/1812.08928">https://arxiv.org/abs/1812.08928</a></li>
</ul>
<h3>Computation based on Sample Difficulty</h3>
<p>上述决定采用什么样的network/model的是人工决定的，那么有没有让机器自己决定采用什么网络的呢？</p><p>比如一张简单的图片，几层或一层网张就能得到结果，而另一张可能前景和或背景更复杂的图片，需要很多层才能最终把特征提取出来，应用同一个模型的话就有点资源浪费了。</p><ul>
<li>SkipNet: Learning Dynamic Routing in Convolutional Networks</li>
<li>Runtime Neural Pruning</li>
<li>BlockDrop: Dynamic Inference Paths in Residual Networks</li>
</ul>
<h1>Meta Learning</h1>
<ul>
<li>学习的学习。</li>
<li>之前的machine learning，输出是明确的任务，比如是一个数字，还是一个分类；而meta-learning，输出是一个model/network，用这个model，可以去做machine learning的任务。</li>
<li>它就相当于一个“返函数的函数”</li>
<li>meta-learning 就是让机器学会去架构一个网络，初始化，学习率等等 $\leftarrow \varPhi$: <code>learnable components</code><ul>
<li>categorize meta learning based on what is learnable</li>
</ul>
</li>
</ul>
<blockquote>
<p>不再深入</p></blockquote>
</div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/%E6%9D%8E%E5%AE%8F%E6%AF%85MACHINE-LEARNING-2021-SPRING%E7%AC%94%E8%AE%B0/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/%E4%B8%80%E5%BC%A0%E5%9B%BE%E8%AF%B4%E6%B8%85%E5%8C%88%E7%89%99%E5%88%A9%E7%AE%97%E6%B3%95%EF%BC%88Hungarian-Algorithm%EF%BC%89/" target="_self">一张图说清匈牙利算法（Hungarian-Algorithm）</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/%E4%B8%80%E5%BC%A0%E5%9B%BE%E8%AF%B4%E6%B8%85%E5%8C%88%E7%89%99%E5%88%A9%E7%AE%97%E6%B3%95%EF%BC%88Hungarian-Algorithm%EF%BC%89/" target="_self">
                <time class="text-uppercase">
                    September 23 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><p>做多目标跟踪的时候会碰到这个算法，每个人都有自己的说法讲清楚这个算法是干什么的？我的老师就跟我说过是什么给工人分配活干（即理解为<code>指派问题</code>），网上还看到有说红娘尽可能匹配多的情侣等，透过这些感性理解，基本上就能理解大概是最大匹配的问题了。</p><p>然后加了限制：后来者优先。即后匹配的<strong>能</strong>抢掉前人已匹配的对象，这个是有数学依据还是只是一种实现思路我就没深究了。</p><p>我的理解不会比别人更高级，之所以能用一张图说清楚，只不过是我作图的时候发现可以把过程画在一张图里，只需要把图示标清楚就好了，这样就不需要每一步画一张图了，一旦理解了，哪怕忘了，一瞅这张图也能立刻回忆起来。</p><p>先上数据：</p><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">relationship_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
</pre></div>
<p>你可以理解为6个工人，7个工作，6个男孩，7个女孩等，当然，6行7列，这么直观理解也是一点问题都没有的。</p><p>算法匹配过程如下：
<figure class="vertical-figure" style="flex: 38.028169014084504" ><img width="648" height="852" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/cf127d2b3fa84eb6c94999802507f38f.png" alt=""/></figure></p><ul>
<li>灰蓝线就是被抢掉的</li>
<li>绿线就是抢夺失败的</li>
<li><code>紫线</code>是被抢了后找候选成功的</li>
<li><code>红线</code>是一次性成功的</li>
</ul>
<p>其中被抢的和抢夺失败的还加了删除线，这是为了强调。匹配成功的就是<code>红线</code>和<code>紫线</code>，也就是说，我们匹配出来的是：</p><div class="highlight"><pre><span></span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span>
</pre></div>
<p>甚至可以这么表示这个过程：</p>
<pre><code>x0,y0
x1,y1
x2,y0 -&gt; x0,y1 -&gt; x1-&gt;y4 (x2抢x0的,x0抢x1的)
x3,y2
x4,y3
x5,y3 -&gt; x4匹配不到新的，抢夺失败，-&gt; x5,null
</code></pre>
<p>有没有说清楚？就两步：</p><ol>
<li>根据关联表直接建立关系</li>
<li>如果当前<code>C</code>匹配的对象已经被<code>B</code>匹配过了，那么尝试把它抢过来：</li>
</ol>
<ul>
<li><code>B</code>去找别的匹配<ul>
<li>找到了(<code>A</code>)就建立新的匹配<ul>
<li>如果新的匹配(<code>A</code>)也已经被别人(<code>D</code>)匹配了，那么那个“别人(<code>D</code>)”也放弃当前匹配去找别的（<em>递归警告</em>）</li>
</ul>
</li>
<li>如果找不到新的匹配，那么<code>C</code>抢夺失败，递归中的<code>D</code>也同理，失败向上冒泡</li>
</ul>
</li>
</ul>
<p>注意递归怎么写代码就能写出来了：</p><div class="highlight"><pre><span></span><span class="n">nx</span><span class="p">,</span> <span class="n">ny</span> <span class="o">=</span> <span class="n">relationship_matrix</span><span class="o">.</span><span class="n">shape</span>    <span class="c1"># 6个x，7个y</span>

<span class="c1"># 如果x0与y0关联，x3也与y0关联，那么x0去找新的匹配时，需要把y0过滤掉</span>
<span class="c1"># 同理x0如果找到下一个y2，y2已被x2关联，那么x2找新的匹配时[y0, y2]都需要过滤掉</span>
<span class="c1"># 我们把这个数组存为y_used</span>
<span class="n">y_used</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">ny</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>  <span class="c1"># 存y是否连接上</span>
<span class="n">path</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">ny</span><span class="p">,),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>  <span class="c1"># 存x连接的对象，没有为-1</span>

<span class="k">def</span> <span class="nf">find_other_path_and_used</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ny</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">relationship_matrix</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">y_used</span><span class="p">[</span><span class="n">y</span><span class="p">]:</span>
            <span class="n">y_used</span><span class="p">[</span><span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>        <span class="c1"># 处于争夺中的y，需要打标，在后续的递归时要过滤掉</span>
            <span class="k">if</span> <span class="n">path</span><span class="p">[</span><span class="n">y</span><span class="p">]</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">or</span> <span class="n">find_other_path_and_used</span><span class="p">(</span><span class="n">path</span><span class="p">[</span><span class="n">y</span><span class="p">]):</span>
                <span class="n">path</span><span class="p">[</span><span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>         <span class="c1"># 直接连接 和 抢夺成功</span>
                <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span>                    <span class="c1"># 抢夺失败 和 默认失败</span>

<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nx</span><span class="p">):</span>
    <span class="n">y_used</span><span class="p">[:]</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># empty</span>
    <span class="n">find_other_path_and_used</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">for</span> <span class="n">y</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">x</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
<p>真的写代码实现的时候，难点反而是<code>y_used</code>这个，第一遍代码没考虑这一点，导致递归的时候每次都从$y_0$开始而出现死循环，意识到后把处于争抢状态中的<code>y</code>打个标就好了。</p><p>scipy中有一个算法实现了Hungarian algorithm：</p><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">linear_sum_assignment</span>

<span class="c1"># relationship_matrix是代价矩阵</span>
<span class="c1"># 所以我们要代价越小越好，就用1来减</span>
<span class="n">rows</span><span class="p">,</span> <span class="n">cols</span> <span class="o">=</span> <span class="n">linear_sum_assignment</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">relationship_matrix</span><span class="p">)</span> 
<span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">))</span>
</pre></div>

<pre><code>[(0, 0), (1, 1), (2, 6), (3, 2), (4, 3), (5, 4)]
</code></pre>
<p>为什么与上面不一样呢？</p><ol>
<li>（0，0），（1，1）的匹配显然不是我们实现的后来者优先</li>
<li>他把行看成是工人，列看成是任务，每个工人总要分配个任务，所以(5,4)这种代价矩阵里没有的关联它也做出来了，目的只是让“总代价”最小</li>
</ol>

<pre><code>(1-relationship_matrix)[rows, cols]  # 总代价为1
</code></pre>

<pre><code>array([0, 0, 0, 0, 0, 1])
</code></pre>
<p>从它的名字也能看出来，它是理解为<code>指派问题</code>的(<code>assignment</code>)</p></div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/%E4%B8%80%E5%BC%A0%E5%9B%BE%E8%AF%B4%E6%B8%85%E5%8C%88%E7%89%99%E5%88%A9%E7%AE%97%E6%B3%95%EF%BC%88Hungarian-Algorithm%EF%BC%89/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/Deep-Learning-with-Python-Notes/" target="_self">《Deep Learning with Python》笔记</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/Deep-Learning-with-Python-Notes/" target="_self">
                <time class="text-uppercase">
                    September 12 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><p>本来是打算趁这个时间好好看看花书的，前几章看下来确实觉得获益匪浅，但看下去就发现跟不上了，特别是抱着急功近利的心态的话，目前也沉不下去真的一节节吃透地往下看。这类书终归不是入门教材，是需要你有过一定的积累后再回过头来看的。</p><p>于是想到了《Deep Learning with Python》，忘记这本书怎么来的了，但是在别的地方看到了有人推荐，说是Keras的作者写的非常好的一本入门书，翻了前面几十页后发现居然跟进去了，不该讲的地方没讲比如数学细节，而且思路也极其统一，从头贯穿到尾（比如representations, latent space,  hypothesis space），我觉得很受用。</p><p>三百多页全英文，居然也没查几个单词就这么看完了，以前看文档最多十来页，也算一个突破了，可见其实还是一个耐心的问题。</p><p>看完后书上做了很多笔记，于是顺着笔记读了第二遍，顺便就把笔记给电子化了。不是教程，不是导读。</p><h1>Fundamentals of deep learning</h1>
<p><strong>核心思想</strong>：
learng useful <code>representations</code> of input data</p><blockquote>
<p>what’s a <code>representation</code>?</p><p>At its core, it’s a different way to look at data—to represent or encode data.</p></blockquote>
<p>简单回顾深度学习之于人工智能的历史，每本书都会写，但每本书里都有作者自己的侧重：</p><ul>
<li>Artificial intelligence</li>
<li>Machine learning<ul>
<li>Machine learning is tightly related to <code>mathematical statistics</code>, but it differs from statistics in several important ways.<ul>
<li>machine learning tends to deal with large, complex datasets (such as a dataset of millions of images, each consisting of tens of thousands of pixels)</li>
<li>classical statistical analysis such as Bayesian analysis would be impractical(不切实际的).</li>
<li>It’s a hands-on discipline in which ideas are proven empirically more often than theoretically.（工程/实践大于理论）</li>
</ul>
</li>
<li>是一种meaningfully transform data<ul>
<li>Machine-learning models are all about finding appropriate representations for their input data—transformations of the data that make it more amenable to the task at hand, such as a classification task.</li>
<li>寻找更有代表性的representation, 通过:(coordinate change, linear projections, tranlsations, nonlinear operations)</li>
<li>只会在<code>hypothesis space</code>里寻找</li>
<li>以某种反馈为信号作为优化指导</li>
</ul>
</li>
</ul>
</li>
<li>Deep learning<ul>
<li>Machine Learing的子集，一种新的learning representation的新方法</li>
<li>虽然叫神经网络(<code>neural network</code>)，但它既非neural，也不是network，更合理的名字：<ul>
<li><code>layered representations learning</code> and <code>hierarchical representations learning</code>.</li>
</ul>
</li>
<li>相对少的层数的实现叫<code>shallow learning</code></li>
</ul>
</li>
</ul>
<h2>Before deep learning</h2>
<ul>
<li>Probabilistic modeling<ul>
<li>the earliest forms of machine learning,</li>
<li>still widely used to this day.<ul>
<li>One of the best-known algorithms in this category</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>is the <code>Naive Bayes algorithm</code>(朴素贝叶斯)
    * 条件概率，把规则理解为“条件”，判断概率，比如垃圾邮件。
        * A closely related model is the logistic regression</p><ul>
<li>Early neural networks<ul>
<li>in the mid-1980s, multiple people independently rediscovered the Backpropagation algorithm</li>
<li>The <code>first</code> successful practical application of neural nets came in 1989 from Bell Labs -&gt; <strong>LeNet</strong></li>
</ul>
</li>
<li>Kernel methods<ul>
<li>Kernel methods are <code>a group of classification algorithms</code>(核方法是一组分类算法)<ul>
<li>the best known of which is the <code>support vector machine</code> (<strong>SVM</strong>).</li>
<li>SVMs aim at solving classification problems <strong>by</strong> finding good <em>decision boundaries</em> between two sets of points belonging to two different categories.<ol>
<li>先把数据映射到高维，decision boundary表示为<code>hyperplane</code></li>
<li>最大化每个类别里离hyperplane最近的点到hyperplane的距离:<code>maximizing the margin</code></li>
</ol>
</li>
<li>The technique of mapping data to a high-dimensional representation 非常消耗计算资源，实际使用的是核函数(<code>kernel function</code>):<ul>
<li>不把每个点转换到高维，而只是计算每两个点在高维中的距离</li>
<li>核函数是手工设计的，不是学习的</li>
</ul>
</li>
<li>SVM在分类问题上是经典方案，但难以扩展到大型数据集上</li>
<li>对于perceptual problems(感知类的问题)如图像分类效果也不好<ul>
<li>它是一个<code>shallow method</code></li>
<li>需要事先手动提取有用特征(<code>feature enginerring</code>)-&gt; difficult and  brittle（脆弱的）</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Decision trees, random forests, and gradient boosting machines<ul>
<li>Random Forest<ul>
<li>you could say that they’re almost always the <em>second-best</em> algorithm for any shallow machine-learning task.</li>
</ul>
</li>
<li>gradient boosting machines (1st):<ul>
<li>a way to improve any machine-learning model by iteratively training new models that specialize in <code>addressing the weak points of the previous models</code>.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>What makes deep learning different</h2>
<p>it completely automates what <em>used to be</em> <strong>the most crucial step</strong> in a machine-learning workflow: <code>feature engineering</code>. 有人认为这叫穷举，思路上有点像，至少得到特征的过程不是靠观察和分析。</p><p><strong>feature engineering</strong></p><blockquote>
<p>manually engineer good layers of representations for their data</p></blockquote>
<h1>Getting started with neural networks</h1>
<h2>Anatomy of a neural network</h2>
<ul>
<li><code>Layers</code>, which are combined into a <code>network</code> (or model)<ul>
<li>layers: 常见的比如卷积层，池化层，全连接层等</li>
<li>models: layers构成的网络，或多个layers构成的模块（用模块组成网络）<ul>
<li>Two-branch networks</li>
<li>Multihead networks</li>
<li>Inception blocks, residual blocks etc.</li>
</ul>
</li>
<li>The topology of a network defines a hypothesis space</li>
<li>本书反复强调的就是这个<code>hypothesis space</code>，一定要理解这个思维：<ul>
<li>By choosing a network topology, you <code>constrain</code> your space of possibilities (hypothesis space) to a specific series of tensor operations, mapping input data to output data.（network的选择约束了tensor变换的步骤）</li>
<li>所以如果选择了不好的network，可能导致你在错误的<code>hyposhesis space</code>里搜索，以致于效果不好。</li>
</ul>
</li>
</ul>
</li>
<li>The <code>input data</code> and corresponding <code>targets</code></li>
<li>The <code>loss</code> function (objective function), which defines the <code>feedback signal</code> used for learning<ul>
<li>The quantity that will be minimized during training.</li>
<li>It represents a measure of success for the task at hand.</li>
<li>多头网络有多个loss function，但基于<code>gradient-descent</code>的网络只允许有一个标量的loss，因此需要把它合并起来（相加，平均...）</li>
</ul>
</li>
<li>The <code>optimizer</code>, which determines how learning proceeds<ul>
<li>Determines how the network will be updated based on the loss function.</li>
<li>It implements a specific variant of stochastic gradient descent (SGD).</li>
</ul>
</li>
</ul>
<h3>Classifying movie reviews: a binary classification example</h3>
<p><strong>一个二元分类的例子</strong></p><p>情感分析/情绪判断，数据源是IMDB的影评数据.</p><p><strong>理解hidden的维度</strong></p><p>how much freedom you’re allowing the network to have when learning internal representations. 即学习表示（别的地方通常叫提取特征）的自由度。</p><p>目前提出了架构网络的时候的两个问题：</p><ol>
<li>多少个隐层</li>
<li>隐层需要多少个神经元（即维度）</li>
</ol>
<p>后面的章节会介绍一些原则。</p><p><strong>激活函数</strong></p><p>李宏毅的课程里，从用整流函数来逼近非线性方程的方式来引入激活函数，也就是说在李宏毅的课程里，激活函数是<strong>因</strong>，推出来的公式是<strong>果</strong>，当然一般的教材都不是这个角度，都是有了线性方程，再去告诉你，这样还不够，需要一个<code>activation</code>。</p><p>本书也一样，告诉你，如果只有<code>wX+b</code>，那么只有线性变换，这样会导致对<code>hypothesis space</code>的极大的限制，为了扩展它的空间，就引入了非线性的后续处理。总之，都是在自己的逻辑体系内的。本书的逻辑体系就是<code>hypothesis space</code>，你想要有解，就是在这个空间里。</p><p><strong>网络结构</strong></p><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">models</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10000</span><span class="p">,)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">))</span>
</pre></div>
<p><strong>entropy</strong></p><p><code>Crossentropy</code> is a quantity from the field of Information Theory（信息论） that measures the distance between probability distributions。</p><p>in this case, between the ground-truth distribution and your predictions.</p><p><strong>keras风格的训练</strong></p><p>其实就是模仿了<code>scikit learn</code>的风格。对快速实验非常友好，缺点就是封装过于严重，不利于调试，但这其实不是问题，谁也不会只用keras。</p><div class="highlight"><pre><span></span><span class="c1"># 演示用类名和字符串分别做参数的方式</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;rmsprop&#39;</span><span class="p">,</span>
            <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span>
            <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>

<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">optimizers</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizers</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">),</span>
            <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span>
            <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>

<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizers</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">),</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">losses</span><span class="o">.</span><span class="n">binary_crossentropy</span><span class="p">,</span>
            <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">metrics</span><span class="o">.</span><span class="n">binary_accuracy</span><span class="p">])</span>

<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;rmsprop&#39;</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;acc&#39;</span><span class="p">])</span>

<span class="c1"># train</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">partial_x_train</span><span class="p">,</span>
                    <span class="n">partial_y_train</span><span class="p">,</span>
                    <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">))</span>
</pre></div>
<p>后续优化，就是对比train和validate阶段的loss和accuracy，找到overfit的节点（比如是第N轮），然后重新训练到第N轮（或者直接用第N轮生成的模型，如果有），用这个模型来预测没有人工标注的数据。</p><p>核心就是要<strong>训练到明显的overfit</strong>为止。这是第一个例子的内容，所以是告诉你怎么用这个简单的网络来进行预测，而不是立即着眼怎么去解决overfit.</p><p><strong>第一个小结</strong></p><ol>
<li>数据需要预处理成tensor, 了解几种tensor化，或vector化的方式</li>
<li>堆叠全连接网络(Dense)，以及activation，就能解决很多分类问题</li>
<li>二元分类的问题通常在Dense后接一个sigmoid函数</li>
<li>引入二元交叉熵(BCE)作为二元分类问题的loss</li>
<li>用了rmsprop优化器，暂时没有过多介绍。这些优化器都是为了解决能不能找到局部极值而进行的努力，具体可看上一篇李宏毅的笔记</li>
<li>使用overfit之前的那一个模型来做预测</li>
</ol>
<h3>Classifying newswires: a multiclass classification example</h3>
<p>这次用路透社的新闻来做多分类的例子，给每篇新闻标记类别。</p><p><strong>预处理，一些要点</strong>:</p><ol>
<li>不会采用所有的词汇，所以预处理时，根据词频，只选了前1000个词</li>
<li>用索引来实现文字-数字的对应</li>
<li>用one-hot来实现数字-向量的对应</li>
<li>理解什么是序列（其实就是一句话）</li>
<li>所以句子有长有短，为了矩阵的批量计算（即多个句子同时处理），需要“对齐”（补0和截断）</li>
<li>理解稠密矩阵(word-embedding)与稀疏矩阵(one-hot)的区别（这里没有讲，用的是one-hot)</li>
</ol>
<p><strong>网络和训练</strong></p><ol>
<li>网络结构不变，每层的神经元为(64, 64, 46)</li>
<li>前面增加了神经元，16个特征对语言来说应该是不够的）</li>
<li>最后一层由1变成了46，因为二元的输出只需要一个数字，而多元输出是用one-hot表示的向量，最有可能的类别在这个向量里拥有最大的值。</li>
</ol>
<p>4。 损失函数为<code>categorial_crossentropy</code>，这在别的教材里应该就是普通的CE.</p><p><strong>新知识</strong></p><ol>
<li>介绍了一种不用one-hot而直接用数字表示真值的方法，但是没有改变网络结构（即最后一层仍然输出46维，而不是因为你用了一个标量而只输出一维。<ul>
<li>看来它仅仅就是一个<strong>语法糖</strong>（loss函数选择<code>sparse_categorial_crossentropy</code>就行了）</li>
</ul>
</li>
<li>尝试把第2层由64改为4，变成<code>bottleneck</code>，演示你有46维的数据要输出的话，前面的层数或少会造成信息压缩过于严重以致于丢失特征。</li>
</ol>
<h3>Predicting house prices: a regression example</h3>
<p>这里用了预测房价的Boston Hosing Price数据集。</p><p>与吴恩达的课程一样，也恰好是在这个例子里引入了对input的normalize，理由也仅仅是简单的把量纲拉平。现在我们应该还知道Normalize还能让数据在进入激活函数前，把值限定在激活函数的梯度敏感区。</p><p>此外，一个知识点就是你对训练集进行Normalize用的均值和标准差，是直接用在测试集上的，而不是各计算各的，可以理解为保持训练集的“分布”。</p><blockquote>
<p>这也是<code>scikit learn</code>里<code>fit_tranform</code>和直接用<code>transform</code>的原因。</p></blockquote>
<ol>
<li>对scalar进行预测是不需要进行激活（即无需把输出压缩到和为1的概率空间）</li>
<li>loss也直观很多，就是predict与target的差（取平方，除2，除批量等都是辅助），预测与直值的差才是核心。</li>
</ol>
<h1>Fundamentals of machine learning</h1>
<ul>
<li>Supervised learning<ul>
<li>binary classification</li>
<li>multiclass classificaiton</li>
<li>scalar regression</li>
<li>vector regression（比如bounding-box)</li>
<li>Sequence generation (摘要，翻译...)</li>
<li>Syntax tree prediction</li>
<li>Object detection (一般bounding-box的坐标仍然是回归出来的)</li>
<li>Image segmentation</li>
</ul>
</li>
<li>Unsupervised learing<ul>
<li>是数据分析的基础，在监督学习前也常常需要用无监督学习来更好地“理解”数据集</li>
<li>主要有降维(<code>Dimensionality reduction</code>)和聚类(<code>clustering</code>)</li>
</ul>
</li>
<li>Self-supervised learning<ul>
<li>其实还是监督学习，因为它仍需要与某个target做比较</li>
<li>往往半监督（自监督）学习仍然有小量有标签数据集，在此基础上训练的不完善的model用来对无标签的数据进行打标，循环中对无标签数据打标的可靠度就越来越高，这样总体数据集的可靠度也越来越高了。有点像生成对抗网络里生成器和辨别器一同在训练过程中完善。</li>
<li><code>autoencoders</code></li>
</ul>
</li>
<li>Reinforcement learning<ul>
<li>an <code>agent</code> receives information about its <code>environment</code> and learns to choose <code>actions</code> that will maximize some <code>reward</code>.</li>
<li>可以用训练狗来理解</li>
<li>工业界的应用除了游戏就是机器人了</li>
</ul>
</li>
</ul>
<h2>Data preprocessing</h2>
<ul>
<li>vectorization</li>
<li>normalization (small, homogenous)</li>
<li>handling missing values<ol>
<li>除非0有特别的含义，不然一般可以对缺失值补0</li>
<li>你不能保证测试集没有缺失值，如果训练集没看到过缺失值，那么将不会学到忽略缺失值<ul>
<li><em>复制</em>一些训练数据并且随机drop掉一些特征</li>
</ul>
</li>
</ol>
</li>
<li>feature extraction<ul>
<li>making a problem easier by expressing it in a simpler way. It usually requires understanding the problem <strong>in depth</strong>.</li>
<li><strong>Before</strong> deep learning, feature engineering used to be <code>critical</code>, because classical <strong>shallow algorithms</strong> didn’t have <code>hypothesis spaces</code> rich enough to learn useful features by themselves. (又见假设空间)</li>
<li>但是好的特征仍然能让你在处理问题上更优雅、更省资源，也能减小对数据集规模的依赖。</li>
</ul>
</li>
</ul>
<h2>Overfitting and underfitting</h2>
<ul>
<li>Machine learning is the tension between <code>optimization</code> and <code>generalization</code>.</li>
<li>optimization要求你在训练过的数据集上能达到最好的效果</li>
<li>generalization则希望你在没见过的数据上有好的效果</li>
<li>如果训练集上loss小，测试集上也小，说明还有优化(optimize)的余地 -&gt; <code>underfitting</code>看loss<ul>
<li>just keep training</li>
</ul>
</li>
<li>如果验证集上generalization stop improving(泛化不再进步，一般看衡量指标，比如准确率) -&gt; <code>overfitting</code></li>
</ul>
<p>解决overfitting的思路：</p><ul>
<li><strong>the best solution</strong> is get more trainging data</li>
<li><strong>the simple way</strong> is to reduce the size of the model<ul>
<li>模型容量(<code>capacity</code>)足够大，就足够容易<em>记住</em>input和target的映射，没推理什么事了</li>
</ul>
</li>
<li>add constraints -&gt; weight <code>regularization</code></li>
<li>add dropout</li>
</ul>
<h2>Regularization</h2>
<p><strong>Occam’s razor</strong></p><blockquote>
<p>given <em>two explanations</em> for something, the explanation most likely to be correct is the <strong>simplest one</strong>—the one that makes <strong>fewer assumptions</strong>.</p></blockquote>
<p>即为传说中<em>如无必要，勿增实体</em>的<code>奥卡姆剃刀原理</code>，这是在艺术创作领域的翻译，我们这里还是直译的好，即能解释一件事的各种理解中，越简单的，假设条件越少的，往往是最正确的，引申到机器学习，就是如何定义一个<code>simple model</code></p><p>A simple model in this context is:</p><ul>
<li>a model where the distribution of parameter values has <code>less entropy</code></li>
<li>or a model with fewer parameters</li>
</ul>
<p>实操就是，就是迫使选择那些值比较小的weights，which makes the distribution of weight values more regular. This is called weight <code>regularization</code>。这个解释是我目前看到的最<code>regularization</code>这个名字最好的解释，“正则化”三个字都认识，根本没人知道这三个字是什么意思，翻译了跟没番一样，而使分布更“常规化，正规化”，好像更有解释性。</p><p>别的教材里还会告诉你这里是对大的权重的<strong>惩罚</strong>（设计损失函数加上自身权重后，权重越大，loss也就越大，这就是对大权重的惩罚）</p><ul>
<li>L1 regularization—The cost added is proportional to the absolute value of the weight coefficients (the L1 norm of the weights).</li>
<li>L2 regularization—The cost added is proportional to the square of the value of the weight coefficients (the L2 norm of the weights).</li>
</ul>
<p>L2 regularization is also called <code>weight decay</code>in the context of neural networks. Don’t let the different name confuse you: weight decay is mathematically <strong>the same as</strong> L2 regularization.</p><blockquote>
<p>只需要在训练时添加正则化</p></blockquote>
<h2>Dropout</h2>
<p>randomly dropping out (setting to zero) a number of output features of the layer during training.</p><p>dropout的作者Geoff Hinton解释dropout的灵感来源于银行办事出纳的不停更换和移动的防欺诈机制，可能认为一次欺诈的成功实施需要员工的配合，所以就尽量降低这种配合的可能性。于是他为了防止神经元也能聚在一起”密谋”，尝试随机去掉一些神经元。以及对输出添加噪声，让模型更难记住某些patten。</p><h2>The universal workflow of machine learning</h2>
<ol>
<li>Defining the problem and assembling a dataset<ul>
<li>What will your input data be?</li>
<li>What are you trying to predict?</li>
<li>What type of problem are you facing?</li>
<li>You hypothesize that your outputs can be predicted given your inputs.</li>
<li>You hypothesize that your available data is sufficiently informative to learn the relationship between inputs and outputs.</li>
<li>Just because you’ve assembled exam- ples of inputs X and targets Y doesn’t mean X contains enough information to predict Y.</li>
</ul>
</li>
<li>Choosing a measure of success<ul>
<li>accuracy? Precision and recall? Customer-retention rate?</li>
<li>balanced-classification problems,<ul>
<li>accuracy and area under the <code>receiver operating characteristic curve</code> (ROC AUC)</li>
</ul>
</li>
<li>class-imbalanced problems<ul>
<li>precision and recall.</li>
</ul>
</li>
<li>ranking problems or multilabel classification<ul>
<li>mean average precision</li>
</ul>
</li>
<li>...</li>
</ul>
</li>
<li>Deciding on an evaluation protocol<ul>
<li>Maintaining a hold-out validation set—The way to go when you have plenty of data</li>
<li>Doing <code>K-fold</code> cross-validation—The right choice when you have too few samples for hold-out validation to be reliable</li>
<li>Doing <code>iterated K-fold</code> validation—For performing highly accurate model evaluation when <em>little data</em> is available</li>
</ul>
</li>
<li>Preparing your data<ul>
<li>tensor化，向量化，归一化等</li>
<li>may do some feature engineering</li>
</ul>
</li>
<li>Developing a model that does better than a baseline<ul>
<li>baseline:<ul>
<li>基本上是用纯随机(比如手写数字识别，随机猜测为10%)，和纯相关性推理（比如用前几天的温度预测今天的温度，因为温度变化是连续的），不用任何机器学习做出baseline</li>
</ul>
</li>
<li>model:<ul>
<li>Last-layer activation<ul>
<li>sigmoid, relu系列， 等等</li>
</ul>
</li>
<li>Loss function<ul>
<li>直接的预测值真值的差，如MSE</li>
<li>度量代理，如crossentropy是ROC AUC的proxy metric</li>
</ul>
</li>
</ul>
</li>
<li>Optimization configuration<ul>
<li>What optimizer will you use? What will its learning rate be? In most cases, it’s safe to go with rmsprop and its default learning rate.</li>
</ul>
</li>
<li>Scaling up: developing a model that overfits<ul>
<li>通过增加layers, 增加capacity，增加training epoch来加速overfitting，从而再通过减模型和加约束等优化</li>
</ul>
</li>
<li>Regularizing your model and tuning your hyperparameters<ul>
<li>Add dropout.</li>
<li>Try different architectures: add or remove layers.</li>
<li>Add L1 and/or L2 regularization.</li>
<li>Try different hyperparameters (such as the number of units per layer or the learning rate of the optimizer) to find the optimal configuration.</li>
<li>Optionally, iterate on feature engineering: add new features, or remove features that don’t seem to be informative.</li>
</ul>
</li>
</ul>
</li>
</ol>
<table>
<thead>
<tr>
  <th>Problem type</th>
  <th>Last-layer activation</th>
  <th>Loss function</th>
</tr>
</thead>
<tbody>
<tr>
  <td>Binary classification</td>
  <td>sigmoid</td>
  <td>binary_crossentropy</td>
</tr>
<tr>
  <td>Multiclass, single-label classification</td>
  <td>softmax</td>
  <td>categorical_crossentropy</td>
</tr>
<tr>
  <td>Multiclass, multilabel classification</td>
  <td>sigmoid</td>
  <td>binary_crossentropy</td>
</tr>
<tr>
  <td>Regression to arbitrary values</td>
  <td>None</td>
  <td>mse</td>
</tr>
<tr>
  <td>Regression to values between 0 and 1</td>
  <td>sigmoi</td>
  <td>mse or binary_crossentropy</td>
</tr>
</tbody>
</table>
<h1>Deep learning for computer vision</h1>
<h2>Convolution Network</h2>
<p>The convolution operation extracts patches from its input feature map and applies the same transformation to all of these patches, producing an output feature map.</p><ul>
<li>convolution layers learn local patterns(局部特征)<ul>
<li>The patterns they learn are translation invariant.（局部特征可在图片别的地方重复）</li>
<li>有的教材里会说每个滑窗一个特征，然后引入<strong>参数共享</strong>才讲到一个特征其实可以用在所有滑窗</li>
</ul>
</li>
<li>They can learn spatial hierarchies of patterns(低级特征堆叠成高级特征)</li>
<li>depth axis no longer stand for specific colors as in RGB input; rather, they stand for filters(表示图片时，3个通道有原始含义，卷积开始后通道只表示filter了)</li>
<li><code>valid</code> and <code>same</code> convolution（加不加padding让filter在最后一个像素时也能计算）</li>
<li><code>stride</code>，滑窗步长</li>
<li><code>max-pooling</code> or <code>average-pooling</code><ul>
<li>usually 2x2 windows by stride 2 -&gt; 下采样(downsample)</li>
<li>更大的感受野</li>
<li>更小的输出</li>
<li>不是唯一的下采样方式（比如在卷积中使用stride也可以）</li>
<li>一般用max而不是average(寻找最强的表现)</li>
</ul>
</li>
<li>小数据集<ul>
<li>data augmenetation(旋转平衡缩放shear翻转等)<ul>
<li>不能产生当前数据集不存在的信息</li>
<li>所以仍需要dropout</li>
</ul>
</li>
<li>pretrained network(适用通用物体)<ul>
<li>feature extraction</li>
<li>fine-tuneing</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3>Using a pretrained convnet</h3>
<p>A pretrained network is a saved network that was previously trained <strong>on a large dataset</strong> typically on a large-scale image-classification task.</p><h3>Feature extraction</h3>
<p>Feature extraction consists of using the representations learned by a previous network to extract interesting features from new samples. These features are then run through a <em>new classifier</em>, which is trained from scratch.</p><ol>
<li>即只使用别的大型模型提取的representations（特征），来构建自己的分类器。</li>
<li>原本模型的分类器不但是为特定任务写的，而且基本上丧失了位置和空间信息，只保留了对该任务上的presence probability.</li>
<li>最初的层一般只能提取到线，边缘，颜色等低级特征，再往后会聚合出一些纹理，更高的层就可能会叠加出一些眼，耳等抽象的特征，所以你的识别对象与pretrained数据源差别很大的时候，就需要考虑把最尾巴的几层layer也舍弃掉。（e.g. VGG16最后一层提取了512个feature map）</li>
<li>两种用法：<ul>
<li>跑一次预训练模型你选中的部分，把参数存起来（$\leftarrow$错），把输出当作dataset作为自己构建的分类器的input。<ul>
<li>快，省资源，但是需要把数据集固定住，等于没法做data augmentation</li>
<li>跑预训练模型时不需要计算梯度(freeze)</li>
<li>其实应用预训练模型就等于别人的预处理数据集，而真实的模型只有一个小分类器</li>
</ul>
</li>
<li>合并到自定义的网络中当成普通网络训练<ul>
<li>慢，但是能做数据增广了</li>
<li>需手动设置来自预训练模型的梯度不需要计算梯度</li>
</ul>
</li>
</ul>
</li>
</ol>
<blockquote>
<p>注：这里为什么单独跑预训练模型不能数据增广呢？</p></blockquote>
<blockquote>
<p>教材用的是keras, 它处理数据的方式是做一个generaotr，只要你给定数据增广的规则（参数），哪怕只有一张图，它也是可以无穷无尽地给你生成下一张的。所以每一次训练都能有新的数据喂到网络里。这是出于内存考虑，不需要真的把数据全部加载到内存里。</p></blockquote>
<blockquote>
<p>而如果你是一个固定的数据集，比如几万条，那么你把所有的数据跑一遍把这个结果当成数据集（全放在内存里），那也不是不可以在这一步用数据增广。</p></blockquote>
<h3>Fine-tuning</h3>
<p>Fine-tuning consists of unfreezing a few of the top layers of a frozen model base used for feature extraction, and jointly training both the newly added part of the model (in this case, the fully connected classifier) and these top layers. This is called fine-tuning because it slightly adjusts the more abstract representations of the model being reused, in order to make them more relevant for the problem at hand.</p><p>前面的feature extraction方式，会把预训练的模型你选中的layers给freeze掉，即不计算梯度。这里之所以叫fine-tuning，意思就是会把最后几层(top-layers)给<code>unfreezing</code>掉，这样的好处是保留低级特征，重新训练高级特征，还保留了原来大型模型的结构，不需要自行构建。</p><figure class="vertical-figure" style="flex: 15.321375186846039" ><img width="410" height="1338" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/705011af7667b591af29afe03230ecc5.png" alt=""/></figure><blockquote>
<p>但是： it’s only possible to fine-tune the top layers of the convolutional base once the classifier on <code>top has already been trained</code>. 预训练模型没有frezze住的话loss将会很大，所以变成了先train一个大体差不多的classifier，再联合起来train一遍高级特征和classifier:</p></blockquote>
<ol>
<li>Add your custom network on top of an already-trained base network.</li>
<li>Freeze the base network.</li>
<li>Train the part you added. (第一次train)</li>
<li>Unfreeze some layers in the base network.</li>
<li>Jointly train both these layers and the part you added.（第二次train）</li>
</ol>
<p>但千万别把所有层都unfrezze来训练了</p><ol>
<li>低级特征都为边缘和颜色，无需重新训练</li>
<li>小数据量训练大型模型，model capacity相当大，非常容易过拟合</li>
</ol>
<h3>Visualizing what convents learn</h3>
<p>并不是所有的深度学习都是黑盒子，至少对图像的卷积网络不是 -&gt; <code>representations of visual concepts</code>, 下面介绍<strong>三种</strong>视觉化和可解释性的representations的方法。</p><h4>Visualizing intermediate activations</h4>
<p>就是把每个中间层(基本上是&quot;卷积+池化+激活“)可视化出来，This gives a view into how an input is <code>decomposed</code> into the different filters learned by the network.</p><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">models</span>
<span class="n">layer_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">layer</span><span class="o">.</span><span class="n">output</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[:</span><span class="mi">8</span><span class="p">]]</span> <span class="n">activation_model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">input</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">layer_outputs</span><span class="p">)</span>

<span class="n">activations</span> <span class="o">=</span> <span class="n">activation_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">first_layer_activation</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>

<span class="c1"># 注意使用的是matshow而不是show</span>
</pre></div>
<figure  style="flex: 101.9047619047619" ><img width="856" height="420" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/e5088f559521b7cc9c7b0cb129d275fe.png" alt=""/></figure><p>以上代码是利用了keras的Model特性，将所有layers的输出<strong>摊平</strong>（就是做了一个多头的模型），然后再顺便取了第4和第7个feature map画出来，可以看到，图一感兴趣的是<code>对角线</code>，图二提取的是<code>蓝色的亮点</code>。</p><p>结构化这些输出，可以确信初始layer确实提取的是简单特征，越往后越高级（抽象）。</p><p>A deep neural network effectively acts as an <code>information distillation</code>(信息蒸馏) pipeline, with raw data going in (in this case, RGB pictures) and being repeatedly transformed so that irrelevant information is filtered out (for example, the specific visual appearance of the image), and useful information is <code>magnified and refined</code> (for example, the class of the image).</p><blockquote>
<p>关键词：有用的信息被不断<strong>放大和强化</strong></p></blockquote>
<p>书里举了个有趣的例子，要你画一辆自行车。你画出来的并不是一辆充满细节的单车，而往往是你抽象出来的单车，你会用基本的线条勾勒出你对单车特征的理解，比如龙头，轮子等关键部件，以及相对位置。画家为什么能画得又真实又好看？那就是他们真的仔细观察了单车，他们绘画的时候用的并不是特征，而是一切细节，然而对于没有受过训练的普通人来说，往往只能用简单几笔勾勒出脑海中的单车的样子（其实并不是样子，而是特征的组合）</p><h4>Visualizing convnet filters</h4>
<p>通过强化filter对输出的反应并绘制出来，这是从数学方法上直接观察filter，看什么最能“刺激”一个filter，用”梯度上升“最能体现这种思路：</p><p>把output当成loss，用梯度上升（每次修改input_image）训练出来的output就是这个filter的极端情况，可以认为这个filter其实是在提取什么（responsive to）：</p><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.applications</span> <span class="kn">import</span> <span class="n">VGG16</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">backend</span> <span class="k">as</span> <span class="n">K</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">VGG16</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="s1">&#39;imagenet&#39;</span><span class="p">,</span> <span class="n">include_top</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">layer_name</span> <span class="o">=</span> <span class="s1">&#39;block3_conv1&#39;</span>
<span class="n">filter_index</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">layer_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span><span class="o">.</span><span class="n">output</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">layer_output</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">filter_index</span><span class="p">])</span>  <span class="c1"># output就是loss</span>

<span class="n">grads</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">input</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># 对input求微分</span>
<span class="n">grads</span> <span class="o">/=</span> <span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">grads</span><span class="p">)))</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span>

<span class="n">iterate</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">function</span><span class="p">([</span><span class="n">model</span><span class="o">.</span><span class="n">input</span><span class="p">],</span> <span class="p">[</span><span class="n">loss</span><span class="p">,</span> <span class="n">grads</span><span class="p">])</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="c1"># 理解静态图的用法</span>
<span class="n">loss_value</span><span class="p">,</span> <span class="n">grads_value</span> <span class="o">=</span> <span class="n">iterate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">))])</span>

<span class="n">input_img_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span> <span class="o">*</span> <span class="mi">20</span> <span class="o">+</span> <span class="mf">128.</span>
<span class="n">step</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">40</span><span class="p">):</span>
    <span class="n">loss_value</span><span class="p">,</span> <span class="n">grads_value</span> <span class="o">=</span> <span class="n">iterate</span><span class="p">([</span><span class="n">input_img_data</span><span class="p">])</span>
    <span class="n">input_img_data</span> <span class="o">+=</span> <span class="n">grads_value</span> <span class="o">*</span> <span class="n">step</span>  <span class="c1"># 梯度上升</span>
</pre></div>
<p>按上述代码的思路结构化输出并绘图：</p><figure class="vertical-figure" style="flex: 47.148288973384034" ><img width="1240" height="1315" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/d3df8099cd6c93450e84a4cd01b67d19.png" alt=""/></figure><p>从线条到纹理到物件（眼睛，毛皮，叶子）</p><blockquote>
<p>each layer in a convnet learns a collection of filters such that their inputs can be expressed as a <code>combination of the filters</code>.</p></blockquote>
<blockquote>
<p>This is similar to how the Fourier transform decomposes signals onto a bank of cosine functions.</p></blockquote>
<p>用傅里叶变换来类比卷积网络每一层就是把input表示成一系列特征的组合。</p><h4>Visualizing heatmaps of class activation</h4>
<p>which parts of a given image led a convnet to its final classification decision. 即图像有哪一部分对最终的决策起了作用。</p><ul>
<li><code>class activation map</code> (CAM) visualization,</li>
<li><code>Grad-CAM</code>: Visual Explanations from Deep Networks via Gradient-based Localization.”</li>
</ul>
<blockquote>
<p>you’re weighting a spatial map of “how intensely the input image activates different channels” by “how important each channel is with regard to the class,” resulting in a spatial map of “how intensely the input image activates the class.</p></blockquote>
<p>解读上面这句话：</p><p>不同channels（特征）对图像的激活的强度<br />
+<br />
每个特征对(鉴定为）该类别的重要程度<br />
=<br />
该“类别”对图像的激活的强度</p><p>一张两只亚洲象的例图，使用VGG16来做分类，得到92.5%的置信度的亚洲象的判断，为了visualize哪个部分才是“最像亚洲象”的，使用<code>Grad-CAM</code>处理：</p><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.applications.vgg16</span> <span class="kn">import</span> <span class="n">VGG16</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">VGG16</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="s1">&#39;imagenet&#39;</span><span class="p">)</span>
<span class="n">african_e66lephant_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">output</span><span class="p">[:,</span> <span class="mi">386</span><span class="p">]</span>  <span class="c1"># 亚洲象在IMGNET的类别是386</span>
<span class="n">last_conv_layer</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="s1">&#39;block5_conv3&#39;</span><span class="p">)</span> <span class="c1"># top conv layer</span>

<span class="n">grads</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">african_elephant_output</span><span class="p">,</span> <span class="n">last_conv_layer</span><span class="o">.</span><span class="n">output</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> 
<span class="n">pooled_grads</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">iterate</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">function</span><span class="p">([</span><span class="n">model</span><span class="o">.</span><span class="n">input</span><span class="p">],</span>
                     <span class="p">[</span><span class="n">pooled_grads</span><span class="p">,</span> <span class="n">last_conv_layer</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
<span class="n">pooled_grads_value</span><span class="p">,</span> <span class="n">conv_layer_output_value</span> <span class="o">=</span> <span class="n">iterate</span><span class="p">([</span><span class="n">x</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">512</span><span class="p">):</span>
    <span class="n">conv_layer_output_value</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*=</span> <span class="n">pooled_grads_value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="n">heatmap</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">conv_layer_output_value</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
<figure  style="flex: 77.77777777777777" ><img width="1036" height="666" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/9020d228898abcf96e2855b7e028374b.png" alt=""/></figure><p>叠加到原图上去（用cv2融合两张图片，即相同维度的数组以不同权重逐像素相加）：</p><figure  style="flex: 75.97402597402598" ><img width="702" height="462" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/9261ae2a67e61c26e078db210f218d11.png" alt=""/></figure><h1>Deep learning for text and sequences</h1>
<p>空间上的序列，时间上的序列组成的数据，比如文本，视频，天气数据等，一般用<code>recurrent neural network</code>(RNN)和<code>1D convnets</code></p><blockquote>
<p>其实很多名词，包括convnets，我并没有在别的地方看到过，好像就是作者自己发明的，但这些不重要，知道它描述的是什么就可以了，不一定要公认术语。</p></blockquote>
<p>通用场景：</p><ul>
<li>[分类: 文本分类] Document classification and timeseries classification, such as identifying the topic of an article or the author of a book</li>
<li>[分类: 文本比较] Timeseries comparisons, such as estimating how closely related two documents or two stock tickers are</li>
<li>[分类: 生成] Sequence-to-sequence learning, such as decoding an English sentence into French</li>
<li>[分类: 情感分析]Sentiment analysis, such as classifying the sentiment of tweets or movie reviews as positive or negative</li>
<li>[回归: 预测]Timeseries forecasting, such as predicting the future weather at a certain location, given recent weather data</li>
</ul>
<p>我画蛇添足地加了是分类问题还是回归问题.</p><blockquote>
<p>none of these deeplearning models truly understand text in a human sense</p></blockquote>
<p>Deep learning for natural-language processing is <code>pattern recognition</code> applied to words, sentences, and paragraphs, in much <strong>the same</strong> way that computer vision is pattern recognition applied to pixels.</p><h2>tokenizer</h2>
<p>图像用像素上的颜色来数字化，那文字也把什么数字化呢？</p><ul>
<li>拆分为词，把每个词转化成向量</li>
<li>拆分为字（或字符），把每个字符转化为向量</li>
<li>把字（词）与前n个字（词）组合成单元，转化为向量，（类似滑窗），N-Grams</li>
</ul>
<p>all of above are <code>tokens</code>, and breaking text into such tokens is called <code>tokenization</code>. These vectors, packed into sequence tensors, are fed into deep neural networks.</p><p><code>N-grams</code>这种生成的token是无序的，就像一个袋子装了一堆词：<code>bag-of-words</code>: a set of tokens rather than a list of sequence.</p><p>所以句子结构信息丢失了，更适合用于浅层网络。作为一种rigid, brittle（僵硬的，脆弱的）特征工程方式，深度学习采用多层网络来提取特征。</p><h2>vectorizer</h2>
<p>token -&gt; vector:</p><ul>
<li>one-hot encoding</li>
<li>token/word embedding (word2vec)</li>
</ul>
<h3>one-hot</h3>
<ol>
<li>以token总数量（一般就是字典容量）为维度</li>
<li>一般无序，所以生成的时候只需要按出现顺序编索引就好了</li>
<li>有时候也往往伴随丢弃不常用词，以减小维度</li>
<li>也可以在字符维度编码（维度更低）</li>
<li>一个小技巧，如果索引数字过大，可以把单词hash到固定维度(未跟进)</li>
</ol>
<p>特点/问题：</p><ul>
<li>sparse</li>
<li>high-dimensional, 比如几千几万</li>
<li>no spatial relationship</li>
<li>hardcoded</li>
</ul>
<h3>word embeddings</h3>
<ul>
<li>Dense</li>
<li>Lower-dimensional，比如128，256...</li>
<li>Spatial relationships (语义接近的向量空间上也接近)</li>
<li>Learned from data</li>
</ul>
<p>to obtain word embeddings:</p><ol>
<li>当成训练参数之一(以Embedding层的身份)，跟着训练任务一起训练</li>
<li>pretrained word embeddings<ul>
<li>Word2Vec(2013, google)<ul>
<li>CBOW</li>
<li>Skip-Gram</li>
</ul>
</li>
<li>GloVe(2014, Stanford))</li>
<li>前提是语言环境差不多，不同学科/专业/行业里的词的关系是完全不同的<ul>
<li>GloVe从wikipedia和很多通用语料库里训练，可以尝试在许多非专业场景里使用。</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>keras加载训练词向量的方式：</p><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_weights</span><span class="p">([</span><span class="n">embedding_matrix</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
<p>pytorch：</p><div class="highlight"><pre><span></span><span class="c1"># TEXT, LABEL为torchtext的Field对象</span>
<span class="kn">from</span> <span class="nn">torchtext.vocab</span> <span class="kn">import</span> <span class="n">Vectors</span>
<span class="n">vectors</span><span class="o">=</span><span class="n">Vectors</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;./sgns.sogou.word&#39;</span><span class="p">)</span> <span class="c1">#使用预训练的词向量，维度为300Dimension</span>
<span class="n">TEXT</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">vectors</span><span class="o">=</span><span class="n">vectors</span><span class="p">)</span> <span class="c1">#构建词典</span>
<span class="n">LABEL</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>

<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">TEXT</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>
<span class="n">vocab_vectors</span> <span class="o">=</span> <span class="n">TEXT</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">vectors</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="c1">#准备好预训练词向量</span>

<span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">num_embeddings</span><span class="o">=</span><span class="n">vocab_size</span><span class="err">，</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_size</span><span class="p">)</span>

<span class="c1"># 上面是为了回顾，真正用来做对比的是下面这两句</span>
<span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">vocab_vectors</span><span class="p">))</span>
<span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
<blockquote>
<p>预训练词向量也可以继续训练，以得到task-specific embedding</p></blockquote>
<h2>Recurrent neural networks(RNN)</h2>
<p>sequence, time series类的数据，天然会受到前后数据的影响，RNN通过将当前token计算的时候引入上一个token的计算结果（反向的话就能获得下一个token的结果）以获取上下文的信息。</p><p>前面碰到的网络，数据消费完就往前走（按我这种说法，后面还有很多“等着二次消费的”模块，比如inception, resdual等等），叫做<code>feedforward network</code>。显然，RNN中，一个token产生输出后并不是直接丢给下一层，而是还复制了一份丢给了同层的下一个token. 这样，当前token的<code>output</code>成了下一个token的<code>state</code>。</p><ul>
<li>因为一个output其实含有“前面“所有的信息，一般只需要最后一个output</li>
<li>如果是堆叠多层网络，则需要返回<strong>所有</strong>output</li>
</ul>
<p>序列过长梯度就消失了，所谓的<strong>遗忘</strong> （推导见另一篇笔记，）  -&gt; <code>LSTM</code>, <code>GRU</code></p><h3>Long Short-Term Memory(LSTM)</h3>
<ol>
<li>想象有一根传送带穿过sequence</li>
<li>同一组input和state会进行三次相同的线性变换，有没有联想到<code>transformer</code>用同一个输出去生成<code>q, k, v</code>？</li>
</ol>
<div class="highlight"><pre><span></span><span class="n">output_t</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">dot</span><span class="p">(</span><span class="n">state_t</span><span class="p">,</span> <span class="n">Uo</span><span class="p">)</span> <span class="o">+</span> <span class="n">dot</span><span class="p">(</span><span class="n">input_t</span><span class="p">,</span> <span class="n">Wo</span><span class="p">)</span> <span class="o">+</span> <span class="n">dot</span><span class="p">(</span><span class="n">C_t</span><span class="p">,</span> <span class="n">Vo</span><span class="p">)</span> <span class="o">+</span> <span class="n">bo</span><span class="p">)</span>
<span class="n">i_t</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">dot</span><span class="p">(</span><span class="n">state_t</span><span class="p">,</span> <span class="n">Ui</span><span class="p">)</span> <span class="o">+</span> <span class="n">dot</span><span class="p">(</span><span class="n">input_t</span><span class="p">,</span> <span class="n">Wi</span><span class="p">)</span> <span class="o">+</span> <span class="n">bi</span><span class="p">)</span> 
<span class="n">f_t</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">dot</span><span class="p">(</span><span class="n">state_t</span><span class="p">,</span> <span class="n">Uf</span><span class="p">)</span> <span class="o">+</span> <span class="n">dot</span><span class="p">(</span><span class="n">input_t</span><span class="p">,</span> <span class="n">Wf</span><span class="p">)</span> <span class="o">+</span> <span class="n">bf</span><span class="p">)</span> 
<span class="n">k_t</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">dot</span><span class="p">(</span><span class="n">state_t</span><span class="p">,</span> <span class="n">Uk</span><span class="p">)</span> <span class="o">+</span> <span class="n">dot</span><span class="p">(</span><span class="n">input_t</span><span class="p">,</span> <span class="n">Wk</span><span class="p">)</span> <span class="o">+</span> <span class="n">bk</span><span class="p">)</span>

<span class="n">c_t</span><span class="o">+</span><span class="mi">1</span> <span class="o">=</span> <span class="n">i_t</span> <span class="o">*</span> <span class="n">k_t</span> <span class="o">+</span> <span class="n">c_t</span> <span class="o">*</span> <span class="n">f_t</span>  <span class="c1"># 仍然有q，k，v的意思（i,k互乘，加上f， 生成新c）</span>
</pre></div>
<blockquote>
<p>不要去考虑哪个是<strong>遗忘门</strong>，<strong>记忆门</strong>，还是<strong>输出门</strong>，最终是由weights决定的，而不是设计。</p></blockquote>
<p>Just keep in mind what the LSTM cell is meant to do:</p><blockquote>
<p>allow past information to be <code>reinjected</code> at a later time, thus fighting the vanishing-gradient problem.</p></blockquote>
<p>关键词：reinject</p><h3>dropout</h3>
<p>不管是keras还是pytorch，都帮你隐藏了dropout的坑。 你能看到应用这些框架的时候，是需要你把dropout传进去的，而不是手动接一个dropoutlayer，原因是需要在序列每一个节点上应用同样的dropout mask才能起作用，不然就会起到反作用。</p><p>keras封装得要复杂一点：</p><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span>
                    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
                    <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
                    <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">float_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])))</span>
</pre></div>
<h3>stacking recurrent layers</h3>
<p>前面说过，设计好的模型的一个判断依据是至少让模型能跑到overfitting。如果到了overfitting，表现还不是很好，那么可以考虑增加模型容量（叠更多层，以及拓宽layer的输出维度）</p><p>堆叠多层就需要用到每个节点上的输出，而不只关心最后一个输出了。</p><h3>Bidriectional</h3>
<p>keras奇葩的bidirectional语法：</p><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Bidirectional</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">32</span><span class="p">)))</span>
</pre></div>
<p>其实这是设计模式在类的封装上的典型应用，善用继承和多态，无侵入地扩展类的方法和属性，而不是不断魔改原代码，加参数，改API。但在脚本语言风格里的环境里，这么玩就有点格格不入了。</p><h2>Sequence processing with convnets</h2>
<ol>
<li>卷积用到序列上去也是可以的</li>
<li>一个向量只表示一个token，如果把token的向量打断就违背了token是最小单元的初衷，所以序列上的卷积，不可能像图片上两个方向去滑窗了。(<code>Conv1D</code>的由来)</li>
<li>一个卷积核等于提取了n个关联的上下文（有点类似<code>n-grams</code>），堆叠得够深感受野更大，可能得到更大的上下文。</li>
<li>但仍然理解为filter在全句里提取局部特征</li>
</ol>
<p>归桕结底，图片的最小单元是一个像素（一个数字），而序列（我们这里说文本）的最小单元是token，而token又被我们定义为vector（一组数字）了，那么卷积核就限制在至少要达到最小单元(vector)的维度了。</p><h3>Combining CNNs and RNNs to process long sequences</h3>
<p>卷积能通过加深网络获取更大的感受野，但仍然是“位置无关”的，因为每个filter本就是在整个序列里搜索相同的特征。</p><p>但是它确实提取出了特征，是否可把位置关系等上下文的作业交给下游任务RNN做呢？</p><figure  style="flex: 50.750750750750754" ><img width="676" height="666" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/bc69a05bd9def95c42c6ce450a5cf164.png" alt=""/></figure><p>不但实现，而且堆叠两种网络，还可以把数据集做得更大（CNN是矩阵运算，还能用GPU加速）。</p><h1>Advanced deep-learning best practices</h1>
<p>这一章是介绍了更多的网络（从keras的封装特性出发）结构和模块，以及batch normalization, model ensembling等知识。</p><h2>beyond Sequential model</h2>
<p>前面介绍的都是Sequential模型，就是一个接一个地layer前后堆叠，现实中有很多场景并不是一进一出的：</p><ol>
<li>multi-input model</li>
</ol>
<p>假设为二手衣物估价：</p><ul>
<li>格式化的元数据（品牌，性别，年龄，款式）: one-hot, dense</li>
<li>商品的文字描述：RNN or 1D convnet</li>
<li>图片展示：2D convnet</li>
<li>每个input用适合自己的网络做输出，然后合并起来作为一个input，回归一个价格</li>
</ul>
<ol start="2">
<li>multi-output model (multi-head)</li>
</ol>
<p>一般的检测器通常就是多头模型，因为既要回归对象类别，还要回归出对象的位置</p><ol start="3">
<li>graph-like model</li>
</ol>
<p>这个名字很好地形容了做深度学习时看别人的网络是什么样的方式：看图。现代的SOTA的网络往往既深且复杂，而网络结构画出来也不再是一条线或几个简单分支，这本书干脆把它们叫图形网络：<code>Inception</code>, <code>Residual</code></p><p>为了能架构这些复杂的网络，keras介绍了新的语法，先看看怎么重写<code>Sequential</code>:</p><div class="highlight"><pre><span></span><span class="n">seq_model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">seq_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,)))</span>
<span class="n">seq_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">seq_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>

<span class="c1"># 重写</span>
<span class="n">input_tensor</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">input_tensor</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">output_tensor</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">output_tensor</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">summayr</span><span class="p">()</span>
</pre></div>
<p>我们自己实现过静态图，最终去执行的时候能从尾追溯到头，并从头来开始计算，这里也是一样的：</p><ol>
<li>input, output是Tensor类，所以有完整的层次信息</li>
<li>output往上追溯，最终溯到缺少一个input</li>
<li>这个input恰好也是Model的构造函数之一，闭环了。</li>
</ol>
<p>书里说的更简单，output是input不断transforming的结果。如果传一个没有这个关系的input进去，就会报错。</p><p><strong>demo</strong></p><p>用一个QA的例子来演示多输入（一个问句，一段资料），输出为答案在资料时的索引（简化为单个词，所以只有一个输出）</p><div class="highlight"><pre><span></span><span class="n">text_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;int32&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;text&#39;</span><span class="p">)</span>
<span class="n">embedded_text</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
    <span class="mi">64</span><span class="p">,</span> <span class="n">text_vocabulary_size</span><span class="p">)(</span><span class="n">text_input</span><span class="p">)</span>
<span class="n">encoded_text</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">32</span><span class="p">)(</span><span class="n">embedded_text</span><span class="p">)</span>  <span class="c1"># lstm 处理资讯</span>
<span class="n">question_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;int32&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;question&#39;</span><span class="p">)</span>


<span class="n">embedded_question</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
    <span class="mi">32</span><span class="p">,</span> <span class="n">question_vocabulary_size</span><span class="p">)(</span><span class="n">question_input</span><span class="p">)</span>
<span class="n">encoded_question</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">16</span><span class="p">)(</span><span class="n">embedded_question</span><span class="p">)</span> <span class="c1"># lstm 处理问句</span>

<span class="n">concatenated</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">encoded_text</span><span class="p">,</span> <span class="n">encoded_question</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 竖向拼接（即不增加内容只增加数量）</span>
<span class="n">answer</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">answer_vocabulary_size</span><span class="p">,</span>
                      <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)(</span><span class="n">concatenated</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">([</span><span class="n">text_input</span><span class="p">,</span> <span class="n">question_input</span><span class="p">],</span> <span class="n">answer</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;rmsprop&#39;</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;acc&#39;</span><span class="p">])</span>
</pre></div>
<p>这里是把答案直接给回归出来了(one-hot)，如果是给出答案的首尾位置，那肯定只能用索引了。</p><p><strong>demo</strong></p><p>多头输出的：</p><div class="highlight"><pre><span></span><span class="c1"># 线性回归</span>
<span class="n">age_prediction</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;age&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="c1"># 逻辑回归</span>
<span class="n">income_prediction</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">num_income_groups</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;income&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="c1"># 二元逻辑回归</span>
<span class="n">gender_prediction</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;gender&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">posts_input</span><span class="p">,</span>
              <span class="p">[</span><span class="n">age_prediction</span><span class="p">,</span> <span class="n">income_prediction</span><span class="p">,</span> <span class="n">gender_prediction</span><span class="p">])</span>
</pre></div>
<p>梯度回归要求loss是一个标量，keras提供了方法将三个loss加起来，同时为了量纲统一，还给了权重参数：</p><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;rmsprop&#39;</span><span class="p">,</span>
<span class="n">loss</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span> <span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span> <span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">],</span> <span class="n">loss_weights</span><span class="o">=</span><span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">])</span>
</pre></div>
<h2>Directed acyclic graphs of layers</h2>
<p>有向无环图。可以理解为最终不会回到出发点。</p><p>现在会介绍的是几个<code>Modules</code>，意思是可以把它当成一个layer，来构造你的网络/模型。</p><h3>Inception Modules</h3>
<ul>
<li>inspired by <code>network-in-network</code></li>
<li>对同一个输入做不同（层数/深度）的卷积（保证最终相同的下采样维度），最后合并为一个输出</li>
<li>因为卷积的深度不尽相同，学到的空间特征也有粗有细</li>
</ul>
<h3>Residual Connections</h3>
<ul>
<li>有些地方叫shortcut</li>
<li>用的是相加，不是concatenate, 如果形状变了，对earlier activation做linear transformation</li>
<li>解决<code>vanishing gradients</code> and <code>representational bottlenecks</code></li>
<li>adding residual connections to any model that has more than 10 layers is likely to be beneficial.</li>
</ul>
<p><strong>representational bottlenecks</strong></p><p>序列模型时，每一层的表示都来自于前一层，如果前一层很小，比如维度过低，那么携带的信息量也被压缩得很有限了，整个模型都会被这个“瓶颈”限制。比如音频信号处理，降维就是降频，比如到0-15kHz，但是下游任务也没法recover dropped frequencies了。所有的损失都是永久的。</p><p>Residual connections, by <code>reinjecting</code> earlier information downstream, partially solve this issue for deep-learning models.（又一次强调<code>reinject</code>）</p><h3>Lyaer weight sharihng</h3>
<p>在网络的不同位置用同一个layer，并且参数也相同。等于共享了相同的知识，相同的表示，以及是同时(simultaneously)训练的。</p><p>一个语义相似度的例子，输入是A和B还是B和A，是一样的（即可以互换）。架构网络的时候，用LSTM来处理句子，需要做两个LSTM吗？当然可以，但是也可以只做一个LSTM，分别喂入两个句子，合并两个输出来做分类。就是考虑到这种互换性，既然能互换，也就是这个layer也能应用另一个句子，因此就不必要再新建一个LSTM.</p><h3>Models as layers</h3>
<p>讲了两点：</p><ol>
<li>model也可以当layer使用</li>
<li>多处使用同一个model也是共享参数，如上一节。</li>
</ol>
<p>举了个双摄像头用以感知深度的例子，每个摄像头都用一个Xception网络提取特征，但是可以共用这个网络，因为拍的是同样的内容，只需要处理两个摄像头拍到的内容的差别就能学习到深度信息。因为希望是用同样的特征提取机制的。</p><p>都是蜻蜓点水。</p><h2>More Advanced</h2>
<h3>Batch Normalization</h3>
<ol>
<li>第一句话就是说为了让样本数据看起来<strong>更相似</strong>，说明这是初衷。</li>
<li>然后是能更好地泛化到未知数据（同样也是因为bn后就<strong>更相似</strong>了）</li>
<li>深度网络中每一层之后也需要做<ul>
<li>还有一个书里没讲到的原因，就是把值移到激活函数的梯度大的区域（比如0附近），否则过大过小的值在激活函数的曲线里都是几乎没有梯度的位置</li>
</ul>
</li>
<li>内部用的指数移动平均(<code>exponential moving average</code>)</li>
<li>一些层数非常深的网络必须用BN，像resnet 50, 101, 152, inception v3, xception等</li>
</ol>
<h3>Depthwise Separable Convolution</h3>
<p>之前的卷积，不管有多少个layer，都是放到矩阵里一次计算的，DSC把每一个layer拆开，单独做卷积（不共享参数），因为没有一个巨大的矩阵，变成了几个小矩阵乘法，参数量也大大变少了。</p><ol>
<li>对于小样本很有效</li>
<li>对于大规模数据集，它可以成为里面的固定结构的模块（它也是Xception的基础架构之一）</li>
</ol>
<blockquote>
<p>In the future, it’s likely that depthwise separable convolutions will <code>completely replace regular convolutions</code>, whether for 1D, 2D, or 3D applications, due to their higher representational efficiency.</p></blockquote>
<p>?!!</p><h3>Model ensembling</h3>
<ol>
<li>Ensembling consists of <strong>pooling together</strong> the predictions of a set of different models, to produce better predictions.</li>
<li>期望每一个<code>good model</code>拥有<code>part of the truth</code>(部分的真相)。盲人摸象的例子，没有哪个盲人拥有直接感知一头象的能力，机器学习可能就是这样一个盲人。</li>
<li>The key to making ensembling work is the <code>diversity</code> of the set of classifiers -&gt; 关键是要“多样性”。 <code>Diversity</code> is what makes ensembling work.</li>
<li>千万<strong>不要</strong>去ensembling同样的网络仅仅改变初始化而去train多次的结果。</li>
<li>比较好的实践有ensemble <code>tree-based</code> models(random forests, gradient-boosted trees) 和深度神经网络</li>
<li>以及<code>wide and deep</code> category of models, blending deep learning with shallow learning.</li>
</ol>
<p>同样是蜻蜓点水。</p><h1>Generative deep learning</h1>
<p>Our perceptual modalities, our language, and our artwork all have <code>statistical structure</code>. Learning this structure is what deep-learning algorithms excel at.</p><p>Machine-learning models can learn the <code>statistical latent space</code> of images, music, and stories, and they can then<code>sample from this space</code>, <strong>creating new artworks</strong> with characteristics similar to those the model has seen in its training data.</p><h2>Text generation with LSTM</h2>
<h3>Language model</h3>
<p>很多地方都在按自己的理解定义<code>language model</code>，这本书定义很明确，能为根据前文预测下一个或多个token建立概率模型的网络。</p><blockquote>
<p>any network that can model the probability of the next token given the previous ones is called a language model.</p></blockquote>
<ol>
<li>所以首先，它是一个network</li>
<li>它做的事是model一个probability</li>
<li>内容是the next token</li>
<li>条件是previous tokens</li>
</ol>
<p>一旦你有了这样一个language model，你就能<code>sample from it</code>，这就是前面笔记里的sample from lantent space, 然后generate了。</p><h3>greedy sampling and stochastic sampling</h3>
<p>如果根据概率模型每次都选“最可能”的输出，在连贯性上被证明是不好的，而且也丧失了创造性，所以还是给了一定的随机性能选到“不那么可能”的输出。</p><p>因为人类思维本身也是<code>跳跃</code>的。</p><p>考虑两个输出下一个token时的极端情况：</p><!-- --> | <!-- --> | <!-- --> | <!-- -->
<p>------- | ------- | ------- | -------
纯随机，所有可选词的概率是均等的 | 毫无意义 | <code>max entropy</code> | 创造性高
greedy sampling | 毫无生趣 | <code>minimum entropy</code> | 可预测性高</p><p>实现方式：<code>softmax temperature</code></p><p>除一个<code>温度</code>，如果温度大于1，那么温度越大，被除数缩幅度就越大（这样温差就越小，分布会更平均）-&gt; 偏向了纯随机的概率结构（均等）</p><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="k">def</span> <span class="nf">reweight_distribution</span><span class="p">(</span><span class="n">original_distribution</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="n">distribution</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">original_distribution</span><span class="p">)</span> <span class="o">/</span> <span class="n">temperature</span>
    <span class="n">distribution</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">distribution</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">distribution</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">distribution</span><span class="p">)</span>
</pre></div>
<p>写成公式
$
\frac{e^{\frac{log(d)}{T}}}{\sum e^{\frac{log(d)}{T}}}
$
这是对温度和sigmoid做了融合：</p><ol>
<li>一个是对目标分布取自然对数后除温度再当成e的指数给幂回去（如果不除温度，那就是先log再e，等于是原数）</li>
<li>标准的sigmoid方程</li>
</ol>
<blockquote>
<p>这里回顾一个概念：Sampling from a space</p></blockquote>
<p>书里大量用了这个概念，结合代码，其实就是一个predict函数，也就是说，一般人理解的“<code>预测，推理</code>”，是从业务逻辑方面来理解，作者更愿意从统计学和线性代数角度来理解。</p><p>两种训练方法：</p><ol>
<li>每次用N个字，来预测第N+1个字，即output只有1个(voc_size, 1)，训练的是language model</li>
<li>每次用N个字(a, b), 来预测(a+1, b+1)， output有N个(voc_size, N)，训练的是特定的任务，比如写诗，作音乐</li>
</ol>
<p>过程：</p><ol>
<li>准备数据，X为一组句子，Y为每一个句子对应的下一个字（全部向量化）</li>
<li>搭建一个LSTM + Dense 的网络，输出根据具体情况要么为1，要么为N</li>
<li>每一个epoch里均进行预测（如果不是为了看过程，有必要吗？我们要最后一轮的预测不就行了？）<ul>
<li>进行一次fit(就是train)，得到优化后的参数</li>
<li>随机取一段文本，用作种子（用来生成第一个字）</li>
<li>计算生成多少个字，就开始for循环<ul>
<li>向量化当前的种子（会越来越长）</li>
<li>predict，得到每个字的概率</li>
<li>softmax temperature，平滑概率，取出next_token</li>
<li>next_token转回文本，附加到seed后面</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3>DeepDream</h3>
<p>看了一遍，不感兴趣。核心思路跟视觉化filter的思路是一样的：<code>gradient ascent</code></p><ol>
<li>从对每个layer里的单个filter做梯度上升变成了对整个layer做梯度上升</li>
<li>不再从随机噪声开始，而是从一张真实图片开始，实现这些layer里对图片影响最大的patterns的distorting</li>
</ol>
<h3>Neural style transfer</h3>
<p>Neural style transfer consists of applying the <code>style</code> of a reference image to a target image while conserving the <code>content</code> of the target image.</p><ul>
<li>两个对象：<code>reference</code>, <code>target</code> image</li>
<li>两个概念：<code>style</code>和<code>content</code></li>
</ul>
<p>对<code>B</code>的content应用<code>A</code>的style，我们可以理解为“笔刷”，或者用前些年的流行应用来解释：把一副画水彩化，或油画化。</p><p>把style分解为不同spatial scales上的：纹理，颜色，和visual pattern</p><p>想用深度学习来尝试解决这个问题，首先至少得定义损失函数是什么样的。</p><p>If we were able to mathematically define <code>content</code> and <code>style</code>, then an appropriate loss function to minimize would be the following:</p><div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">distance</span><span class="p">(</span><span class="n">style</span><span class="p">(</span><span class="n">reference_image</span><span class="p">)</span> <span class="o">-</span> <span class="n">style</span><span class="p">(</span><span class="n">generated_image</span><span class="p">))</span> <span class="o">+</span>
        <span class="n">distance</span><span class="p">(</span><span class="n">content</span><span class="p">(</span><span class="n">original_image</span><span class="p">)</span> <span class="o">-</span> <span class="n">content</span><span class="p">(</span><span class="n">generated_image</span><span class="p">))</span>
</pre></div>
<p>即对新图而言，<code>纹理要无限靠近A，内容要无限靠近B</code>。</p><ul>
<li>the content loss<ul>
<li>图像内容属于高级抽象，因此只需要top layers参与就行了，实际应用中只取了最顶层</li>
</ul>
</li>
<li>the style loss<ul>
<li>应用<code>Gram matrix</code><ul>
<li>the inner product of the feature maps of a given layer</li>
<li>correlations between the layer's feature</li>
<li>需要生成图和参考图的每一个对应的layer拥有相同的纹理(same <code>textures</code> at different <code>spatial scales</code>)，因此需要所有的layer参与</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>从这里应该也能判断出要搭建网络的话，input至少由三部分（三张图片）构成了。</p><p><strong>demo</strong></p><ul>
<li>input为参考图，目标图，和生成图（占位），concatenate成一个tensor</li>
<li>用VGG19来做特征提取</li>
<li>计算loss<ol>
<li>用生成图和<code>目标图</code>的<code>top_layer</code>以L2 norm距离做loss</li>
<li>用生成图和<code>参考图</code>的<code>every</code> layer以L2 Norm做loss并累加</li>
<li>对生成图偏移1像素做regularization loss（具体看书）</li>
<li>上述三组loss累加，为一轮的loss</li>
</ol>
</li>
<li>用loss计算对input(即三联图)的梯度</li>
</ul>
<h2>Generating images</h2>
<blockquote>
<p>Sampling from a latent space of images to create entirely new images</p></blockquote>
<p>熟悉的句式又来了。</p><p>核心思想：</p><ol>
<li>low-dimensional <code>latent space</code> of representations<ul>
<li>一般是个vector space</li>
<li>any point can be mapped to a realistic-looking image</li>
</ul>
</li>
<li>the module capable of <code>realizing this mapping</code>, can take point as input, then output an image, this called:<ul>
<li>generator -&gt; GAN</li>
<li>decoder -&gt; VAE</li>
</ul>
</li>
</ol>
<p>VAE v.s. GAN</p><ul>
<li>VAEs are great for learning latent spaces that are <code>well structured</code></li>
<li>GANs generate images that can potentially be <code>highly realistic</code>, but the latent space they come from may not have as much structure and continuity.</li>
</ul>
<h3>VAE（variational autoencoders）</h3>
<p>given a <code>latent space</code> of representations, or an embedding space, <code>certain directions</code> in the space <strong>may</strong> encode interesting axes of variation in the original data. -&gt; inspired by <code>concept space</code></p><p>比如包含人脸的数据集的latent space里，是否会存在<code>smile vectors</code>，定位这样的vector，就可以修改图片，让它projecting到这个latent space里去。</p><p><strong>Variational autoencoders</strong></p><p>Variational autoencoders are a kind of <em>generative model</em> that’s especially appropriate for the task of <strong>image editing</strong> via concept vectors.</p><p>They’re a modern take on <code>autoencoders</code> (a type of network that aims to <code>encode</code>an input to a <code>low-dimensional</code> latent space and then decode it back) that mixes ideas from deep learning with <strong>Bayesian inference</strong>.</p><ul>
<li>VAE把图片视作隐藏空间的参数进行统计过程的结果。</li>
<li>参数就是表示一种正态分布的mean和variance（实际取的log_variance)</li>
<li>用这个分布可以进行采样(sample)</li>
<li>映射回original image</li>
</ul>
<ol>
<li>An encoder module turns the input samples <em>input_img</em> into two parameters in a latent space of representations, <code>z_mean</code> and <code>z_log_variance</code>.</li>
<li>You randomly sample a point z from the latent normal distribution that’s assumed to generate the input image, via $z = z_mean + e^{z_log_variance} \times \epsilon$, where $\epsilon$ is a random tensor of small values.</li>
<li>A decoder module maps <em>this point</em> in the latent space back to the original input image.</li>
</ol>
<blockquote>
<p>Because epsilon is random, the process ensures that every point that’s <strong>close to the latent location</strong> where you encoded input_img (z-mean) can be decoded to something <strong>similar</strong> to input_img, thus forcing the latent space to be continuously meaningful.</p></blockquote>
<ol>
<li>所以VAE生成的图片是可解释的，比如在latent space中距离相近的两点，decode出来的图片相似度也就很高。</li>
<li>多用于编辑图片，并且能生成动画过程（因为是连续的）</li>
</ol>
<p>伪代码(不算，可以说是骨干代码）：</p><div class="highlight"><pre><span></span><span class="n">z_mean</span><span class="p">,</span> <span class="n">z_log_variance</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">input_img</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">z_mean</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="n">z_log_variance</span><span class="p">)</span> <span class="o">*</span> <span class="n">epsilon</span>  <span class="c1"># sampling</span>
<span class="n">reconstructed_img</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">input_img</span><span class="p">,</span> <span class="n">reconstructed_img</span><span class="p">)</span>
</pre></div>
<p>VAE encoder network</p><div class="highlight"><pre><span></span><span class="n">img_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">latent_dim</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">input_img</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">shape_before_flattening</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">int_shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">z_mean</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">z_log_var</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
<ol>
<li>可见是一个标准的multi-head的网络</li>
<li>可见所谓的latent space，其实就是transforming后的结果</li>
<li>encode的目的是回归出两个参数（本例是两个2维参数）</li>
<li>两个参数一个理解为mean, 一个理解为log_variance</li>
</ol>
<p>decoder过程就是对mean和var随机采样（得到z)，然后不断上采样(<code>Conv2DTranspose</code>)得到形状与源图一致的输出(得到z_decode)的过程。</p><ol>
<li>z_decode跟z做BCE loss</li>
<li>还要加一个regularization loss防止overfitting</li>
</ol>
<blockquote>
<p>此处请看书，演示了自定义的loss。因为keras高度封装，所以各种在封装之外的自定义的用法尤其值得关注。比如这里，自定义了loss之后，Model和fit里就不需要传Y，compile时也不需要传loss了。</p></blockquote>
<blockquote>
<p>loss是在最后一层layer里计算的，并且通过一个layer方法<code>add_loss</code>，把loss和input通知给了network（如果你想知道注入点的话）</p></blockquote>
<p>使用模型的话，就是生成两组随机数，当成mean和log_variance，观察decode之后的结果。</p><h3>GAN</h3>
<p><code>Generative adversarial network</code>可以创作以假乱真的图片。通过训练最好的造假和和最好的鉴别者来达到“创造”越来越逼近人类创作的作品。</p><ul>
<li><strong>Generator</strong> network: Takes as input a random vector (a random point in the latent space), and decodes it into a synthetic image</li>
<li><strong>Discriminator</strong> network (or adversary): Takes as input an image (real or synthetic), and predicts whether the image came from the training set or was created by the generator network.</li>
</ul>
<p><strong>deep convolutional GAN (DCGAN)</strong></p><ul>
<li>a GAN where the generator and discriminator are deep convnets.</li>
<li>In particular, it uses a <code>Conv2DTranspose</code> layer for image upsampling in the generator.</li>
</ul>
<p>训练生成器是冲着能让鉴别器尽可能鉴别为真的方向的：the generator is trained to <code>fool</code> the discriminator。</p><blockquote>
<p>这句话其实暗含了一个前提，下面会说，就是此时discriminator是确定的。即在确定的鉴别能力下，尽可能去拟合generator的输出，让它能通过当前鉴别器的测试。</p></blockquote>
<p>书中说训练DCGAN很复杂，而且很多trick, 超参靠的是经验而不是理论支撑，摘抄并笔记a bag of tricks如下：</p><ul>
<li>We use <code>tanh</code> as the last activation in the generator, instead of sigmoid, which is more commonly found in other types of models.</li>
<li>We sample points from the latent space using a <code>normal distribution</code> (Gaussian distribution), not a uniform distribution.</li>
<li>Stochasticity is good to induce robustness. Because GAN training results in a dynamic equilibrium, GANs are likely to get stuck in all sorts of ways. Introducing randomness during training helps prevent this. We introduce randomness in two ways:<ul>
<li>by using <code>dropout</code> in the discriminator</li>
<li>and by adding <code>random noise</code> to the labels for the discriminator.</li>
</ul>
</li>
<li>Sparse gradients can hinder GAN training. In deep learning, sparsity is often a desirable property, <strong>but not in GANs</strong>. Two things can induce gradient sparsity: <code>max pooling</code> operations and <code>ReLU</code> activations.<ul>
<li>Instead of max pooling, we recommend using <code>strided convolutions</code> for downsampling(用步长卷积代替pooling),</li>
<li>and we recommend using a <code>LeakyReLU</code> layer instead of a ReLU activation. It’s similar to ReLU, but it relaxes sparsity constraints by allowing small negative activation values.</li>
</ul>
</li>
<li>In generated images, it’s common to see <code>checkerboard artifacts</code>(stirde和kernel size不匹配千万的) caused by unequal coverage of the pixel space in the generator.<ul>
<li>To fix this, we use a kernel size that’s divisible by the stride size whenever we use a strided <code>Conv2DTranpose</code> or Conv2D in both the generator and the discriminator.</li>
</ul>
</li>
</ul>
<p><strong>Train</strong></p><ol>
<li>Draw random points in the latent space (random noise).</li>
<li>Generate images with generator using this random noise.</li>
<li>Mix the generated images with real ones.</li>
<li>Train discriminator using these mixed images, with corresponding targets:<ul>
<li>either “real” (for the real images) or “fake” (for the generated images).</li>
<li>所以鉴别器是<code>单独训练的</code>（前面笔记铺垫过了）</li>
<li>下面就是train整个DCGAN了：</li>
</ul>
</li>
<li>Draw new random points in the latent space.</li>
<li>Train gan using these random vectors, with targets that all say “these are real images.” This updates the weights of the generator (only, because the discriminator is frozen inside gan) to move them toward getting the discriminator to predict “these are real images” for generated images: this trains the generator to fool the discriminator.<ul>
<li>只train网络里的generator</li>
<li>discriminator不训练，因为是要用“已经训练到目前程度的”discriminator来做下面的任务</li>
<li>任务就是只送入伪造图，并声明所有图都是真的，去让generator生成能逼近这个声明的图</li>
<li>generator就是这么训练出来的。</li>
<li>所以实际代码是一次epoch是由train一个<code>discriminator</code>和train一个<code>GAN</code>组成.</li>
</ul>
</li>
</ol>
<p>因为鉴别器和生成器是一起训练的，因此前几轮生成的肯定是噪音，但前几轮鉴别器也是瞎鉴别的。</p></div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/Deep-Learning-with-Python-Notes/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/%E5%87%A0%E5%A4%A7%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95python%E5%AE%9E%E7%8E%B0/" target="_self">几大排序算法python实现</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/%E5%87%A0%E5%A4%A7%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95python%E5%AE%9E%E7%8E%B0/" target="_self">
                <time class="text-uppercase">
                    August 23 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><h2>冒泡排序</h2>
<p>冒泡排序基础原理是每一轮都让最大的值移到最右边，一句话就够了。</p><p>如果想小优化一下，可以在每一轮过后都把最后一个（已经是最大的值）排除出去，这种我把它称之为“压缩边界“，在下面的几种排序算法里都有反复提及。而且之所以说优化，就是不做也行，如果只是想演示算法核心思想的话。</p><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">bubble_sort</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span><span class="o">-</span><span class="n">i</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">arr</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">arr</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">]:</span>
                <span class="n">arr</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">arr</span>
<span class="n">bubble_sort</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</pre></div>
<p>ouput:</p>
<pre><code>[0, 1, 1, 1, 2, 3, 5]
</code></pre>
<h2>快速排序</h2>
<p>选出一个合适的（或任意的）中值(<code>pivot</code>），把比它大的和小的分列到两边，再对两边进行上述分类的递归操作。实际操作中往往会选定了<code>pivot</code>后，从右往左搜小数，从左往右搜大数，以规避pivot本身过大或过小时，如果选定的方向不对，可能每一次都需要把整个数组几乎遍历完才找到合适的数的情况。</p><p>again，这只是优化，如果不考虑这些，那么核心思想是非常简单的：</p><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">q_sort</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">arr</span>
    <span class="n">pivot</span> <span class="o">=</span> <span class="n">arr</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
    <span class="n">left</span>  <span class="o">=</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">arr</span> <span class="k">if</span> <span class="n">item</span> <span class="o">&lt;=</span> <span class="n">pivot</span><span class="p">]</span>
    <span class="n">right</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">arr</span> <span class="k">if</span> <span class="n">item</span> <span class="o">&gt;</span> <span class="n">pivot</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">q_sort</span><span class="p">(</span><span class="n">left</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">pivot</span><span class="p">]</span> <span class="o">+</span> <span class="n">q_sort</span><span class="p">(</span><span class="n">right</span><span class="p">)</span>
</pre></div>
<p>这个不但实现了（中值+两侧+递归）的思路+没有任何优化，效果已经出奇的好了！</p><p>但网上演示的都是下面这种花活，从两侧来压缩备选区域（压缩的意思是排好了的区域就不要管了），下面列了个表格来演示过程，看大家是不是能轻松看懂快排的两个核心机制：<code>标红位</code>，和<code>边界压缩</code>。说明如下：</p><ul>
<li>任意写个数组[6,7,3,2,14,9]，任取一个数为pivot，就第1个吧（6），</li>
<li>左箭头表示从右往左找第一个小于pivot的值，右箭头表示从左往右找第一个大于pivot的值</li>
<li>红色代表标红位，废位，即当前位找到本轮符合要求的值，但挪到两侧去了，$\color{red}{下一轮的符合条件的值应该放入这个标红位里}$</li>
<li>括号里的表示是这一轮该位置赋的新值，它来自于标红位，同时，括号的位置也就是上一轮的标红位</li>
<li>划掉的表示已经压缩了左右边界，下一轮就不要在这些数里面选了（为了视觉简洁，标红位就不划了）</li>
</ul>
<p>$
\require{cancel}
\begin{array}{c|cccccc|l}
index&amp;0&amp;1&amp;2&amp;3&amp;4&amp;5&amp;\
\hline
array&amp;\color{red}6&amp;7&amp;3&amp;2&amp;14&amp;9\
\underleftarrow{\small找小数}&amp;\cancel{(2)}&amp;7&amp;3&amp;\color{red}2&amp;\cancel{14}&amp;\cancel{9}&amp;找到2，放到索引0\
\underrightarrow{\small找大数}&amp;\cancel{2}&amp;\color{red}7&amp;3&amp;(7)&amp;\cancel{14}&amp;\cancel{9}&amp;找到7，放到索引3\
\underleftarrow{\small找小数}&amp;\cancel{2}&amp;(3)&amp;\color{red}3&amp;\cancel{7}&amp;\cancel{14}&amp;\cancel{9}&amp;找到3，放到索引2\
&amp;2&amp;3&amp;(6)&amp;7&amp;14&amp;9&amp;(1,2)索引间已没有大于6的数，排序完成，回填6
\end{array}
$</p><ol>
<li>注意第1次从右往左找比6小的数时，找到2，右边的14，9就可以全部划掉了，因为我永远是在用6在左右查找，这一次pass了，后面永远会pass</li>
</ol>
<ul>
<li>这样边界压缩得非常快，这就是称之为“快速”排序的原因吧？</li>
</ul>
<ol start="2">
<li>目前只完成一次分割（即按6为标识切分左右），接下来用同样的逻辑递归6左边的<code>[2]</code>和右边的<code>[7,14,9]</code>排序即可</li>
</ol>
<ul>
<li>所以快排就3个部分，一个主体，执行一次分割，然后对分割后的两个数组分别递归回去，这样代码怎么写也出来了：</li>
</ul>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">q_sort</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">):</span>
    <span class="c1"># （left， right）用来保存不断缩小的查找数组索引界限</span>
    <span class="c1">#  我上面模拟的过程里，就是划掉的数字的左右边界</span>
    <span class="n">left</span><span class="p">,</span> <span class="n">right</span> <span class="o">=</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">start</span>
    <span class="n">pivot</span> <span class="o">=</span> <span class="n">array</span><span class="p">[</span><span class="n">start</span><span class="p">]</span>

    <span class="k">while</span> <span class="n">left</span> <span class="o">&lt;</span> <span class="n">right</span><span class="p">:</span>
        <span class="c1"># 从右往左选小于pivot的数</span>
        <span class="n">matched</span> <span class="o">=</span> <span class="kc">False</span> <span class="c1"># 标识这一轮有没有找到合适的数（如果没找到其实说明排序已经完成）</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">left</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">right</span><span class="o">+</span><span class="mi">1</span><span class="p">)):</span> <span class="c1"># 去头，含尾, 反序</span>
            <span class="k">if</span> <span class="n">array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">pivot</span><span class="p">:</span>
                <span class="n">array</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="n">right</span> <span class="o">=</span> <span class="n">i</span>  <span class="c1"># 从右到左比到第i个才有比pivot小的数，那么i右侧全大于pivot，下次可以缩小范围了</span>
                <span class="n">index</span> <span class="o">=</span> <span class="n">i</span>
                <span class="n">matched</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">break</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">matched</span><span class="p">:</span>
            <span class="k">break</span>  <span class="c1"># 右侧没有找到更小的数，说明剩余数组全是大数，已经排完了</span>

        <span class="n">left</span> <span class="o">+=</span> <span class="mi">1</span> <span class="c1"># 找到了填入新数后就顺移一位</span>
        <span class="n">matched</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="c1"># 从左往右选大于pivot的数</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">left</span><span class="p">,</span> <span class="n">right</span><span class="p">):</span> <span class="c1"># 有头无尾</span>
            <span class="k">if</span> <span class="n">array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">pivot</span><span class="p">:</span>
                <span class="n">array</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="n">left</span> <span class="o">=</span> <span class="n">i</span> <span class="c1"># 此时i左侧也没有比pivot大的数，下次再找也可以忽略了，也标记下缩小范围</span>
                <span class="n">index</span> <span class="o">=</span> <span class="n">i</span>
                <span class="n">matched</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">break</span><span class="p">;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">matched</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">right</span> <span class="o">-=</span> <span class="mi">1</span>
    <span class="n">array</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">pivot</span> <span class="c1"># 把标红位设为pivot</span>

    <span class="c1"># 开始递归处理左右切片</span>
    <span class="k">if</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span>
        <span class="n">q_sort</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">end</span> <span class="o">&gt;</span> <span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span>
        <span class="n">q_sort</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">array</span>

<span class="n">arr</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="c1"># 我封装时为了兼容递归，要人为传入start, end，进入函数时自行计算一下好了</span>
<span class="n">q_sort</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
<p>output</p>
<pre><code>[0, 1, 1, 1, 2, 3, 5]
</code></pre>
<h2>堆排序</h2>
<ol>
<li>其实就是把数字摆成二叉树，知道二叉树是啥就行，或者看下面的动图</li>
<li>每当一个数字排入堆中的时候，都与父节点比一下大小，如果大于父节点，则与父节点交换位置</li>
</ol>
<ul>
<li>不与兄弟节点比较，即兄弟节点之间暂不排序</li>
</ul>
<ol start="3">
<li>交换到父节点后再跟当前位置的父节点比较，如此往复，至到根节点（<strong>递归警告</strong>）</li>
<li>一轮摆完后，最大的数肯定已经<strong>上浮</strong>到根节点了，把它与最末的一个数字调换位置（这个数字是一个相对小，但不一定是最小的），然后把最大的这个数从堆里移除（已经确认是最大的，位置也就确认了，不再参与比较）</li>
<li>实现的时候，因为有“找父/子节点比大小”这样的逻辑，显然可以直接用上二叉树的性质，不要自己去观察或归纳了。</li>
</ol>
<p>动图比较长，耐心看下：</p><figure class="vertical-figure" style="flex: 35.714285714285715" ><img width="220" height="308" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/72b4116eb1e49a3b1eaa9097a2890b60.jpg" alt=""/></figure><blockquote>
<p>在实现每一轮的遍历数字较大的那个子节点并交换数字的过程中，我之前用的是递归，在小数据量顺利通过，但上万条数据时碰到了<code>RecursionError: maximum recursion depth exceeded in comparison</code>, 查询本机迭代大小设置为1000，但设到几十万就不起作用了（虽然不报错），于是改成了<code>while</code>循环，代码几乎没变，但是秒过了。</p></blockquote>
<p>递归只是让代码看起来简洁而牛逼，并没有创造什么新的东西，while能行那就算过了吧。</p><p>但是代码开始dirty了起来，大量的代码在控制边界和描述场景，显然有些条件可能是冗余的，我没有很好地合并这些边界和条件导致if太多，这是个不好的演示，但三个核心函数还是阐释了这种算法的思路：</p><ul>
<li>摆成树（堆）</li>
<li>从leaf到root冒泡 (child去比parent)</li>
<li>从root到leaf冒泡 (parent去比child)</li>
</ul>
<div class="highlight"><pre><span></span><span class="c1"># helper</span>
<span class="n">get_parent_index</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">i</span> <span class="p">:</span> <span class="nb">max</span><span class="p">((</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">get_child_index</span>  <span class="o">=</span> <span class="k">lambda</span> <span class="n">i</span> <span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>

<span class="k">def</span> <span class="nf">heap_sort</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="n">heapify</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>                    <span class="c1"># 初排</span>
    <span class="n">siftDown</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>    <span class="c1"># 整理</span>
    <span class="k">return</span> <span class="n">arr</span>

<span class="k">def</span> <span class="nf">heapify</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">while</span> <span class="n">index</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
        <span class="n">p_index</span> <span class="o">=</span> <span class="n">get_parent_index</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
        <span class="n">parent</span>  <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">p_index</span><span class="p">]</span>
        <span class="n">child</span>   <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">child</span> <span class="o">&gt;</span> <span class="n">parent</span><span class="p">:</span>
            <span class="n">arr</span><span class="p">[</span><span class="n">p_index</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">p_index</span><span class="p">]</span>
            <span class="n">siftUp</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">p_index</span><span class="p">)</span>
        <span class="n">index</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">arr</span>

<span class="k">def</span> <span class="nf">siftUp</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">c_index</span><span class="p">):</span>
    <span class="n">p_index</span> <span class="o">=</span> <span class="n">get_parent_index</span><span class="p">(</span><span class="n">c_index</span><span class="p">)</span>
    <span class="n">parent</span>  <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">p_index</span><span class="p">]</span>
    <span class="n">leaf</span>    <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">c_index</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">parent</span> <span class="o">&lt;</span> <span class="n">leaf</span><span class="p">:</span>
        <span class="n">arr</span><span class="p">[</span><span class="n">p_index</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">c_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">c_index</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">p_index</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">p_index</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">siftUp</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">p_index</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">siftDown</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    1. 交换首尾两个数，这样尾数就变成了最大</span>
<span class="sd">    2. 跟两个子节点中较大的比较，并迭代，递归下去</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">while</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="n">end</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">start</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">arr</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">end</span><span class="p">]</span> <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">end</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">left_i</span>  <span class="o">=</span> <span class="n">get_child_index</span><span class="p">(</span><span class="n">start</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">left_i</span> <span class="o">&gt;=</span> <span class="n">end</span><span class="p">:</span> 
            <span class="c1"># 子结点是end，就不要比了，把当前节点设为新end</span>
            <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">end</span> <span class="o">-=</span> <span class="mi">1</span>
            <span class="k">continue</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">right_i</span> <span class="o">=</span> <span class="n">left_i</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">left_i</span>
            <span class="k">if</span> <span class="n">right_i</span> <span class="o">&lt;</span> <span class="n">end</span><span class="p">:</span>
                <span class="c1"># 右边没有到end的话，取出值比大小</span>
                <span class="c1"># 并且把下一轮的start设为选中的子节点</span>
                <span class="n">index</span> <span class="o">=</span> <span class="n">left_i</span> <span class="k">if</span> <span class="n">arr</span><span class="p">[</span><span class="n">left_i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">arr</span><span class="p">[</span><span class="n">right_i</span><span class="p">]</span> <span class="k">else</span> <span class="n">right_i</span>
            <span class="n">parent</span>  <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">start</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">parent</span> <span class="o">&lt;</span> <span class="n">arr</span><span class="p">[</span><span class="n">index</span><span class="p">]:</span>
                <span class="n">arr</span><span class="p">[</span><span class="n">start</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">start</span><span class="p">]</span>
        <span class="c1"># 如果左叶子已经被标记为end  (已提前return)</span>
        <span class="c1"># 如果右边叶子被标记为end</span>
        <span class="c1"># 如果下一个索引被标记为end</span>
        <span class="c1"># 都表示本轮遍历已经到底, end往前移一位即可</span>
        <span class="k">if</span> <span class="n">right_i</span> <span class="o">&gt;=</span> <span class="n">end</span> <span class="ow">or</span> <span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">right_i</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># 用start=0表示需要进行一次首尾替换再从头到尾移动一次</span>
            <span class="n">end</span> <span class="o">-=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># 否则进入下一个循环</span>
            <span class="c1"># 起点就是用来跟父级做比较的索引</span>
            <span class="c1"># 终点不变</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">index</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
    <span class="kn">import</span> <span class="nn">time</span>
<span class="c1">#     np.random.seed(7)</span>
<span class="c1">#     length = 20000</span>
<span class="c1">#     arr = list(np.random.randint(0, length*5, size=(length,)))</span>
    <span class="n">arr</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="s2">&quot;65318724&quot;</span><span class="p">)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">heap_sort</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">s</span><span class="p">[:</span><span class="mi">100</span><span class="p">])</span>
</pre></div>
<p>output</p>
<pre><code>4.696846008300781e-05 
 ['1', '2', '3', '4', '5', '6', '7', '8']
</code></pre>
<h2>归并排序</h2>
<p>这次先看图吧，看你能总结出啥：
<figure class="vertical-figure" style="flex: 35.714285714285715" ><img width="220" height="308" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/72b4116eb1e49a3b1eaa9097a2890b60.jpg" alt=""/></figure></p><ol>
<li>第一步是把数组打散后两两排序，实现每一组（2个元素）是排好序的</li>
<li>第二步仍然是两两排序，但是把前面排序好的每两个组成一个组：</li>
</ol>
<ul>
<li>这样每组就有2个数了，但组数就减半了</li>
<li>每一组拿出当前最前面的数出来比较，每次挑1个最小的，移出来</li>
<li>剩下的组里数字有多有少，仍然比较组里面排最前的那个（因为每组已经从小到大排好了，最前面那个就是组里最小的）</li>
<li>所以代码里能跟踪两个组里当前的“最前的索引”是多少就行了</li>
</ul>
<ol start="3">
<li>继续合并，单从理论上你也能发现，每组的数字个数会越来越多，组数却越来越少， 显然，最终会归并成一个组，而且已经是排好序了的。</li>
</ol>
<p>这就是归并名字的<strong>由来</strong>。后面还有一种<code>希尔算法</code>，正好是它的相反，即打得越来越散，散成每组只有一个元素的时候，排序也排好了，看到那一节的时候注意对比。</p><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>
<span class="k">def</span> <span class="nf">merge_sort</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    每一轮比较的时候是把选中的元素填到另一个数组里</span>
<span class="sd">    为了减少内存消耗，就循环用两个数组</span>
<span class="sd">    我们用交替设置i和j为0和1来实现这个逻辑</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">start</span>    <span class="o">=</span> <span class="mi">0</span>
    <span class="n">step</span>     <span class="o">=</span> <span class="mi">1</span>
    <span class="n">length</span>   <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">lists</span>    <span class="o">=</span> <span class="p">[</span><span class="n">arr</span><span class="p">,</span> <span class="p">[]]</span>
    <span class="n">i</span><span class="p">,</span> <span class="n">j</span>     <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span>
    <span class="k">while</span> <span class="n">step</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">:</span>
        <span class="n">compare</span><span class="p">(</span><span class="n">lists</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">start</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">lists</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
        <span class="n">step</span> <span class="o">*=</span> <span class="mi">2</span>
        <span class="n">i</span><span class="p">,</span> <span class="n">j</span>  <span class="o">=</span> <span class="n">j</span><span class="p">,</span> <span class="n">i</span>
    <span class="k">return</span> <span class="n">lists</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">gen_indexs</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">length</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    根据左边界和步长确定本轮拿来比较的两个数组的边界</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">left_end</span>    <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="n">step</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">right_start</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">start</span> <span class="o">+</span> <span class="n">step</span><span class="p">,</span> <span class="n">length</span><span class="p">)</span>
    <span class="n">right_end</span>   <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">right_start</span> <span class="o">+</span> <span class="n">step</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">length</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">start</span><span class="p">,</span> <span class="n">left_end</span><span class="p">,</span> <span class="n">right_start</span><span class="p">,</span> <span class="n">right_end</span>


<span class="k">def</span> <span class="nf">compare</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">result</span><span class="p">):</span>
    <span class="n">result</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
    <span class="n">left_start</span><span class="p">,</span> <span class="n">left_end</span><span class="p">,</span> <span class="n">right_start</span><span class="p">,</span> <span class="n">right_end</span> \
                <span class="o">=</span> <span class="n">gen_indexs</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">length</span><span class="p">)</span>
    <span class="n">left_index</span>  <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 组内索引(0, step-1)</span>
    <span class="n">right_index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">left_start</span> <span class="o">&lt;=</span> <span class="n">length</span><span class="p">:</span>
        <span class="n">left</span>    <span class="o">=</span> <span class="n">left_start</span> <span class="o">+</span> <span class="n">left_index</span>
        <span class="n">right</span>   <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">right_start</span> <span class="o">+</span> <span class="n">right_index</span><span class="p">,</span> <span class="n">length</span><span class="p">)</span>
        <span class="n">l_done</span>  <span class="o">=</span> <span class="kc">False</span>
        <span class="n">r_done</span>  <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">arr</span><span class="p">[</span><span class="n">left</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">arr</span><span class="p">[</span><span class="n">right</span><span class="p">]:</span>
            <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">arr</span><span class="p">[</span><span class="n">left</span><span class="p">])</span>
            <span class="n">left_index</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">left</span>   <span class="o">=</span> <span class="n">left_start</span> <span class="o">+</span> <span class="n">left_index</span>
            <span class="n">l_done</span> <span class="o">=</span> <span class="n">left</span> <span class="o">==</span> <span class="n">right_start</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">arr</span><span class="p">[</span><span class="n">right</span><span class="p">])</span>
            <span class="n">right_index</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">r_done</span> <span class="o">=</span> <span class="p">(</span><span class="n">right_start</span> <span class="o">+</span> <span class="n">right_index</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">right_end</span>
        <span class="k">if</span> <span class="n">l_done</span> <span class="ow">or</span> <span class="n">r_done</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">l_done</span><span class="p">:</span>
                <span class="c1"># 左边没数了，右边的数全塞到result里去</span>
                <span class="n">result</span> <span class="o">+=</span> <span class="n">arr</span><span class="p">[</span><span class="n">right</span><span class="p">:</span><span class="n">right_end</span><span class="p">]</span>
                <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">arr</span><span class="p">[</span><span class="n">right_end</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># 右边没数了，左边剩下的数全塞到result里去</span>
                <span class="n">result</span> <span class="o">+=</span> <span class="n">arr</span><span class="p">[</span><span class="n">left</span><span class="p">:</span><span class="n">right_start</span><span class="p">]</span>
            <span class="n">left_start</span><span class="p">,</span> <span class="n">left_end</span><span class="p">,</span> <span class="n">right_start</span><span class="p">,</span> <span class="n">right_end</span> \
                        <span class="o">=</span> <span class="n">gen_indexs</span><span class="p">(</span><span class="n">right_end</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">length</span><span class="p">)</span>
            <span class="n">left_index</span>  <span class="o">=</span> <span class="mi">0</span>
            <span class="n">right_index</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
    <span class="kn">import</span> <span class="nn">time</span>
<span class="c1">#     np.random.seed(7)</span>
<span class="c1">#     length = 20000</span>
<span class="c1">#     arr = list(np.random.randint(0, length*5, size=(length,)))</span>
    <span class="n">arr</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">17</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">22</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">65</span><span class="p">]</span><span class="c1">#,2,13,4,6,17,33,8,0,4,17,22]</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">merge_sort</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="p">,</span> <span class="n">s</span><span class="p">[:</span><span class="mi">100</span><span class="p">])</span>
</pre></div>
<p>output</p>
<pre><code>5.626678466796875e-05
[0, 1, 1, 2, 3, 5, 6, 7, 8, 9, 9, 17, 22, 65]
</code></pre>
<p>以上是我对着动画实现的一个版本，很繁琐，而且只是直观地把动画演示了一遍，即先两两组合，对比，再四四对比，直到最后只有两个大数组，比一次。直到我看到这个思路，我把它实现出来如下：</p><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mergesort</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">end</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">mid</span> <span class="o">=</span> <span class="p">(</span><span class="n">start</span> <span class="o">+</span> <span class="n">end</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="n">mergesort</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">mid</span><span class="p">)</span> <span class="c1"># left</span>
        <span class="n">mergesort</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">mid</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span> <span class="c1"># right</span>
        <span class="n">sort</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">mid</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span> 
        <span class="c1"># 最里层：([0:1],[1:2]) -&gt; (start, mid, end) 为(0,1,2)</span>
        <span class="c1"># 所以退出条件是 end - start &gt; 1</span>

<span class="k">def</span> <span class="nf">sort</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">left</span><span class="p">,</span> <span class="n">mid</span><span class="p">,</span> <span class="n">right</span><span class="p">):</span>
    <span class="n">p1</span> <span class="o">=</span> <span class="n">left</span>
    <span class="n">p2</span> <span class="o">=</span> <span class="n">mid</span>
    <span class="n">temp</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># 本轮排序的结果</span>
    <span class="c1"># 左右两个数组分别按顺序取出最前一个来比较大小</span>
    <span class="c1"># 小数拿到临时数组里去，游标加1</span>
    <span class="k">while</span> <span class="n">p1</span> <span class="o">&lt;</span> <span class="n">mid</span> <span class="ow">and</span> <span class="n">p2</span> <span class="o">&lt;</span> <span class="n">right</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">arr</span><span class="p">[</span><span class="n">p2</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">arr</span><span class="p">[</span><span class="n">p1</span><span class="p">]:</span>
            <span class="n">temp</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">arr</span><span class="p">[</span><span class="n">p1</span><span class="p">])</span>
            <span class="n">p1</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">temp</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">arr</span><span class="p">[</span><span class="n">p2</span><span class="p">])</span>
            <span class="n">p2</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="c1"># 不管是左边还是右边，剩下的都是已经排好的（大数），直接接到数组后面</span>
    <span class="k">if</span> <span class="n">p1</span> <span class="o">&lt;</span> <span class="n">mid</span><span class="p">:</span>
        <span class="n">temp</span> <span class="o">+=</span> <span class="n">arr</span><span class="p">[</span><span class="n">p1</span><span class="p">:</span><span class="n">mid</span><span class="p">]</span>
    <span class="k">elif</span> <span class="n">p2</span> <span class="o">&lt;</span> <span class="n">right</span><span class="p">:</span>
        <span class="n">temp</span> <span class="o">+=</span> <span class="n">arr</span><span class="p">[</span><span class="n">p2</span><span class="p">:</span><span class="n">right</span><span class="p">]</span>

    <span class="n">arr</span><span class="p">[</span><span class="n">left</span><span class="p">:</span><span class="n">right</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span>
</pre></div>
<p>sort部分没变，还是两边比较，永远取小的一个，直到排成一排变成一组。主体变成了mergesort()的递归。用文字描述的话，就是这个方法就做了一件事：把当前数组左右分开，然后用永远取最前一个来当最小值的方式（sort方法）完成排序。
等于是直接就走到了我实现的方法的最后一步，而用递归的方式，让更小的单元完成排序，比如每8个，每4个，每2个，真实发生排序的时候，仍然是我写的代码的第一层，就是两两排序。但是代码简洁抽象好多。</p><p>如果把递归理解为异步的话：</p><div class="highlight"><pre><span></span><span class="k">await</span> <span class="nx">sort_lert</span><span class="p">()</span>
<span class="k">await</span> <span class="nx">sort_right</span><span class="p">()</span>
<span class="nx">sort</span><span class="p">(</span><span class="nx">left</span><span class="p">,</span> <span class="nx">right</span><span class="p">)</span>
</pre></div>
<p>即代码真走到第3行了的话，所有的数据已经排好序了</p><h2>基数排序</h2>
<figure class="vertical-figure" style="flex: 35.714285714285715" ><img width="220" height="308" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/72b4116eb1e49a3b1eaa9097a2890b60.jpg" alt=""/></figure><p>看图，为什么从个位向高位依次排过去为什么就能保证后面高位的排序不会影响低序的，直观来理解的话，就是</p><ol>
<li>如果高位数字不一样，那么低位顺序是没意义的，按高位大小排即可</li>
<li>如果高位数字一样，那么低位已经排好序了</li>
<li>按这个逻辑由低位向高位排，按归纳法，可以推到适用普遍情况的</li>
</ol>
<p>这里就有一个逻辑bug了，我本来就是要根据大小排序比如1万个数字，结果你说要先把这1万个数字根据个位数大小排一遍，再根据十位数大小排一遍，我无数次地排这1万个数字，为何不直接按大小把它排好算了呢？</p><p>这就是这个算法存在的意义吧，根据位数排序数次快的很，因为你不需要排它，你只需要做10个容器，编号为0-9，你要排序的位数上，数字是几就把整个数字丢到对应编号的容器里，自然就实现了排序，因为0-9本身就是个排好了序的数组。</p><blockquote>
<p>你甚至可以用字典，key就是0到9，但数组天生自带了数字Index，何乐而不为？</p></blockquote>
<p>演示：385, 17, 45, 26, 72, 1265, 用个位数字排序，排好后的容器（数组）应该是：</p><div class="highlight"><pre><span></span><span class="p">[</span>
    <span class="p">[],</span>
    <span class="p">[],</span>
    <span class="p">[</span><span class="mi">72</span><span class="p">],</span>
    <span class="p">[],</span>
    <span class="p">[],</span>
    <span class="p">[</span><span class="mi">835</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">1265</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">26</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">17</span><span class="p">],</span>
    <span class="p">[],</span>
    <span class="p">[]</span>
<span class="p">]</span>
</pre></div>
<p>其实这也是排序，和接下来要讲的插入排序很像。它没有查找的过程，时间复杂度为0。上面剧透的shell排序还没讲，又剧透了另一个。</p><p>别的就没啥好说的了，由低位到高位循环就是了。</p><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_number</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    提取指定位数数字的方法：</span>
<span class="sd">    个位：527 % 10^1 // 10^0 = 7</span>
<span class="sd">    十位：527 % 10^2 // 10^1 = 2</span>
<span class="sd">    百位：527 % 10^3 // 10^2 = 5</span>
<span class="sd">    千位：527 % 10^4 // 10^3 = 0</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="n">num</span> <span class="o">%</span> <span class="mi">10</span><span class="o">**</span><span class="p">(</span><span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">10</span><span class="o">**</span><span class="n">index</span>

<span class="k">def</span> <span class="nf">digit_length</span><span class="p">(</span><span class="n">number</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">math</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">number</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">10</span> <span class="k">else</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">number</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">digit_sort</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    对第index个数字进行排序</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">):</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">([])</span> <span class="c1"># [[]] * 10 会造成引用传递</span>
    <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">arr</span><span class="p">:</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">get_number</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">num</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">digit</span> <span class="k">for</span> <span class="n">sublist</span> <span class="ow">in</span> <span class="n">results</span> <span class="k">for</span> <span class="n">digit</span> <span class="ow">in</span> <span class="n">sublist</span><span class="p">]</span>  <span class="c1"># flatten the 2-d array</span>

<span class="k">def</span> <span class="nf">radix_sort</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="n">length</span> <span class="o">=</span> <span class="n">digit_length</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">arr</span><span class="p">))</span> <span class="c1"># 演示如何从数学上取得数字的长度（几十万次迭代效率只有毫米级的差别）</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">length</span><span class="p">):</span>
        <span class="n">arr</span> <span class="o">=</span> <span class="n">digit_sort</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">arr</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
    <span class="kn">import</span> <span class="nn">time</span>
<span class="c1">#     np.random.seed(7)</span>
<span class="c1">#     length = 20000</span>
<span class="c1">#     arr = list(np.random.randint(0, length*50, size=(length,)))</span>
    <span class="n">arr</span> <span class="o">=</span> <span class="p">[</span><span class="mi">954</span><span class="p">,</span><span class="mi">354</span><span class="p">,</span><span class="mi">309</span><span class="p">,</span><span class="mi">411</span><span class="p">]</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">radix_sort</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="p">,</span> <span class="n">s</span><span class="p">[:</span><span class="mi">100</span><span class="p">])</span>
</pre></div>
<p>output:</p>
<pre><code>0.0008242130279541016
[309, 354, 411, 954]
</code></pre>
<h2>插入排序</h2>
<p>准备一个空数组，依次把原数组的每一个数插入到该数组里的适当位置。上面说的基数排序里的按位初排就有点类似插入排序，只不过基数排序里不需要比较大小（即235， 15， 1375）这样的数，如果看个位，都是在索引5的位置，且无序），而且插入的位置是固定的，所以没有时间复杂度。</p><p>而插入排序则实实在在地要在排入的数组里遍历才能找到正确的插入位置，越排到后面，新数组就越长，时间复杂度也就越来越大了。</p><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">insert_sort</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="n">rst</span> <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">arr</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
        <span class="n">found</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">rst</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">j</span> <span class="o">&gt;</span> <span class="n">i</span><span class="p">:</span>
                <span class="n">rst</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="c1"># 排到第一个比它大的前面</span>
                <span class="n">found</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">break</span><span class="p">;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">found</span><span class="p">:</span>
            <span class="n">rst</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">rst</span>

<span class="n">insert_sort</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">34</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
<p>output</p>
<pre><code>[0, 1, 5, 6, 9, 34]
</code></pre>
<h2>希尔排序</h2>
<ol>
<li><code>归并排序</code>是化整为零，两两比较后再组合，分组越来越大，最终变成一组</li>
<li>希尔排序是一开始就对半分（注：如果不能整除，如11//2=5, 这样会有3组），每一组相同位置的数做比较，实现一轮过后分组间<code>同位置的数</code>是顺序排列的</li>
<li>每组元素再减半，就上一条来说是(5//2=2，即上一层一组5个，下一轮每组就只有2个了)，以此往复，让组数越来越多，组内元素却越来越少，极端情况就是每组只有1个了，再参考前面总结的“<strong>分组间同位置的数是顺序排列的</strong>”这一结论，说明整个数组已经排好序了（退出条件get）。这个思路妙不妙？</li>
</ol>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">shell_sort</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="n">group</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>

    <span class="k">while</span> <span class="n">group</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)):</span>
            <span class="n">right</span>   <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">current</span> <span class="o">=</span> <span class="n">i</span>
            <span class="k">while</span> <span class="n">current</span> <span class="o">&gt;=</span> <span class="n">group</span> <span class="ow">and</span> <span class="n">arr</span><span class="p">[</span><span class="n">current</span> <span class="o">-</span> <span class="n">group</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">right</span><span class="p">:</span>
                <span class="n">arr</span><span class="p">[</span><span class="n">current</span><span class="p">]</span> <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">current</span> <span class="o">-</span> <span class="n">group</span><span class="p">]</span>
                <span class="n">current</span> <span class="o">-=</span> <span class="n">group</span>
            <span class="n">arr</span><span class="p">[</span><span class="n">current</span><span class="p">]</span> <span class="o">=</span> <span class="n">right</span>
        <span class="n">group</span> <span class="o">//=</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">arr</span>

<span class="n">shell_sort</span><span class="p">([</span><span class="mi">34</span><span class="p">,</span><span class="mi">24</span><span class="p">,</span><span class="mi">538</span><span class="p">,</span><span class="mi">536</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
<p>output</p>
<pre><code>[1, 24, 34, 536, 538]
</code></pre>
<hr />
<p>最后，生成可重复的随机数测几轮， quick sort要快一些：</p><div class="highlight"><pre><span></span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
    <span class="kn">import</span> <span class="nn">time</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
    <span class="n">length</span> <span class="o">=</span> <span class="mi">20000</span>
    <span class="n">arr</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">length</span><span class="o">*</span><span class="mi">50</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">length</span><span class="p">,)))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">length</span><span class="si">}</span><span class="s1"> random integers sort comparation:&#39;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;-------------round </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">------------&#39;</span><span class="p">)</span>
        <span class="c1"># insert is too slow</span>
        <span class="c1"># or my implementation is not so good</span>
<span class="c1">#         start = time.time()</span>
<span class="c1">#         s1 = insert_sort(arr)</span>
<span class="c1">#         print(f&quot;insert_sort\t {time.time()-start:.5f} seconds&quot;)</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">s2</span> <span class="o">=</span> <span class="n">quick_sort</span><span class="p">(</span><span class="n">arr</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;quick_sort</span><span class="se">\t</span><span class="s2"> </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">s3</span> <span class="o">=</span> <span class="n">shell_sort</span><span class="p">(</span><span class="n">arr</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;shell_sort</span><span class="se">\t</span><span class="s2"> </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">s4</span> <span class="o">=</span> <span class="n">heap_sort</span><span class="p">(</span><span class="n">arr</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;heap_sort</span><span class="se">\t</span><span class="s2"> </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">s5</span> <span class="o">=</span> <span class="n">merge_sort</span><span class="p">(</span><span class="n">arr</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;merge_sort</span><span class="se">\t</span><span class="s2"> </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">s6</span> <span class="o">=</span> <span class="n">radix_sort</span><span class="p">(</span><span class="n">arr</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;radix_sort</span><span class="se">\t</span><span class="s2"> </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;first 10 numbers:</span><span class="se">\n</span><span class="si">{</span><span class="n">s2</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="si">{</span><span class="n">s3</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="si">{</span><span class="n">s4</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="si">{</span><span class="n">s5</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="si">{</span><span class="n">s6</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
<p>output</p>
<pre><code>20000 random integers sort comparation:
-------------round 1------------
quick_sort	 0.07970 seconds
shell_sort	 0.17623 seconds
heap_sort	 0.32919 seconds
merge_sort	 0.20177 seconds
radix_sort	 0.18000 seconds
-------------round 2------------
quick_sort	 0.05894 seconds
shell_sort	 0.15423 seconds
heap_sort	 0.28844 seconds
merge_sort	 0.20043 seconds
radix_sort	 0.19310 seconds
-------------round 3------------
quick_sort	 0.06169 seconds
shell_sort	 0.18299 seconds
heap_sort	 0.33159 seconds
merge_sort	 0.20836 seconds
radix_sort	 0.20003 seconds
-------------round 4------------
quick_sort	 0.05780 seconds
shell_sort	 0.15414 seconds
heap_sort	 0.26780 seconds
merge_sort	 0.18810 seconds
radix_sort	 0.17084 seconds
</code></pre>
</div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/%E5%87%A0%E5%A4%A7%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95python%E5%AE%9E%E7%8E%B0/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/%E6%88%91%E7%9A%84%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/" target="_self">我的知识图谱入门笔记</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/%E6%88%91%E7%9A%84%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/" target="_self">
                <time class="text-uppercase">
                    June 17 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><h1>Knowledge Graph</h1>
<ul>
<li>信息是指外部的客观事实。举例：这里有一瓶水，它现在是7°。</li>
<li>知识是对外部客观规律的归纳和总结。举例：水在零度的时候会结冰。</li>
</ul>
<p>换句话说，知识图谱是由一条条知识组成，每条知识表示为一个SPO三元组(Subject-Predicate-Object)。</p><p>$\boxed{Subject} \xrightarrow{Predicate} \boxed{Object}$</p><h1>语义网络(Semantic Network)</h1>
<p>语义网络由相互连接的节点和边组成，节点表示概念或者对象，边表示他们之间的关系(is-a关系，比如：猫是一种哺乳动物；part-of关系，比如：脊椎是哺乳动物的一部分)</p><p>。在表现形式上，语义网络和知识图谱相似，但语义网络更侧重于描述概念与概念之间的关系，（有点像生物的层次分类体系——界门纲目科属种），而知识图谱则更偏重于描述实体之间的关联。</p><h1>RDF(Resoure Description Framework)</h1>
<p>RDF(Resource Description Framework)，即资源描述框架，是W3C制定的，用于描述实体/资源的标准数据模型。RDF图中一共有三种类型，International Resource Identifiers(IRIs)，blank nodes 和 literals。下面是SPO每个部分的类型约束：</p><ul>
<li>Subject可以是IRI或blank node。可以理解为<code>URI</code></li>
<li>Predicate是IRI。</li>
<li>Object三种类型都可以。</li>
</ul>
<p>也就是说字面量不能做主语？</p><p>将罗纳尔多的原名与中文名关联起来的RDF表示：</p><p>$\boxed{www.kg.com/person/1} \xrightarrow{kg:chineseName} \boxed{罗纳尔多·路易斯·纳扎里奥·达·利马}$</p><blockquote>
<p>可见，主语的指代性要强很多，所以字面量（宾语）用作主语会丧失这种精确性（唯一性）。</p></blockquote>
<ul>
<li>&quot;<code>www.kg.com/person/1</code>&quot;是一个IRI，用来唯一的表示“罗纳尔多”这个实体。&quot;kg:chineseName&quot;也是一个IRI，用来表示“中文名”这样一个属性。&quot;kg:&quot;是RDF文件中所定义的prefix，如下所示。</li>
<li>@<code>prefix kg</code>: <a href="http://www.kg.com/ontology/">http://www.kg.com/ontology/</a> 即，kg:chineseName其实就是&quot;http:// www.kg.com/ontology/chineseName&quot;的缩写。</li>
</ul>
<p>这样知识图谱的正确表示其实是：</p><figure  style="flex: 81.6618911174785" ><img width="1140" height="698" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/055b61a7e0a74762f15ece471ec22ee8.png" alt=""/></figure><p>而不是网传的简单的画几个对象连几根线，也就是说，能用URI表示的，尽量都用URI表示。</p><h1>Identifying graph-shaped problems(应用场景)</h1>
<ul>
<li><p>does our problem involve understanding <code>relationships</code> between entities?</p><ul>
<li>Recommendations</li>
<li>Next best action</li>
<li>Fraud detection</li>
<li>Identity resolution</li>
<li>Data lineage</li>
</ul>
</li>
<li><p>does our problem involve a lot of <code>self-referencing</code> to the same type of entity?</p><ul>
<li>Organisational hierachies</li>
<li>Social influencers</li>
<li>Friends of friends</li>
<li>Churn detection</li>
</ul>
</li>
<li><p>does the problem explore <code>relationships of varying or unknown depth</code>?</p><ul>
<li>Supply chain visibility</li>
<li>Bill of  Materials(BOM)</li>
<li>Network management</li>
</ul>
</li>
<li><p>does our problem involve discovering lots of <code>different routers or paths</code>?</p><ul>
<li>Logistics and routing</li>
<li>Infrastructure management</li>
<li>Dependency tracing</li>
</ul>
</li>
</ul>
<h1>Neo4j</h1>
<figure  style="flex: 88.44507845934379" ><img width="1240" height="701" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/1da110e1163eb31331ea2c21528a4ae2.png" alt=""/></figure><ol>
<li><code>:person</code>, <code>:Car</code>, <code>:Vehicle</code> are <code>Label</code></li>
<li>even <code>relationship</code> can also have(own) properties</li>
</ol>
<h2>AsciiArt</h2>
<h3>for Nodes</h3>
<p><code>(p:Person:Mammal{name:'walker'})</code></p><h3>for Relationships</h3>
<p><code>- [:HIRED {type: 'fulltime'}] -&gt;</code></p><h2>CRUD</h2>
<h3>Create</h3>
<figure  style="flex: 87.94326241134752" ><img width="1240" height="705" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/1664c809c2f78ab7dc14392c1ef00ac9.png" alt=""/></figure><h4>Constraints</h4>

<pre><code>CREATE  CONSTRAINT ON (p:Person)
ASSERT p.name IS UNIQUE
</code></pre>
<p>所以如下语句会报错：</p>
<pre><code>CREATE (a:Person {name: &quot;Ann&quot;})
CREATE (a) - [:HAS_PET] -&gt; (:Dog {name: &quot;Sam&quot;})
</code></pre>
<p>要用<code>merge</code></p>
<pre><code>MERGE (a:Person {name: &quot;Ann&quot;})
CREATE (a) - [:HAS_PET] -&gt; (:Dog {name: &quot;Sam&quot;})
</code></pre>
<h4>Set</h4>
<p>属性可以用<code>JSON</code>格式写，也可以用<code>SET</code>语法（下面的查询语句也是一样）</p>
<pre><code>MERGE (a:Person {name: &quot;Ann&quot;})
ON CREATE SET
    a.twitter = &quot;@ann&quot;
MERGE (a) - [:HAS_PET] -&gt; (:Dog {name: &quot;Sam&quot;})
</code></pre>
<p>同时，看到了吗？<code>create</code>只能出现一次（同一个对象的话）</p><h3>Read</h3>
<blockquote>
<p>who drives a car owned by a lover?</p></blockquote>
<div class="highlight"><pre><span></span><span class="k">MATCH</span><span class="w"></span>
<span class="p">(</span><span class="n">p1</span><span class="p">:</span><span class="n">Person</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">[:</span><span class="n">DRIVES</span><span class="p">]</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">(</span><span class="k">c</span><span class="p">:</span><span class="n">Car</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">[:</span><span class="n">OWNED_BY</span><span class="p">]</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">(</span><span class="n">p2</span><span class="p">:</span><span class="n">Person</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="p">[:</span><span class="n">LOVES</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">(</span><span class="n">P1</span><span class="p">)</span><span class="w"></span>
<span class="k">RETURN</span><span class="w"></span>
<span class="n">p1</span><span class="w"></span>
</pre></div>
<p>其中，因为问的是lover，没有指向性，所以如果是两个互相相爱，后半截也可以是p2指向p1</p>
<pre><code>match p = (n) -[*1..2] -&gt; (m) where n.name='特朗普' return p
match p =  ({name: '特朗普'}) - [*1..2] -&gt; () return p # 简化
match p = (n)-[m]-&gt;(q) where m.name = '丈夫' return n,q skip 10 limit 5
match p = (n)-[:丈夫]-&gt;(q) return n,q skip 10 limit 5 # 简化
</code></pre>
<p>解读：</p><ol>
<li>上面写法很简略，注意观察一下</li>
<li>又一次演示了直接用json来做where和单独用<code>where</code>关键字的写法区别（要多命名一个变量）</li>
<li>p跟n的区别，p是返了整个网络，如果<code>return n</code>，那么就是n自身(一个节点）。</li>
</ol>
<p>4, 但是如果<code>return n, m</code>，那么又把一层网络给select出来了 
5. [*1..2]表示跟踪两层
6. 如果不需要对n,m进行where,set操作，可以不设置变量
7. <code>relationship</code>也可以过滤，也有name等属性
8. <code>limit</code>, <code>skip</code> 等用法</p><h4>Tabular Results</h4>
<div class="highlight"><pre><span></span><span class="k">MATCH</span><span class="w"> </span><span class="p">(</span><span class="n">p</span><span class="p">:</span><span class="n">Person</span><span class="w"> </span><span class="err">{</span><span class="n">name</span><span class="p">:</span><span class="w"> </span><span class="ss">&quot;Tom Hanks&quot;</span><span class="err">}</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">[</span><span class="n">r</span><span class="p">:</span><span class="n">ACTED_IN</span><span class="o">|</span><span class="n">DIRECTED</span><span class="p">]</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">(</span><span class="n">m</span><span class="p">:</span><span class="n">Movie</span><span class="p">)</span><span class="w"></span>
<span class="k">RETURN</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="n">r</span><span class="p">,</span><span class="n">m</span><span class="w"></span>
<span class="k">RETURN</span><span class="w"> </span><span class="n">p</span><span class="p">.</span><span class="n">name</span><span class="p">,</span><span class="w"> </span><span class="k">type</span><span class="p">(</span><span class="n">r</span><span class="p">),</span><span class="w"> </span><span class="n">m</span><span class="p">.</span><span class="n">title</span><span class="w"></span>
</pre></div>
<p>前者返回Graph，后者返回表格数据</p><h3>Update</h3>
<blockquote>
<p>P.S. <code>where</code>是对属性做限制，所以查询条件既可以写在属性里，也可以用<code>where</code>语句来做过滤.</p></blockquote>
<p>要对查询结果进行修改，用<code>set</code>（有则改，无则加）</p>
<pre><code>MATCH
(:Person {name: &quot;张三&quot;}) - [:LOVES] -&gt; (p:Person)
RETURN
p

MATCH
(p1:Person) - [:LOVES] -&gt; (p2:Person)
WHERE
p1.name = &quot;张三&quot;
SET
p2.age = 33  # set by property
# or
p2 += {age: 33, height: 180}  # set by JSON
RETURN
p2
</code></pre>
<p>可以把<code>Neo4j</code>理解为命名实体识别(<code>NER</code>)，即你创造一句话，为句子里的每个实体打上标签，然后你想要谁就用实体标签把它取出来。</p><p>比如“张三爱李四”：</p>
<pre><code>step1: 写框架
CREATE () - [] -&gt; ()
step2: 填节点和关联
CREATE (:Person {name: &quot;张三&quot;}) - [:LOVES] -&gt; (:Person {name: &quot;李四&quot;})
</code></pre>
<p>而你要问张三爱谁:</p>
<pre><code>MATCH
(:Person {name: &quot;张三&quot;}) - [:LOVES] -&gt; (p:Person)
RETURN
p
</code></pre>
<p>看到查询语句了吗？除了<code>CREATE</code>, <code>MATCH</code>等关键词，句子顺序是完全一样的，也就是说，一直是在“<strong>陈述</strong>”一件事。</p><p>而事实上，这个<code>NER</code>在知识图谱中表示为<code>RDF</code>。</p><h3>Delete</h3>
<div class="highlight"><pre><span></span><span class="k">match</span><span class="w"> </span><span class="n">p</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="k">where</span><span class="w"> </span><span class="n">n</span><span class="p">.</span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;小明&#39;</span><span class="w"> </span><span class="n">detach</span><span class="w"> </span><span class="k">delete</span><span class="w"> </span><span class="n">n</span><span class="w"></span>
</pre></div>
<h2>Query</h2>
<h3>最短距离</h3>
<div class="highlight"><pre><span></span><span class="k">MATCH</span><span class="w"></span>
<span class="w">  </span><span class="p">(</span><span class="n">martin</span><span class="p">:</span><span class="n">Person</span><span class="w"> </span><span class="err">{</span><span class="n">name</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;Martin Sheen&#39;</span><span class="err">}</span><span class="p">),</span><span class="w"></span>
<span class="w">  </span><span class="p">(</span><span class="n">oliver</span><span class="p">:</span><span class="n">Person</span><span class="w"> </span><span class="err">{</span><span class="n">name</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;Oliver Stone&#39;</span><span class="err">}</span><span class="p">),</span><span class="w"></span>
<span class="w">  </span><span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">shortestPath</span><span class="p">((</span><span class="n">martin</span><span class="p">)</span><span class="o">-</span><span class="p">[</span><span class="o">*</span><span class="p">..</span><span class="mi">15</span><span class="p">]</span><span class="o">-</span><span class="p">(</span><span class="n">oliver</span><span class="p">))</span><span class="w"> </span><span class="o">#</span><span class="w"> </span><span class="mi">1</span><span class="w"></span>
<span class="w">  </span><span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">allShortestPath</span><span class="p">(....)</span><span class="w"> </span><span class="o">#</span><span class="w"> </span><span class="mi">2</span><span class="w"></span>
<span class="w">  </span><span class="k">WHERE</span><span class="w"> </span><span class="k">none</span><span class="p">(</span><span class="n">r</span><span class="w"> </span><span class="k">IN</span><span class="w"> </span><span class="n">relationships</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="w"> </span><span class="k">WHERE</span><span class="w"> </span><span class="k">type</span><span class="p">(</span><span class="n">r</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;FATHER&#39;</span><span class="p">)</span><span class="w"> </span><span class="o">#</span><span class="w"> </span><span class="mi">3</span><span class="w"></span>
<span class="k">RETURN</span><span class="w"> </span><span class="n">p</span><span class="w"></span>
</pre></div>
<ol>
<li>限定了15层</li>
<li>Finds <code>all</code> the shortest paths between two nodes.</li>
<li>排除了关系<code>type</code>为<strong>FATHER</strong>的</li>
</ol>
<p>给你们看一下一个<code>relationships</code>长啥样：</p><div class="highlight"><pre><span></span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="nt">&quot;identity&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">36629</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="nt">&quot;start&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">31343</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="nt">&quot;end&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">33922</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;author-&gt;title&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="nt">&quot;properties&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;author-&gt;title&quot;</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
<h3>模糊匹配(%)</h3>
<div class="highlight"><pre><span></span><span class="k">match</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">-</span><span class="p">[</span><span class="o">*</span><span class="mi">1</span><span class="p">..</span><span class="mi">2</span><span class="p">]</span><span class="o">-&gt;</span><span class="p">(</span><span class="n">b</span><span class="p">:</span><span class="n">content</span><span class="p">)</span><span class="w"> </span><span class="k">where</span><span class="w"> </span><span class="n">a</span><span class="p">.</span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;李白&#39;</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">b</span><span class="p">.</span><span class="n">name</span><span class="w"> </span><span class="o">=~</span><span class="w"> </span><span class="s1">&#39;.*明月.*&#39;</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="w"></span>
</pre></div>
<p><code>.*</code>就相当于sql里的<code>%</code>吧</p><h3>get by id</h3>

<pre><code>MATCH (n)
WHERE id(n) IN [0, 3, 5]
RETURN n
</code></pre>
<ol>
<li>id(n) -&gt; search with id</li>
<li>multiple id use <code>in</code></li>
</ol>
<h3>outer join (optional relationships)</h3>
<div class="highlight"><pre><span></span><span class="k">match</span><span class="w"> </span><span class="p">(</span><span class="n">a</span><span class="p">:</span><span class="n">author</span><span class="w"> </span><span class="err">{</span><span class="n">name</span><span class="p">:</span><span class="s1">&#39;李白&#39;</span><span class="err">}</span><span class="p">)</span><span class="w"></span>
<span class="n">optional</span><span class="w"> </span><span class="k">match</span><span class="w"> </span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="c1">--&gt;(b)</span>
<span class="k">return</span><span class="w"> </span><span class="n">b</span><span class="w"></span>
</pre></div>
<p>在实例中，b包含了两种实例：</p><ol>
<li>title</li>
<li>introduce</li>
</ol>
<p>等同于sql中user表outer join了两个表(title, introduce)</p></div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/%E6%88%91%E7%9A%84%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/HMM%E3%80%81NER%E3%80%81PoS%E3%80%81Viterbi%E7%AC%94%E8%AE%B0/" target="_self">HMM、NER、PoS、Viterbi笔记</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/HMM%E3%80%81NER%E3%80%81PoS%E3%80%81Viterbi%E7%AC%94%E8%AE%B0/" target="_self">
                <time class="text-uppercase">
                    June 14 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><p>开局一句话，隐马尔可夫，就是在“溯源”，即产生你这个现象的源头在哪。</p><ul>
<li>比如你掷出的这个显示为6的骰子，是来自于六面体的还是四面体的，或是来自于普通的还是灌铅了的</li>
<li>又比如你一句话里的某一个词，它是处于开始位置还是中间位置，或是它是一个人名还是一个地点或是一个介词</li>
</ul>
<p>任何一种表现形式，都有一个它的“原因”或“属性”。 现在正式开始，来自我能理解的网络资料，我的课程，以及一些思考</p><p>首先几个基础概念：</p><h1>命名实体识别(NER)</h1>
<p><strong>实体</strong>：人物(PER)，地点(LOC)，等
<strong>BIOES</strong>: 开始(Begin)， 中间(Inner)， 结尾(E)，单个(Single)，其它(Other)</p><p>比如人名：张北京，就可以被识别为$\Rightarrow$ B-PER, I-PER, E-PER</p><h1>Part-of-Speech Tagging（词性标注）</h1>
<p>词性标注是为输入文本中的每个词性标注词分配词性标记的过程。标记算法的输入是一系列(标记化的)单词和标记集，输出是一系列标记，每个标记一个。</p><p>标记是一项消除歧义的任务;单词是模糊的，有不止一个可能的词性(歧义)，我们的目标是为这种情况找到正确的标签。例如，book可以是动词(book that flight)，也可以是名词(hand me that book)。That可以是一个限定词(Does that flight serve dinner)，也可以是一个补语连词(I thought that your flight was earlier)。后置标记的目标是解决这些分辨率模糊，为上下文选择合适的标记</p><h1>Sequence model</h1>
<p>Sequence models are central to NLP: they are models where there is some sort of <code>dependence through time</code> between your inputs.</p><ul>
<li>The classical example of a sequence model is the <code>Hidden Markov Model</code> for <strong>part-of-speech tagging</strong>. (词性标注)</li>
<li>Another example is the <code>conditional random field</code>.</li>
</ul>
<p>HMM模型的典型应用是词性标注</p><figure  style="flex: 94.51219512195122" ><img width="1240" height="656" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/8246ff19ee962171c5d3b15abd234eec.png" alt=""/></figure><p>词性标注语料库是统计标注算法的关键训练(和测试)集。三个主要的标注语料库始终用于训练和测试英语词性标注器。</p><ol>
<li>布朗语料库是1961年在美国出版的500篇不同体裁的书面文本的100万单词样本。</li>
<li>《华尔街日报》语料库收录了1989年发表在《华尔街日报》上的100万个单词。</li>
<li>总机语料库由1990-1991年收集的200万字电话对话组成。语料库的创建是通过在文本上运行一个自动的词性标记，然后由人工注释器手工更正每个标记。</li>
</ol>
<h1>HMM</h1>
<p>HMM是一个序列模型(<code>sequence model</code>)。序列模型或序列分类器是一个模型，其工作是为序列中的每个单元分配一个标签或类，从而将一个观察序列(观察状态)映射到一个标签序列(隐藏状态)。HMM是一种概率序列模型：给定一个单位序列(单词、字母、语素、句子等等)，它计算可能的标签序列的概率分布，并选择最佳标签序列。</p><ul>
<li>3个骰子，6面体，4面体，8面体(D6, D4, D8)</li>
<li>每次随机选出一个骰子投掷，得到一个数字</li>
<li>共十次，得到10个数字</li>
</ul>
<ol>
<li><code>可见状态链</code>：10次投掷得到10个数字(1,3,5...)$\Rightarrow$对应你看得的10个单词</li>
<li><code>隐含状态链</code>：每一次投掷都有可能拿到三种骰子之一，(D6, D6, D4...) $\Rightarrow$对应为每个单词的词性</li>
<li>转换概率（<code>transition probability</code>）：隐含状态之间的概率($\Rightarrow$对应为语法)：<ul>
<li>每一次拿到某种骰子之后，下一次拿到三种骰子的概率（[1/3,1/3,1/3],...)</li>
<li>或者说主动决策下一次用哪个骰子的概率[a,b,c...] (相加为1)</li>
</ul>
</li>
<li>可见状态之间没有转换概率</li>
<li>输出概率（<code>emission probability</code>）：隐含状态和可见状态之间的概率，比如D4下1的概率为1/4，D6下为1/6 (表现概率，激发概率，多种翻译)</li>
</ol>
<figure  style="flex: 106.16438356164383" ><img width="1240" height="584" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/1c9b14ebecf2d171b7f511296c425412.png" alt=""/></figure><p>应用HMM模型时候，往往是缺失了一部分信息的，</p><ul>
<li>有时候你知道骰子有几种，每种骰子是什么，但是不知道掷出来的骰子序列；</li>
<li>有时候你只是看到了很多次掷骰子的结果，剩下的什么都不知道。</li>
</ul>
<p>如何应用算法去估计这些缺失的信息，就成了一个很重要的问题，这也是HMM模型能做的几件事：</p><h2>Decoding</h2>
<p>解码的过程就是在给出一串序列和已知HMM模型的情况下，找到最可能的隐性状态序列。</p><p>比如结果是：1 6 3 5 2 7 3 5 2 4, 求最可能的骰子序列</p><h3>Viterbi algorithm</h3>
<ol>
<li>掷出1的最大概率是4面体： P1(D4) = P(1|D4) * P(D4) = 1/4 * 1/3</li>
<li>掷出6的最大概率是 P2(D6) = P(6|D6) * P(D6) = 1/6 * 1/3</li>
<li>连续1，6的概率就成了1的概率 * 2的概率 P2(D6) = P1(D4) * P2(D6) = 1/216</li>
<li>1,6,3 =&gt; P3(D4) = P2(D6) * P(3|D4) * P(D4) = $\frac{1}{216} \cdot \frac{1}{3} \cdot \frac{1}{4}$</li>
<li>and so on</li>
<li>但这个例子忽略了转移概率，即P(D6|D4), P(D4|D6,D4)，或者说默认了转移概率就是1/3，即每次挑中三个骰子的机率均等。</li>
</ol>
<h2>Evaluation</h2>
<p>根据条件和序列结果求这一序列的概率是多少，比如三种骰子，投出了1，6，3的结果：</p><figure  style="flex: 169.86301369863014" ><img width="1240" height="365" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/3008ce6fdd893286e56d1b7f9ad1a342.png" alt=""/></figure><ul>
<li>第1列表示第一次投掷得到1的可能性和为0.18</li>
<li>第2列为1 6的的可能性和为0.05</li>
<li>第3列为1 6 3的可能性和为0.03</li>
</ul>
<p>如果远低于或远高于这个概率，必然有做过手脚的骰子。</p><h2>转移概率的矩阵表示</h2>
<p>这次假定不同的骰子是用来作弊的，作弊者会根据情况来挑选骰子，这样转移概率就不可能是均等的了：</p><figure  style="flex: 83.33333333333333" ><img width="500" height="300" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/8259518e5781e3ce798778e8da69de85.png" alt=""/></figure><p>很幸运，这么复杂的概率转移图，竟然能用矩阵表达：</p><p>$$A = 
\begin{bmatrix}
0.15 &amp; 0.45 &amp; 0.4 \\
0.25 &amp; 0.35 &amp; 0.4 \\
0.10 &amp; 0.55 &amp; 0.35
\end{bmatrix}
$$</p>
<p>既然是3行3列，显然$A_{ij}$就是从i切换到j的概率，比如$A_{12}$ 就应该是这个人把骰子从作弊骰子1切换到2的概率。</p><figure  style="flex: 102.04081632653062" ><img width="500" height="245" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/653c3cb7f8c2f3a541d170421fe489bf.png" alt=""/></figure><p>相应地，发射概率（即不同骰子摇出的点数的概率）也能表示为矩阵：</p><p>$$B = 
\begin{bmatrix}
0.16 &amp; 0.16 &amp; 0.16 &amp; 0.16 &amp; 0.16 &amp; 0.16 \\
0.02 &amp; 0.02 &amp; 0.02 &amp; 0.02 &amp; 0.02 &amp; 0.90 \\
0.40 &amp; 0.20 &amp; 0.25 &amp; 0.05 &amp; 0.05 &amp; 0.05 \\
\end{bmatrix}
$$</p>
<p>现在有了转移概率和发射概率，我们再来看看前面的掷出1，6，3的骰子的概率：
骰子设为D1 D2 D3, 每一轮的可能性为P1 P2 P3, 则P = P3D1 + P3D2 + P3D3 即第3轮时3种骰子能投出3的概率和</p><p>我来推导一下P3D1怎么来的，上面的表格是我从别人的博客里复制的，这里就不做一个一模一样的图了，我们一步步来吧：</p><ul>
<li>第一次投掷每个骰子的概率应该是隐含了各为1/3吧？(这个好像叫&quot;<code>初始隐状态</code>&quot; $\pi$)</li>
<li>P1D1 = 0.16 * 0.33, 即1/3概率拿到D1，0.16概率投出1，同理：<ul>
<li>P1D2 = 0.02 * 0.33</li>
<li>P1D3 = 0.40 * 0.33</li>
</ul>
</li>
<li>P2D1 =<ul>
<li>P1D1 * $A_{00}$ * $B_{05}$ = P1D1 * 0.15 * 0.16 即P1D1前提下，乘上D1换到D1的概率，再乘上D1选出6的概率</li>
<li>$+$</li>
<li>P1D2 * $A_{10}$ * $B_{05}$ = P1D1 * 0.25 * 0.16 即P1D2前提下，乘上D2换到D1的概率，再乘上D1选出6的概率</li>
<li>$+$</li>
<li>P1D3 * $A_{20}$ * $B_{05}$ = P1D1 * 0.10 * 0.16 即P1D3前提下，乘上D3换到D1的概率，再乘上D1选出6的概率</li>
<li>以此类推得到P2D2, P2D3</li>
</ul>
</li>
<li>P3D2 = （<em>D1的概率太平均，这次换个D2来演示</em>）<ul>
<li>P2D1 * $A_{01}$ * $B_{12}$ = P2D1 * 0.45 * 0.02 即P2D1前提下，乘上D1换到D2的概率，再乘上D2选出3的概率</li>
<li>$+$</li>
<li>P2D2 * $A_{11}$ * $B_{12}$ = P2D1 * 0.35 * 0.02 即P2D2前提下，乘上D2换到D2的概率，再乘上D2选出3的概率</li>
<li>$+$</li>
<li>P2D3 * $A_{21}$ * $B_{12}$ = P2D1 * 0.35 * 0.02 即P2D3前提下，乘上D3换到D2的概率，再乘上D2选出3的概率</li>
<li>以此类推得到P3D1, P3D2</li>
</ul>
</li>
<li>P = P3D1 + P3D2 + P3D3</li>
</ul>
<p>$$
\sum_{r\in R}\prod_t^TP(v(t)|w_r(t)) | w_r(t-1))
$$</p>
<ul>
<li>v: visible 可见序列</li>
<li>w: 隐性状态序列</li>
<li>R: 所有隐状态的可能性</li>
</ul>
<ol>
<li>t-1隐状态前提下得到t的概率（转移概率）如D2换到D3的概率</li>
<li>上一概率前提下得到v(t)的概率，如D3扔出1的概率</li>
<li>一种隐状态下出序列的结果为累乘</li>
<li>所有隐状态下出该序列的结果为3的累加</li>
</ol>
<p>简单来说：</p><ol>
<li>可见序列$v(t)$的概率依赖当前$t$下的隐状态（比如是不是作弊了的骰子）$w_r(t)$<ul>
<li>得到：$P(v(t)\ \color{red}|\ w_r(t))$</li>
</ul>
</li>
<li>当前隐状态$w_r(t)$又有两个特征:<ol>
<li>由$w_r(t-1)$转换而来的: $P(v(t)|w_r(t))\color{red}{|}w_r(t-1)$</li>
<li>$T$是链式的，概率累乘： $\color{red}{\prod_t^T}P(v(t)|w_r(t)) | w_r(t-1))$</li>
</ol>
</li>
<li>最后一步时的隐状态显然是几种之一，累加起来就是所有可能性：<ul>
<li>$\color{red}{\sum_{r\in R}}\prod_t^TP(v(t)|w_r(t)) | w_r(t-1))$</li>
</ul>
</li>
</ol>
<h1>应用</h1>
<ol>
<li>初始概率</li>
</ol>
<p>以<code>BMES</code>为例（参考NER），把其认为是隐状态，然后认为每个词（里的字）是由隐状态产生的。</p><p>即<code>B</code>对应的字可能有“<code>中</code>”，“<code>国</code>”，等等，能作为词语打头的字都可能由隐状态<code>B</code>产生，其它状态依次类推。</p><p>就像我们三种骰子的初始概率，完全取决于每种骰子占总数的多少一样，HHM应用到语言模型里，初始概率就是先把文字全部用<code>BMES</code>表示，然后分别数出个数，与总数做个对比。（此时已经可以判断出<code>M</code>和<code>E</code>的概率只能是0了。</p><ol start="2">
<li>转移概率</li>
</ol>
<p>应该是4个循环吧，每次把当前状态后面跟上四个状态的情况都数出来，就是一个隐状态到其它四个状态的转移概率，四行拼到一起就是一个转移概率的矩阵，类似上面的三种骰子互相切换的矩阵。</p><p>也可以用字典，比如 BE BS BB BM等共16个键，两两遍历整个字符串完后，16个count就出来了，group后就能得到概率了。</p><ol start="3">
<li>观测概率（发射概率）</li>
</ol>
<p>这个就是每一个隐状态下对应不同表面文字的概率了，比如：{s:{&quot;周&quot;: 0.3357, &quot;爬&quot;:0.00003}...}</p><p>要知道，三种概率里面是有很多0的，意思就是在现有的语法体系里面不可能出现的场景，比如第一个字不可能是M和E，B后面不可能跟S，B，而M后面不可能跟B，S，以及S后面不可能跟M，E等，再比如假如哪个字永远不可能是第一个字，那么它的观测概率在S里面就永远是0，等等。</p><p>这里要计算的话，因为隐状态是用文字推断出来的，所以这个映射关系还在，那么整理一下两个数组就能把每个隐状态能对应的文字全部映射上了。</p><hr />
<p>以下是我课程里的笔记，理解了上面的内容，理解下面是没有任何障碍的。</p><h1>viterbi in NLP</h1>
<p>$\overbrace{
  0
  \xrightarrow[农]{2.5}
  1
  \xrightarrow[产]{4.0}
  2
}^{1.4}
\xrightarrow[物]{2.3}
3$</p><p>$0
\xrightarrow[农]{2.5}
\underbrace{
  1
  \xrightarrow[产]{4.0}
  2
  \xrightarrow[物]{2.3}
  3
}_{2.1}$</p><blockquote>
<p>数字画圈的写法 $\enclose{circle}{3}$ 这个生成器暂不支持</p></blockquote>
<ul>
<li>node: $\enclose{circle}{2}$ ，圆圈，就是位置索引</li>
<li>edge: 词， 箭头，很好理解：string[0,1] = '农'</li>
<li>Each edge weight is a <code>negative log probality</code><ul>
<li>-log(P(农)) = 2.5</li>
<li>-log(P(产)) = 4.0</li>
<li>-log(P(农产)) = 1.4</li>
<li>-log(P(产物)) = 2.1</li>
</ul>
</li>
<li>Each path is a segmentation for the sentence</li>
<li>Each path weight is a sentence <code>unigram</code> negative log probability<ul>
<li>-log(P(农产)) + -log(P(物)) = 1.4 + 2.3 = 3.7</li>
<li>农 + 产 + 物 = 2.5 + 4.0 + 2.3 = 8.8</li>
<li>农 + 产物 = 2.5 + 2.1 = 4.6</li>
</ul>
</li>
</ul>
<h2>two step</h2>
<p>1.前向，从左往右，找到<strong>最佳路径</strong>的分数
2.后向，从右往左，创建一条最佳路径</p><h3>forward algorithm</h3>
<p>pseudo code</p><div class="highlight"><pre><span></span><span class="n">best_score</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>
<span class="k">for</span> <span class="n">each</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">graph</span> <span class="p">(</span><span class="n">ascending</span> <span class="n">order</span><span class="p">)</span>
  <span class="n">best_score</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="err">∞</span>
  <span class="k">for</span> <span class="n">each</span> <span class="n">incoming</span> <span class="n">edge</span> <span class="n">of</span> <span class="n">node</span>
    <span class="n">score</span><span class="o">=</span><span class="n">best_score</span><span class="p">[</span><span class="n">edgeprev_node</span><span class="p">]</span><span class="o">+</span><span class="n">edge</span><span class="o">.</span><span class="n">score</span>
    <span class="k">if</span> <span class="n">score</span> <span class="o">&lt;</span> <span class="n">best_score</span><span class="p">[</span><span class="n">node</span><span class="p">]</span>
      <span class="n">best_score</span><span class="p">[</span><span class="n">node</span><span class="p">]</span><span class="o">=</span><span class="n">score</span>
      <span class="n">best_edge</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span><span class="n">edge</span>
</pre></div>
<p>example:
<figure  style="flex: 82.65765765765765" ><img width="734" height="444" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/5aa5426eb70b4c6cd0b8c4b1dacda749.png" alt=""/></figure></p><ul>
<li>初始节点打分0，其它节点打分为$\infty$</li>
<li>每个节点打分由其(<code>incoming edge</code>)(即来源箭头)和来源节点的打分构成</li>
<li>如果有多个来源，则计算出该来源的得分，与该节点当前的得分做对比，取得分低的那个</li>
<li>把该节点的分值和来源edge存到该节点上（edge就是词）。</li>
</ul>
<ol>
<li>简单来说，还是和之前的骰子一样，每一次算出到当前节点的最低分数的路径。</li>
<li>上图中，我们就把e1, e2, e5选出来了，这个过程中，删除了e3, e4这几条路径</li>
<li>best_score=(0.0, 2.5, 1.4, 3.7), best_edge = (NULL, e1, e2, e5)</li>
<li>用字典来把Node映射上去：{0:(0.0, NULL), 1:(2.5, e1), 2:(1.4, e2), 3:(3.7, e5)}</li>
</ol>
<h3>backward algorithm</h3>
<div class="highlight"><pre><span></span><span class="n">best_path</span><span class="o">=</span><span class="p">[]</span>
<span class="n">next_edge</span><span class="o">=</span><span class="n">best_edge</span><span class="p">[</span><span class="n">best_edge</span><span class="o">.</span><span class="n">length</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="k">while</span> <span class="n">next_edge</span> <span class="o">!=</span> <span class="n">NULL</span>
  <span class="n">add</span> <span class="n">next_edge</span> <span class="n">to</span> <span class="n">best_path</span>
  <span class="n">next_edge</span> <span class="o">=</span><span class="n">best_edge</span><span class="p">[</span><span class="n">next_edge</span><span class="o">.</span><span class="n">prev_node</span><span class="p">]</span>
<span class="n">reverse</span> <span class="n">best</span> <span class="n">path</span>
</pre></div>
<p>举例：
<figure  style="flex: 102.56410256410257" ><img width="800" height="390" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/c97b27230bd42318efb72375c245c50b.png" alt=""/></figure></p><ul>
<li>从图片可知，<code>path</code>就是<code>edge</code></li>
<li>初始path是空，[]</li>
<li>从<code>forward</code>的结果字典里找到node 3的best_edge，就是e5 [e5]</li>
<li>e5的来源的是node 2</li>
<li>从字典里找到2的best_edge，是e2 [e5, e2]</li>
<li>e2的来源是node 0</li>
<li>0的best_edge是NULL，结束递归</li>
<li>reverse: [e2, e5]</li>
</ul>
<figure  style="flex: 53.25581395348837" ><img width="458" height="430" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/c7650195782b845d9be6e36dac55f277.png" alt=""/></figure><p>这个很好理解</p><ol>
<li>0到农，到农产，到农产物的概率，表示为0.0+ -log(p(农/农产/农产物))</li>
<li>在农的前提下，就有农到产，和农到产物：best(1) + -log(P(产/产物))</li>
<li>在产的前提下，就只有best(2) + -log(P(物))了</li>
</ol>
<p>应用到NLP：</p><figure  style="flex: 73.99193548387096" ><img width="734" height="496" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/5e643e29e4fe62ef780ba545dc3a04fb.png" alt=""/></figure><p>这里就是把node, egde具体了一下：</p><ol>
<li>多包了一层for-each，意思是前面的代码是处理一行的</li>
<li>node对应是单词结尾(word_end)，其实就是一个index，前面说过了</li>
<li>edge对应是单词(word)，前面也说过了，即<code>string[5,7]</code>的意思</li>
<li>score由uni-gram来计算</li>
<li>计算上，就是找到以基准字当作单词结尾，然后前面的字跟它拼起来的所有可能性，找最低分：<ul>
<li>比如abcdefg, 如果当前是e，那么分别比较：abced, bcde, cde, de</li>
</ul>
</li>
<li>接上例，输出结果应该这么解读：<ul>
<li>以b为结尾的单词，最有可能的是xxx, 它的得分是，它的索引是，</li>
<li>以c为结尾的单词，最有可能是bc或是abc，它的得分是，bc/abc的索引是(1,2)，这样</li>
</ul>
</li>
</ol>
<figure  style="flex: 90.2439024390244" ><img width="592" height="328" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/9de76a49c53e036f86c0c2e7a950c8cd.png" alt=""/></figure><ol>
<li>显然这里已经知道edge不知道是一个词，而且是一个词的首尾边界</li>
<li>也知道存到best_edges里面的其实就是词的位置索引</li>
<li>反向的时候，从最后一个索引找到得分最低的词，再从这个单词向前找，一直找到<ul>
<li>所以next_edge[0]其实就是当前单词词首，[1]就是词尾</li>
<li>所以把当前单词存进去后，向前搜索就要以next_edge[0]为字典，找对应的best_edge</li>
<li>再从best_edge里面解析出最合适的单词的首尾索引，存到结果数组里</li>
</ul>
</li>
</ol>
</div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/HMM%E3%80%81NER%E3%80%81PoS%E3%80%81Viterbi%E7%AC%94%E8%AE%B0/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
</section>

<div class="container">
    <section id="prism__page__pagination" class="prism-pagination" class="col-md-8 offset-md-2">
        <ul>
            
            <li class="next">
                <a class="no-link" href="/page/2/" target="_self"><i class="fa fa-chevron-left" aria-hidden="true"></i>Newer</a>
            </li>
            
            
            <li class="prev">
                <a class="no-link" href="/page/4/" target="_self">Older<i class="fa fa-chevron-right" aria-hidden="true"></i></a>
            </li>
            
        </ul>
    </section>
</div>


</main>

            <footer id="prism__footer">
                <section>
                    <div>
                        <nav class="social-links">
                            <ul><li><a class="no-link" title="Twitter" href="https://twitter.com/walkerwzy" target="_blank" rel="noopener noreferrer nofollow"><i class="gi gi-twitter"></i></a></li><li><a class="no-link" title="GitHub" href="https://github.com/walkerwzy" target="_blank" rel="noopener noreferrer nofollow"><i class="gi gi-github"></i></a></li><li><a class="no-link" title="Weibo" href="https://weibo.com/1071696872" target="_blank" rel="noopener noreferrer nofollow"><i class="gi gi-weibo"></i></a></li></ul>
                        </nav>
                    </div>

                    <section id="prism__external_links">
                        <ul>
                            
                            <li>
                                <a class="no-link" target="_blank" href="https://github.com/AlanDecode/Maverick" rel="noopener noreferrer nofollow">Maverick</a>：🏄‍ Go My Own Way.
                                <span>|</span>
                            </li>
                            
                            <li>
                                <a class="no-link" target="_blank" href="https://www.imalan.cn" rel="noopener noreferrer nofollow">Triple NULL</a>：Home page for AlanDecode.
                                <span>|</span>
                            </li>
                            
                        </ul>
                    </section>

                    <div class="copyright">
                        <p class="copyright-text">
                            <span class="brand">walker's code blog</span>
                            <span>Copyright © 2022 AlanDecode</span>
                        </p>
                        <p class="copyright-text powered-by">
                            | Powered by <a href="https://github.com/AlanDecode/Maverick" class="no-link" target="_blank" rel="noopener noreferrer nofollow">Maverick</a> | Theme <a href="https://github.com/Reedo0910/Maverick-Theme-Prism" target="_blank" class="no-link" rel="noopener noreferrer nofollow">Prism</a>
                        </p>
                    </div>
                    <div class="footer-addon">
                        
                    </div>
                </section>
                <script>
                    var site_build_date = "2019-12-06T12:00+08:00"

                </script>
                <script src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/prism-efa8685153.js"></script>
            </footer>
        </div>
    </div>
    </div>

    <script src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/ExSearch/jquery.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/ExSearch/ExSearch-493cb9cd89.js"></script>

    <!--katex-->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/katex.min.js"></script>
    <script>
        mathOpts = {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "\\[", right: "\\]", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false }
            ]
        };

    </script>
    <script defer src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/auto-render.min.js" onload="renderMathInElement(document.body, mathOpts);"></script>

    
</body>

</html>