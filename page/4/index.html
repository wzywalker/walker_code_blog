<!DOCTYPE HTML>
<html lang="english">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="renderer" content="webkit">
    <meta name="HandheldFriendly" content="true">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Maverick,AlanDecode,Galileo,blog" />
    <meta name="generator" content="Maverick 1.2.1" />
    <meta name="template" content="Prism" />
    <link rel="alternate" type="application/rss+xml" title="walker's code blog &raquo; RSS 2.0" href="/feed/index.xml" />
    <link rel="alternate" type="application/atom+xml" title="walker's code blog &raquo; ATOM 1.0" href="/feed/atom/index.xml" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/prism-b9d78ff38a.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/ExSearch/ExSearch-182e5a8869.css">
    <link href="https://fonts.googleapis.com/css?family=Fira+Code&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css">
    <script>
        var ExSearchConfig = {
            root: "",
            api: "https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/e83a979bdfe76b3eb88998ea61cf43ad.json"
        }

    </script>
    
<title>walker's code blog</title>
<meta name="author" content="AlanDecode" />
<meta name="description" content="coder, reader" />
<meta property="og:title" content="walker's code blog" />
<meta property="og:description" content="coder, reader" />
<meta property="og:site_name" content="walker's code blog" />
<meta property="og:type" content="website" />
<meta property="og:url" content="/page/4/" />
<meta property="og:image" content="walker's code blog" />
<meta name="twitter:title" content="walker's code blog" />
<meta name="twitter:description" content="coder, reader" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/android-chrome-512x512.png" />


    
</head>

<body>
    <div class="container prism-container">
        <header class="prism-header" id="prism__header">
            <h1 class="text-uppercase brand"><a class="no-link" href="/" target="_self">walker's code blog</a></h1>
            <p>coder, reader</p>
            <nav class="prism-nav"><ul><li><a class="no-link text-uppercase " href="/" target="_self">Home</a></li><li><a class="no-link text-uppercase " href="/archives/" target="_self">Archives</a></li><li><a class="no-link text-uppercase " href="/about/" target="_self">About</a></li><li><a href="#" target="_self" class="search-form-input no-link text-uppercase">Search</a></li></ul></nav>
        </header>
        <div class="prism-wrapper" id="prism__wrapper">
            
<main>    
    

<section id="prism__post-list" class="prism-section row">
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/Deep-Learning-with-Python-Notes-3/" target="_self">《Deep Learning with Python》笔记[3]</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/Deep-Learning-with-Python-Notes-3/" target="_self">
                <time class="text-uppercase">
                    September 18 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><h1>Fundamentals of machine learning</h1>
<ul>
<li>Supervised learning<ul>
<li>binary classification</li>
<li>multiclass classificaiton</li>
<li>scalar regression</li>
<li>vector regression（比如bounding-box)</li>
<li>Sequence generation (摘要，翻译...)</li>
<li>Syntax tree prediction</li>
<li>Object detection (一般bounding-box的坐标仍然是回归出来的)</li>
<li>Image segmentation</li>
</ul>
</li>
<li>Unsupervised learing<ul>
<li>是数据分析的基础，在监督学习前也常常需要用无监督学习来更好地“理解”数据集</li>
<li>主要有降维(<code>Dimensionality reduction</code>)和聚类(<code>clustering</code>)</li>
</ul>
</li>
<li>Self-supervised learning<ul>
<li>其实还是监督学习，因为它仍需要与某个target做比较</li>
<li>往往半监督（自监督）学习仍然有小量有标签数据集，在此基础上训练的不完善的model用来对无标签的数据进行打标，循环中对无标签数据打标的可靠度就越来越高，这样总体数据集的可靠度也越来越高了。有点像生成对抗网络里生成器和辨别器一同在训练过程中完善。</li>
<li><code>autoencoders</code></li>
</ul>
</li>
<li>Reinforcement learning<ul>
<li>an <code>agent</code> receives information about its <code>environment</code> and learns to choose <code>actions</code> that will maximize some <code>reward</code>.</li>
<li>可以用训练狗来理解</li>
<li>工业界的应用除了游戏就是机器人了</li>
</ul>
</li>
</ul>
<h2>Data preprocessing</h2>
<ul>
<li>vectorization</li>
<li>normalization (small, homogenous)</li>
<li>handling missing values<ol>
<li>除非0有特别的含义，不然一般可以对缺失值补0</li>
<li>你不能保证测试集没有缺失值，如果训练集没看到过缺失值，那么将不会学到忽略缺失值<ul>
<li><em>复制</em>一些训练数据并且随机drop掉一些特征</li>
</ul>
</li>
</ol>
</li>
<li>feature extraction<ul>
<li>making a problem easier by expressing it in a simpler way. It usually requires understanding the problem <strong>in depth</strong>.</li>
<li><strong>Before</strong> deep learning, feature engineering used to be <code>critical</code>, because classical <strong>shallow algorithms</strong> didn’t have <code>hypothesis spaces</code> rich enough to learn useful features by themselves. (又见假设空间)</li>
<li>但是好的特征仍然能让你在处理问题上更优雅、更省资源，也能减小对数据集规模的依赖。</li>
</ul>
</li>
</ul>
<h2>Overfitting and underfitting</h2>
<ul>
<li>Machine learning is the tension between <code>optimization</code> and <code>generalization</code>.</li>
<li>optimization要求你在训练过的数据集上能达到最好的效果</li>
<li>generalization则希望你在没见过的数据上有好的效果</li>
<li>如果训练集上loss小，测试集上也小，说明还有优化(optimize)的余地 -&gt; <code>underfitting</code>看loss<ul>
<li>just keep training</li>
</ul>
</li>
<li>如果验证集上generalization stop improving(泛化不再进步，一般看衡量指标，比如准确率) -&gt; <code>overfitting</code></li>
</ul>
<p>解决overfitting的思路：</p><ul>
<li><strong>the best solution</strong> is get more trainging data</li>
<li><strong>the simple way</strong> is to reduce the size of the model<ul>
<li>模型容量(<code>capacity</code>)足够大，就足够容易<em>记住</em>input和target的映射，没推理什么事了</li>
</ul>
</li>
<li>add constraints -&gt; weight <code>regularization</code></li>
<li>add dropout</li>
</ul>
<h2>Regularization</h2>
<p><strong>Occam’s razor</strong></p><blockquote>
<p>given <em>two explanations</em> for something, the explanation most likely to be correct is the <strong>simplest one</strong>—the one that makes <strong>fewer assumptions</strong>.</p></blockquote>
<p>即为传说中<em>如无必要，勿增实体</em>的<code>奥卡姆剃刀原理</code>，这是在艺术创作领域的翻译，我们这里还是直译的好，即能解释一件事的各种理解中，越简单的，假设条件越少的，往往是最正确的，引申到机器学习，就是如何定义一个<code>simple model</code></p><p>A simple model in this context is:</p><ul>
<li>a model where the distribution of parameter values has <code>less entropy</code></li>
<li>or a model with fewer parameters</li>
</ul>
<p>实操就是，就是迫使选择那些值比较小的weights，which makes the distribution of weight values more regular. This is called weight <code>regularization</code>。这个解释是我目前看到的最<code>regularization</code>这个名字最好的解释，“正则化”三个字都认识，根本没人知道这三个字是什么意思，翻译了跟没番一样，而使分布更“常规化，正规化”，好像更有解释性。</p><p>别的教材里还会告诉你这里是对大的权重的<strong>惩罚</strong>（设计损失函数加上自身权重后，权重越大，loss也就越大，这就是对大权重的惩罚）</p><ul>
<li>L1 regularization—The cost added is proportional to the absolute value of the weight coefficients (the L1 norm of the weights).</li>
<li>L2 regularization—The cost added is proportional to the square of the value of the weight coefficients (the L2 norm of the weights).</li>
</ul>
<p>L2 regularization is also called <code>weight decay</code>in the context of neural networks. Don’t let the different name confuse you: weight decay is mathematically <strong>the same as</strong> L2 regularization.</p><blockquote>
<p>只需要在训练时添加正则化</p></blockquote>
<h2>Dropout</h2>
<p>randomly dropping out (setting to zero) a number of output features of the layer during training.</p><p>dropout的作者Geoff Hinton解释dropout的灵感来源于银行办事出纳的不停更换和移动的防欺诈机制，可能认为一次欺诈的成功实施需要员工的配合，所以就尽量降低这种配合的可能性。于是他为了防止神经元也能聚在一起”密谋”，尝试随机去掉一些神经元。以及对输出添加噪声，让模型更难记住某些patten。</p><h2>The universal workflow of machine learning</h2>
<ol>
<li>Defining the problem and assembling a dataset<ul>
<li>What will your input data be?</li>
<li>What are you trying to predict?</li>
<li>What type of problem are you facing?</li>
<li>You hypothesize that your outputs can be predicted given your inputs.</li>
<li>You hypothesize that your available data is sufficiently informative to learn the relationship between inputs and outputs.</li>
<li>Just because you’ve assembled exam- ples of inputs X and targets Y doesn’t mean X contains enough information to predict Y.</li>
</ul>
</li>
<li>Choosing a measure of success<ul>
<li>accuracy? Precision and recall? Customer-retention rate?</li>
<li>balanced-classification problems,<ul>
<li>accuracy and area under the <code>receiver operating characteristic curve</code> (ROC AUC)</li>
</ul>
</li>
<li>class-imbalanced problems<ul>
<li>precision and recall.</li>
</ul>
</li>
<li>ranking problems or multilabel classification<ul>
<li>mean average precision</li>
</ul>
</li>
<li>...</li>
</ul>
</li>
<li>Deciding on an evaluation protocol<ul>
<li>Maintaining a hold-out validation set—The way to go when you have plenty of data</li>
<li>Doing <code>K-fold</code> cross-validation—The right choice when you have too few samples for hold-out validation to be reliable</li>
<li>Doing <code>iterated K-fold</code> validation—For performing highly accurate model evaluation when <em>little data</em> is available</li>
</ul>
</li>
<li>Preparing your data<ul>
<li>tensor化，向量化，归一化等</li>
<li>may do some feature engineering</li>
</ul>
</li>
<li>Developing a model that does better than a baseline<ul>
<li>baseline:<ul>
<li>基本上是用纯随机(比如手写数字识别，随机猜测为10%)，和纯相关性推理（比如用前几天的温度预测今天的温度，因为温度变化是连续的），不用任何机器学习做出baseline</li>
</ul>
</li>
<li>model:<ul>
<li>Last-layer activation<ul>
<li>sigmoid, relu系列， 等等</li>
</ul>
</li>
<li>Loss function<ul>
<li>直接的预测值真值的差，如MSE</li>
<li>度量代理，如crossentropy是ROC AUC的proxy metric</li>
</ul>
</li>
</ul>
</li>
<li>Optimization configuration<ul>
<li>What optimizer will you use? What will its learning rate be? In most cases, it’s safe to go with rmsprop and its default learning rate.</li>
</ul>
</li>
<li>Scaling up: developing a model that overfits<ul>
<li>通过增加layers, 增加capacity，增加training epoch来加速overfitting，从而再通过减模型和加约束等优化</li>
</ul>
</li>
<li>Regularizing your model and tuning your hyperparameters<ul>
<li>Add dropout.</li>
<li>Try different architectures: add or remove layers.</li>
<li>Add L1 and/or L2 regularization.</li>
<li>Try different hyperparameters (such as the number of units per layer or the learning rate of the optimizer) to find the optimal configuration.</li>
<li>Optionally, iterate on feature engineering: add new features, or remove features that don’t seem to be informative.</li>
</ul>
</li>
</ul>
</li>
</ol>
<table>
<thead>
<tr>
  <th>Problem type</th>
  <th>Last-layer activation</th>
  <th>Loss function</th>
</tr>
</thead>
<tbody>
<tr>
  <td>Binary classification</td>
  <td>sigmoid</td>
  <td>binary_crossentropy</td>
</tr>
<tr>
  <td>Multiclass, single-label classification</td>
  <td>softmax</td>
  <td>categorical_crossentropy</td>
</tr>
<tr>
  <td>Multiclass, multilabel classification</td>
  <td>sigmoid</td>
  <td>binary_crossentropy</td>
</tr>
<tr>
  <td>Regression to arbitrary values</td>
  <td>None</td>
  <td>mse</td>
</tr>
<tr>
  <td>Regression to values between 0 and 1</td>
  <td>sigmoi</td>
  <td>mse or binary_crossentropy</td>
</tr>
</tbody>
</table>
</div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/Deep-Learning-with-Python-Notes-3/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/Deep-Learning-with-Python-Notes-2/" target="_self">《Deep Learning with Python》笔记[2]</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/Deep-Learning-with-Python-Notes-2/" target="_self">
                <time class="text-uppercase">
                    September 15 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><h1>Getting started with neural networks</h1>
<h2>Anatomy of a neural network</h2>
<ul>
<li><code>Layers</code>, which are combined into a <code>network</code> (or model)<ul>
<li>layers: 常见的比如卷积层，池化层，全连接层等</li>
<li>models: layers构成的网络，或多个layers构成的模块（用模块组成网络）<ul>
<li>Two-branch networks</li>
<li>Multihead networks</li>
<li>Inception blocks, residual blocks etc.</li>
</ul>
</li>
<li>The topology of a network defines a hypothesis space</li>
<li>本书反复强调的就是这个<code>hypothesis space</code>，一定要理解这个思维：<ul>
<li>By choosing a network topology, you <code>constrain</code> your space of possibilities (hypothesis space) to a specific series of tensor operations, mapping input data to output data.（network的选择约束了tensor变换的步骤）</li>
<li>所以如果选择了不好的network，可能导致你在错误的<code>hyposhesis space</code>里搜索，以致于效果不好。</li>
</ul>
</li>
</ul>
</li>
<li>The <code>input data</code> and corresponding <code>targets</code></li>
<li>The <code>loss</code> function (objective function), which defines the <code>feedback signal</code> used for learning<ul>
<li>The quantity that will be minimized during training.</li>
<li>It represents a measure of success for the task at hand.</li>
<li>多头网络有多个loss function，但基于<code>gradient-descent</code>的网络只允许有一个标量的loss，因此需要把它合并起来（相加，平均...）</li>
</ul>
</li>
<li>The <code>optimizer</code>, which determines how learning proceeds<ul>
<li>Determines how the network will be updated based on the loss function.</li>
<li>It implements a specific variant of stochastic gradient descent (SGD).</li>
</ul>
</li>
</ul>
<h3>Classifying movie reviews: a binary classification example</h3>
<p><strong>一个二元分类的例子</strong></p><p>情感分析/情绪判断，数据源是IMDB的影评数据.</p><p><strong>理解hidden的维度</strong></p><p>how much freedom you’re allowing the network to have when learning internal representations. 即学习表示（别的地方通常叫提取特征）的自由度。</p><p>目前提出了架构网络的时候的两个问题：</p><ol>
<li>多少个隐层</li>
<li>隐层需要多少个神经元（即维度）</li>
</ol>
<p>后面的章节会介绍一些原则。</p><p><strong>激活函数</strong></p><p>李宏毅的课程里，从用整流函数来逼近非线性方程的方式来引入激活函数，也就是说在李宏毅的课程里，激活函数是<strong>因</strong>，推出来的公式是<strong>果</strong>，当然一般的教材都不是这个角度，都是有了线性方程，再去告诉你，这样还不够，需要一个<code>activation</code>。</p><p>本书也一样，告诉你，如果只有<code>wX+b</code>，那么只有线性变换，这样会导致对<code>hypothesis space</code>的极大的限制，为了扩展它的空间，就引入了非线性的后续处理。总之，都是在自己的逻辑体系内的。本书的逻辑体系就是<code>hypothesis space</code>，你想要有解，就是在这个空间里。</p><p><strong>网络结构</strong></p><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">models</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10000</span><span class="p">,)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">))</span>
</pre></div>
<p><strong>entropy</strong></p><p><code>Crossentropy</code> is a quantity from the field of Information Theory（信息论） that measures the distance between probability distributions。</p><p>in this case, between the ground-truth distribution and your predictions.</p><p><strong>keras风格的训练</strong></p><p>其实就是模仿了<code>scikit learn</code>的风格。对快速实验非常友好，缺点就是封装过于严重，不利于调试，但这其实不是问题，谁也不会只用keras。</p><div class="highlight"><pre><span></span><span class="c1"># 演示用类名和字符串分别做参数的方式</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;rmsprop&#39;</span><span class="p">,</span>
            <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span>
            <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>

<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">optimizers</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizers</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">),</span>
            <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span>
            <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>

<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizers</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">),</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">losses</span><span class="o">.</span><span class="n">binary_crossentropy</span><span class="p">,</span>
            <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">metrics</span><span class="o">.</span><span class="n">binary_accuracy</span><span class="p">])</span>

<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;rmsprop&#39;</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;acc&#39;</span><span class="p">])</span>

<span class="c1"># train</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">partial_x_train</span><span class="p">,</span>
                    <span class="n">partial_y_train</span><span class="p">,</span>
                    <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">))</span>
</pre></div>
<p>后续优化，就是对比train和validate阶段的loss和accuracy，找到overfit的节点（比如是第N轮），然后重新训练到第N轮（或者直接用第N轮生成的模型，如果有），用这个模型来预测没有人工标注的数据。</p><p>核心就是要<strong>训练到明显的overfit</strong>为止。这是第一个例子的内容，所以是告诉你怎么用这个简单的网络来进行预测，而不是立即着眼怎么去解决overfit.</p><p><strong>第一个小结</strong></p><ol>
<li>数据需要预处理成tensor, 了解几种tensor化，或vector化的方式</li>
<li>堆叠全连接网络(Dense)，以及activation，就能解决很多分类问题</li>
<li>二元分类的问题通常在Dense后接一个sigmoid函数</li>
<li>引入二元交叉熵(BCE)作为二元分类问题的loss</li>
<li>用了rmsprop优化器，暂时没有过多介绍。这些优化器都是为了解决能不能找到局部极值而进行的努力，具体可看上一篇李宏毅的笔记</li>
<li>使用overfit之前的那一个模型来做预测</li>
</ol>
<h3>Classifying newswires: a multiclass classification example</h3>
<p>这次用路透社的新闻来做多分类的例子，给每篇新闻标记类别。</p><p><strong>预处理，一些要点</strong>:</p><ol>
<li>不会采用所有的词汇，所以预处理时，根据词频，只选了前1000个词</li>
<li>用索引来实现文字-数字的对应</li>
<li>用one-hot来实现数字-向量的对应</li>
<li>理解什么是序列（其实就是一句话）</li>
<li>所以句子有长有短，为了矩阵的批量计算（即多个句子同时处理），需要“对齐”（补0和截断）</li>
<li>理解稠密矩阵(word-embedding)与稀疏矩阵(one-hot)的区别（这里没有讲，用的是one-hot)</li>
</ol>
<p><strong>网络和训练</strong></p><ol>
<li>网络结构不变，每层的神经元为(64, 64, 46)</li>
<li>前面增加了神经元，16个特征对语言来说应该是不够的）</li>
<li>最后一层由1变成了46，因为二元的输出只需要一个数字，而多元输出是用one-hot表示的向量，最有可能的类别在这个向量里拥有最大的值。</li>
</ol>
<p>4。 损失函数为<code>categorial_crossentropy</code>，这在别的教材里应该就是普通的CE.</p><p><strong>新知识</strong></p><ol>
<li>介绍了一种不用one-hot而直接用数字表示真值的方法，但是没有改变网络结构（即最后一层仍然输出46维，而不是因为你用了一个标量而只输出一维。<ul>
<li>看来它仅仅就是一个<strong>语法糖</strong>（loss函数选择<code>sparse_categorial_crossentropy</code>就行了）</li>
</ul>
</li>
<li>尝试把第2层由64改为4，变成<code>bottleneck</code>，演示你有46维的数据要输出的话，前面的层数或少会造成信息压缩过于严重以致于丢失特征。</li>
</ol>
<h3>Predicting house prices: a regression example</h3>
<p>这里用了预测房价的Boston Hosing Price数据集。</p><p>与吴恩达的课程一样，也恰好是在这个例子里引入了对input的normalize，理由也仅仅是简单的把量纲拉平。现在我们应该还知道Normalize还能让数据在进入激活函数前，把值限定在激活函数的梯度敏感区。</p><p>此外，一个知识点就是你对训练集进行Normalize用的均值和标准差，是直接用在测试集上的，而不是各计算各的，可以理解为保持训练集的“分布”。</p><blockquote>
<p>这也是<code>scikit learn</code>里<code>fit_tranform</code>和直接用<code>transform</code>的原因。</p></blockquote>
<ol>
<li>对scalar进行预测是不需要进行激活（即无需把输出压缩到和为1的概率空间）</li>
<li>loss也直观很多，就是predict与target的差（取平方，除2，除批量等都是辅助），预测与直值的差才是核心。</li>
</ol>
</div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/Deep-Learning-with-Python-Notes-2/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/Deep-Learning-with-Python-Notes-1/" target="_self">《Deep Learning with Python》笔记[1]</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/Deep-Learning-with-Python-Notes-1/" target="_self">
                <time class="text-uppercase">
                    September 12 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><p>本来是打算趁这个时间好好看看花书的，前几章看下来确实觉得获益匪浅，但看下去就发现跟不上了，特别是抱着急功近利的心态的话，目前也沉不下去真的一节节吃透地往下看。这类书终归不是入门教材，是需要你有过一定的积累后再回过头来看的。</p><p>于是想到了《Deep Learning with Python》，忘记这本书怎么来的了，但是在别的地方看到了有人推荐，说是Keras的作者写的非常好的一本入门书，翻了前面几十页后发现居然跟进去了，不该讲的地方没讲比如数学细节，而且思路也极其统一，从头贯穿到尾（比如representations, latent space,  hypothesis space），我觉得很受用。</p><p>三百多页全英文，居然也没查几个单词就这么看完了，以前看文档最多十来页，也算一个突破了，可见其实还是一个耐心的问题。</p><p>看完后书上做了很多笔记，于是顺着笔记读了第二遍，顺便就把笔记给电子化了。不是教程，不是导读。</p><h1>Fundamentals of deep learning</h1>
<p><strong>核心思想</strong>：
learng useful <code>representations</code> of input data</p><blockquote>
<p>what’s a <code>representation</code>?</p><p>At its core, it’s a different way to look at data—to represent or encode data.</p></blockquote>
<p>简单回顾深度学习之于人工智能的历史，每本书都会写，但每本书里都有作者自己的侧重：</p><ul>
<li>Artificial intelligence</li>
<li>Machine learning<ul>
<li>Machine learning is tightly related to <code>mathematical statistics</code>, but it differs from statistics in several important ways.<ul>
<li>machine learning tends to deal with large, complex datasets (such as a dataset of millions of images, each consisting of tens of thousands of pixels)</li>
<li>classical statistical analysis such as Bayesian analysis would be impractical(不切实际的).</li>
<li>It’s a hands-on discipline in which ideas are proven empirically more often than theoretically.（工程/实践大于理论）</li>
</ul>
</li>
<li>是一种meaningfully transform data<ul>
<li>Machine-learning models are all about finding appropriate representations for their input data—transformations of the data that make it more amenable to the task at hand, such as a classification task.</li>
<li>寻找更有代表性的representation, 通过:(coordinate change, linear projections, tranlsations, nonlinear operations)</li>
<li>只会在<code>hypothesis space</code>里寻找</li>
<li>以某种反馈为信号作为优化指导</li>
</ul>
</li>
</ul>
</li>
<li>Deep learning<ul>
<li>Machine Learing的子集，一种新的learning representation的新方法</li>
<li>虽然叫神经网络(<code>neural network</code>)，但它既非neural，也不是network，更合理的名字：<ul>
<li><code>layered representations learning</code> and <code>hierarchical representations learning</code>.</li>
</ul>
</li>
<li>相对少的层数的实现叫<code>shallow learning</code></li>
</ul>
</li>
</ul>
<h2>Before deep learning</h2>
<ul>
<li>Probabilistic modeling<ul>
<li>the earliest forms of machine learning,</li>
<li>still widely used to this day.<ul>
<li>One of the best-known algorithms in this category</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>is the <code>Naive Bayes algorithm</code>(朴素贝叶斯)
    * 条件概率，把规则理解为“条件”，判断概率，比如垃圾邮件。
        * A closely related model is the logistic regression</p><ul>
<li>Early neural networks<ul>
<li>in the mid-1980s, multiple people independently rediscovered the Backpropagation algorithm</li>
<li>The <code>first</code> successful practical application of neural nets came in 1989 from Bell Labs -&gt; <strong>LeNet</strong></li>
</ul>
</li>
<li>Kernel methods<ul>
<li>Kernel methods are <code>a group of classification algorithms</code>(核方法是一组分类算法)<ul>
<li>the best known of which is the <code>support vector machine</code> (<strong>SVM</strong>).</li>
<li>SVMs aim at solving classification problems <strong>by</strong> finding good <em>decision boundaries</em> between two sets of points belonging to two different categories.<ol>
<li>先把数据映射到高维，decision boundary表示为<code>hyperplane</code></li>
<li>最大化每个类别里离hyperplane最近的点到hyperplane的距离:<code>maximizing the margin</code></li>
</ol>
</li>
<li>The technique of mapping data to a high-dimensional representation 非常消耗计算资源，实际使用的是核函数(<code>kernel function</code>):<ul>
<li>不把每个点转换到高维，而只是计算每两个点在高维中的距离</li>
<li>核函数是手工设计的，不是学习的</li>
</ul>
</li>
<li>SVM在分类问题上是经典方案，但难以扩展到大型数据集上</li>
<li>对于perceptual problems(感知类的问题)如图像分类效果也不好<ul>
<li>它是一个<code>shallow method</code></li>
<li>需要事先手动提取有用特征(<code>feature enginerring</code>)-&gt; difficult and  brittle（脆弱的）</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Decision trees, random forests, and gradient boosting machines<ul>
<li>Random Forest<ul>
<li>you could say that they’re almost always the <em>second-best</em> algorithm for any shallow machine-learning task.</li>
</ul>
</li>
<li>gradient boosting machines (1st):<ul>
<li>a way to improve any machine-learning model by iteratively training new models that specialize in <code>addressing the weak points of the previous models</code>.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>What makes deep learning different</h2>
<p>it completely automates what <em>used to be</em> <strong>the most crucial step</strong> in a machine-learning workflow: <code>feature engineering</code>. 有人认为这叫穷举，思路上有点像，至少得到特征的过程不是靠观察和分析。</p><p><strong>feature engineering</strong></p><blockquote>
<p>manually engineer good layers of representations for their data</p></blockquote>
</div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/Deep-Learning-with-Python-Notes-1/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/%E5%87%A0%E5%A4%A7%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95python%E5%AE%9E%E7%8E%B0/" target="_self">几大排序算法python实现</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/%E5%87%A0%E5%A4%A7%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95python%E5%AE%9E%E7%8E%B0/" target="_self">
                <time class="text-uppercase">
                    August 23 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><h2>冒泡排序</h2>
<p>冒泡排序基础原理是每一轮都让最大的值移到最右边，一句话就够了。</p><p>如果想小优化一下，可以在每一轮过后都把最后一个（已经是最大的值）排除出去，这种我把它称之为“压缩边界“，在下面的几种排序算法里都有反复提及。而且之所以说优化，就是不做也行，如果只是想演示算法核心思想的话。</p><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">bubble_sort</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span><span class="o">-</span><span class="n">i</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">arr</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">arr</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">]:</span>
                <span class="n">arr</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">arr</span>
<span class="n">bubble_sort</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</pre></div>
<p>ouput:</p>
<pre><code>[0, 1, 1, 1, 2, 3, 5]
</code></pre>
<h2>快速排序</h2>
<p>选出一个合适的（或任意的）中值(<code>pivot</code>），把比它大的和小的分列到两边，再对两边进行上述分类的递归操作。实际操作中往往会选定了<code>pivot</code>后，从右往左搜小数，从左往右搜大数，以规避pivot本身过大或过小时，如果选定的方向不对，可能每一次都需要把整个数组几乎遍历完才找到合适的数的情况。</p><p>again，这只是优化，如果不考虑这些，那么核心思想是非常简单的：</p><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">q_sort</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">arr</span>
    <span class="n">pivot</span> <span class="o">=</span> <span class="n">arr</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
    <span class="n">left</span>  <span class="o">=</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">arr</span> <span class="k">if</span> <span class="n">item</span> <span class="o">&lt;=</span> <span class="n">pivot</span><span class="p">]</span>
    <span class="n">right</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">arr</span> <span class="k">if</span> <span class="n">item</span> <span class="o">&gt;</span> <span class="n">pivot</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">q_sort</span><span class="p">(</span><span class="n">left</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">pivot</span><span class="p">]</span> <span class="o">+</span> <span class="n">q_sort</span><span class="p">(</span><span class="n">right</span><span class="p">)</span>
</pre></div>
<p>这个不但实现了（中值+两侧+递归）的思路+没有任何优化，效果已经出奇的好了！</p><p>但网上演示的都是下面这种花活，从两侧来压缩备选区域（压缩的意思是排好了的区域就不要管了），下面列了个表格来演示过程，看大家是不是能轻松看懂快排的两个核心机制：<code>标红位</code>，和<code>边界压缩</code>。说明如下：</p><ul>
<li>任意写个数组[6,7,3,2,14,9]，任取一个数为pivot，就第1个吧（6），</li>
<li>左箭头表示从右往左找第一个小于pivot的值，右箭头表示从左往右找第一个大于pivot的值</li>
<li>红色代表标红位，废位，即当前位找到本轮符合要求的值，但挪到两侧去了，$\color{red}{下一轮的符合条件的值应该放入这个标红位里}$</li>
<li>括号里的表示是这一轮该位置赋的新值，它来自于标红位，同时，括号的位置也就是上一轮的标红位</li>
<li>划掉的表示已经压缩了左右边界，下一轮就不要在这些数里面选了（为了视觉简洁，标红位就不划了）</li>
</ul>
<p>$
\require{cancel}
\begin{array}{c|cccccc|l}
index&amp;0&amp;1&amp;2&amp;3&amp;4&amp;5&amp;\
\hline
array&amp;\color{red}6&amp;7&amp;3&amp;2&amp;14&amp;9\
\underleftarrow{\small找小数}&amp;\cancel{(2)}&amp;7&amp;3&amp;\color{red}2&amp;\cancel{14}&amp;\cancel{9}&amp;找到2，放到索引0\
\underrightarrow{\small找大数}&amp;\cancel{2}&amp;\color{red}7&amp;3&amp;(7)&amp;\cancel{14}&amp;\cancel{9}&amp;找到7，放到索引3\
\underleftarrow{\small找小数}&amp;\cancel{2}&amp;(3)&amp;\color{red}3&amp;\cancel{7}&amp;\cancel{14}&amp;\cancel{9}&amp;找到3，放到索引2\
&amp;2&amp;3&amp;(6)&amp;7&amp;14&amp;9&amp;(1,2)索引间已没有大于6的数，排序完成，回填6
\end{array}
$</p><ol>
<li>注意第1次从右往左找比6小的数时，找到2，右边的14，9就可以全部划掉了，因为我永远是在用6在左右查找，这一次pass了，后面永远会pass</li>
</ol>
<ul>
<li>这样边界压缩得非常快，这就是称之为“快速”排序的原因吧？</li>
</ul>
<ol start="2">
<li>目前只完成一次分割（即按6为标识切分左右），接下来用同样的逻辑递归6左边的<code>[2]</code>和右边的<code>[7,14,9]</code>排序即可</li>
</ol>
<ul>
<li>所以快排就3个部分，一个主体，执行一次分割，然后对分割后的两个数组分别递归回去，这样代码怎么写也出来了：</li>
</ul>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">q_sort</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">):</span>
    <span class="c1"># （left， right）用来保存不断缩小的查找数组索引界限</span>
    <span class="c1">#  我上面模拟的过程里，就是划掉的数字的左右边界</span>
    <span class="n">left</span><span class="p">,</span> <span class="n">right</span> <span class="o">=</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">start</span>
    <span class="n">pivot</span> <span class="o">=</span> <span class="n">array</span><span class="p">[</span><span class="n">start</span><span class="p">]</span>

    <span class="k">while</span> <span class="n">left</span> <span class="o">&lt;</span> <span class="n">right</span><span class="p">:</span>
        <span class="c1"># 从右往左选小于pivot的数</span>
        <span class="n">matched</span> <span class="o">=</span> <span class="kc">False</span> <span class="c1"># 标识这一轮有没有找到合适的数（如果没找到其实说明排序已经完成）</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">left</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">right</span><span class="o">+</span><span class="mi">1</span><span class="p">)):</span> <span class="c1"># 去头，含尾, 反序</span>
            <span class="k">if</span> <span class="n">array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">pivot</span><span class="p">:</span>
                <span class="n">array</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="n">right</span> <span class="o">=</span> <span class="n">i</span>  <span class="c1"># 从右到左比到第i个才有比pivot小的数，那么i右侧全大于pivot，下次可以缩小范围了</span>
                <span class="n">index</span> <span class="o">=</span> <span class="n">i</span>
                <span class="n">matched</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">break</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">matched</span><span class="p">:</span>
            <span class="k">break</span>  <span class="c1"># 右侧没有找到更小的数，说明剩余数组全是大数，已经排完了</span>

        <span class="n">left</span> <span class="o">+=</span> <span class="mi">1</span> <span class="c1"># 找到了填入新数后就顺移一位</span>
        <span class="n">matched</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="c1"># 从左往右选大于pivot的数</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">left</span><span class="p">,</span> <span class="n">right</span><span class="p">):</span> <span class="c1"># 有头无尾</span>
            <span class="k">if</span> <span class="n">array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">pivot</span><span class="p">:</span>
                <span class="n">array</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="n">left</span> <span class="o">=</span> <span class="n">i</span> <span class="c1"># 此时i左侧也没有比pivot大的数，下次再找也可以忽略了，也标记下缩小范围</span>
                <span class="n">index</span> <span class="o">=</span> <span class="n">i</span>
                <span class="n">matched</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">break</span><span class="p">;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">matched</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">right</span> <span class="o">-=</span> <span class="mi">1</span>
    <span class="n">array</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">pivot</span> <span class="c1"># 把标红位设为pivot</span>

    <span class="c1"># 开始递归处理左右切片</span>
    <span class="k">if</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span>
        <span class="n">q_sort</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">end</span> <span class="o">&gt;</span> <span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span>
        <span class="n">q_sort</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">array</span>

<span class="n">arr</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="c1"># 我封装时为了兼容递归，要人为传入start, end，进入函数时自行计算一下好了</span>
<span class="n">q_sort</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
<p>output</p>
<pre><code>[0, 1, 1, 1, 2, 3, 5]
</code></pre>
<h2>堆排序</h2>
<ol>
<li>其实就是把数字摆成二叉树，知道二叉树是啥就行，或者看下面的动图</li>
<li>每当一个数字排入堆中的时候，都与父节点比一下大小，如果大于父节点，则与父节点交换位置</li>
</ol>
<ul>
<li>不与兄弟节点比较，即兄弟节点之间暂不排序</li>
</ul>
<ol start="3">
<li>交换到父节点后再跟当前位置的父节点比较，如此往复，至到根节点（<strong>递归警告</strong>）</li>
<li>一轮摆完后，最大的数肯定已经<strong>上浮</strong>到根节点了，把它与最末的一个数字调换位置（这个数字是一个相对小，但不一定是最小的），然后把最大的这个数从堆里移除（已经确认是最大的，位置也就确认了，不再参与比较）</li>
<li>实现的时候，因为有“找父/子节点比大小”这样的逻辑，显然可以直接用上二叉树的性质，不要自己去观察或归纳了。</li>
</ol>
<p>动图比较长，耐心看下：</p><figure class="vertical-figure" style="flex: 35.714285714285715" ><img width="220" height="308" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/72b4116eb1e49a3b1eaa9097a2890b60.jpg" alt=""/></figure><blockquote>
<p>在实现每一轮的遍历数字较大的那个子节点并交换数字的过程中，我之前用的是递归，在小数据量顺利通过，但上万条数据时碰到了<code>RecursionError: maximum recursion depth exceeded in comparison</code>, 查询本机迭代大小设置为1000，但设到几十万就不起作用了（虽然不报错），于是改成了<code>while</code>循环，代码几乎没变，但是秒过了。</p></blockquote>
<p>递归只是让代码看起来简洁而牛逼，并没有创造什么新的东西，while能行那就算过了吧。</p><p>但是代码开始dirty了起来，大量的代码在控制边界和描述场景，显然有些条件可能是冗余的，我没有很好地合并这些边界和条件导致if太多，这是个不好的演示，但三个核心函数还是阐释了这种算法的思路：</p><ul>
<li>摆成树（堆）</li>
<li>从leaf到root冒泡 (child去比parent)</li>
<li>从root到leaf冒泡 (parent去比child)</li>
</ul>
<div class="highlight"><pre><span></span><span class="c1"># helper</span>
<span class="n">get_parent_index</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">i</span> <span class="p">:</span> <span class="nb">max</span><span class="p">((</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">get_child_index</span>  <span class="o">=</span> <span class="k">lambda</span> <span class="n">i</span> <span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>

<span class="k">def</span> <span class="nf">heap_sort</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="n">heapify</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>                    <span class="c1"># 初排</span>
    <span class="n">siftDown</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>    <span class="c1"># 整理</span>
    <span class="k">return</span> <span class="n">arr</span>

<span class="k">def</span> <span class="nf">heapify</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">while</span> <span class="n">index</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
        <span class="n">p_index</span> <span class="o">=</span> <span class="n">get_parent_index</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
        <span class="n">parent</span>  <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">p_index</span><span class="p">]</span>
        <span class="n">child</span>   <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">child</span> <span class="o">&gt;</span> <span class="n">parent</span><span class="p">:</span>
            <span class="n">arr</span><span class="p">[</span><span class="n">p_index</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">p_index</span><span class="p">]</span>
            <span class="n">siftUp</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">p_index</span><span class="p">)</span>
        <span class="n">index</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">arr</span>

<span class="k">def</span> <span class="nf">siftUp</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">c_index</span><span class="p">):</span>
    <span class="n">p_index</span> <span class="o">=</span> <span class="n">get_parent_index</span><span class="p">(</span><span class="n">c_index</span><span class="p">)</span>
    <span class="n">parent</span>  <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">p_index</span><span class="p">]</span>
    <span class="n">leaf</span>    <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">c_index</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">parent</span> <span class="o">&lt;</span> <span class="n">leaf</span><span class="p">:</span>
        <span class="n">arr</span><span class="p">[</span><span class="n">p_index</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">c_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">c_index</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">p_index</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">p_index</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">siftUp</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">p_index</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">siftDown</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    1. 交换首尾两个数，这样尾数就变成了最大</span>
<span class="sd">    2. 跟两个子节点中较大的比较，并迭代，递归下去</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">while</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="n">end</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">start</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">arr</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">end</span><span class="p">]</span> <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">end</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">left_i</span>  <span class="o">=</span> <span class="n">get_child_index</span><span class="p">(</span><span class="n">start</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">left_i</span> <span class="o">&gt;=</span> <span class="n">end</span><span class="p">:</span> 
            <span class="c1"># 子结点是end，就不要比了，把当前节点设为新end</span>
            <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">end</span> <span class="o">-=</span> <span class="mi">1</span>
            <span class="k">continue</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">right_i</span> <span class="o">=</span> <span class="n">left_i</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">left_i</span>
            <span class="k">if</span> <span class="n">right_i</span> <span class="o">&lt;</span> <span class="n">end</span><span class="p">:</span>
                <span class="c1"># 右边没有到end的话，取出值比大小</span>
                <span class="c1"># 并且把下一轮的start设为选中的子节点</span>
                <span class="n">index</span> <span class="o">=</span> <span class="n">left_i</span> <span class="k">if</span> <span class="n">arr</span><span class="p">[</span><span class="n">left_i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">arr</span><span class="p">[</span><span class="n">right_i</span><span class="p">]</span> <span class="k">else</span> <span class="n">right_i</span>
            <span class="n">parent</span>  <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">start</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">parent</span> <span class="o">&lt;</span> <span class="n">arr</span><span class="p">[</span><span class="n">index</span><span class="p">]:</span>
                <span class="n">arr</span><span class="p">[</span><span class="n">start</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">start</span><span class="p">]</span>
        <span class="c1"># 如果左叶子已经被标记为end  (已提前return)</span>
        <span class="c1"># 如果右边叶子被标记为end</span>
        <span class="c1"># 如果下一个索引被标记为end</span>
        <span class="c1"># 都表示本轮遍历已经到底, end往前移一位即可</span>
        <span class="k">if</span> <span class="n">right_i</span> <span class="o">&gt;=</span> <span class="n">end</span> <span class="ow">or</span> <span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">right_i</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># 用start=0表示需要进行一次首尾替换再从头到尾移动一次</span>
            <span class="n">end</span> <span class="o">-=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># 否则进入下一个循环</span>
            <span class="c1"># 起点就是用来跟父级做比较的索引</span>
            <span class="c1"># 终点不变</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">index</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
    <span class="kn">import</span> <span class="nn">time</span>
<span class="c1">#     np.random.seed(7)</span>
<span class="c1">#     length = 20000</span>
<span class="c1">#     arr = list(np.random.randint(0, length*5, size=(length,)))</span>
    <span class="n">arr</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="s2">&quot;65318724&quot;</span><span class="p">)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">heap_sort</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">s</span><span class="p">[:</span><span class="mi">100</span><span class="p">])</span>
</pre></div>
<p>output</p>
<pre><code>4.696846008300781e-05 
 ['1', '2', '3', '4', '5', '6', '7', '8']
</code></pre>
<h2>归并排序</h2>
<p>这次先看图吧，看你能总结出啥：
<figure class="vertical-figure" style="flex: 35.714285714285715" ><img width="220" height="308" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/72b4116eb1e49a3b1eaa9097a2890b60.jpg" alt=""/></figure></p><ol>
<li>第一步是把数组打散后两两排序，实现每一组（2个元素）是排好序的</li>
<li>第二步仍然是两两排序，但是把前面排序好的每两个组成一个组：</li>
</ol>
<ul>
<li>这样每组就有2个数了，但组数就减半了</li>
<li>每一组拿出当前最前面的数出来比较，每次挑1个最小的，移出来</li>
<li>剩下的组里数字有多有少，仍然比较组里面排最前的那个（因为每组已经从小到大排好了，最前面那个就是组里最小的）</li>
<li>所以代码里能跟踪两个组里当前的“最前的索引”是多少就行了</li>
</ul>
<ol start="3">
<li>继续合并，单从理论上你也能发现，每组的数字个数会越来越多，组数却越来越少， 显然，最终会归并成一个组，而且已经是排好序了的。</li>
</ol>
<p>这就是归并名字的<strong>由来</strong>。后面还有一种<code>希尔算法</code>，正好是它的相反，即打得越来越散，散成每组只有一个元素的时候，排序也排好了，看到那一节的时候注意对比。</p><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>
<span class="k">def</span> <span class="nf">merge_sort</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    每一轮比较的时候是把选中的元素填到另一个数组里</span>
<span class="sd">    为了减少内存消耗，就循环用两个数组</span>
<span class="sd">    我们用交替设置i和j为0和1来实现这个逻辑</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">start</span>    <span class="o">=</span> <span class="mi">0</span>
    <span class="n">step</span>     <span class="o">=</span> <span class="mi">1</span>
    <span class="n">length</span>   <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">lists</span>    <span class="o">=</span> <span class="p">[</span><span class="n">arr</span><span class="p">,</span> <span class="p">[]]</span>
    <span class="n">i</span><span class="p">,</span> <span class="n">j</span>     <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span>
    <span class="k">while</span> <span class="n">step</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">:</span>
        <span class="n">compare</span><span class="p">(</span><span class="n">lists</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">start</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">lists</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
        <span class="n">step</span> <span class="o">*=</span> <span class="mi">2</span>
        <span class="n">i</span><span class="p">,</span> <span class="n">j</span>  <span class="o">=</span> <span class="n">j</span><span class="p">,</span> <span class="n">i</span>
    <span class="k">return</span> <span class="n">lists</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">gen_indexs</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">length</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    根据左边界和步长确定本轮拿来比较的两个数组的边界</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">left_end</span>    <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="n">step</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">right_start</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">start</span> <span class="o">+</span> <span class="n">step</span><span class="p">,</span> <span class="n">length</span><span class="p">)</span>
    <span class="n">right_end</span>   <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">right_start</span> <span class="o">+</span> <span class="n">step</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">length</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">start</span><span class="p">,</span> <span class="n">left_end</span><span class="p">,</span> <span class="n">right_start</span><span class="p">,</span> <span class="n">right_end</span>


<span class="k">def</span> <span class="nf">compare</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">result</span><span class="p">):</span>
    <span class="n">result</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
    <span class="n">left_start</span><span class="p">,</span> <span class="n">left_end</span><span class="p">,</span> <span class="n">right_start</span><span class="p">,</span> <span class="n">right_end</span> \
                <span class="o">=</span> <span class="n">gen_indexs</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">length</span><span class="p">)</span>
    <span class="n">left_index</span>  <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 组内索引(0, step-1)</span>
    <span class="n">right_index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">left_start</span> <span class="o">&lt;=</span> <span class="n">length</span><span class="p">:</span>
        <span class="n">left</span>    <span class="o">=</span> <span class="n">left_start</span> <span class="o">+</span> <span class="n">left_index</span>
        <span class="n">right</span>   <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">right_start</span> <span class="o">+</span> <span class="n">right_index</span><span class="p">,</span> <span class="n">length</span><span class="p">)</span>
        <span class="n">l_done</span>  <span class="o">=</span> <span class="kc">False</span>
        <span class="n">r_done</span>  <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">arr</span><span class="p">[</span><span class="n">left</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">arr</span><span class="p">[</span><span class="n">right</span><span class="p">]:</span>
            <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">arr</span><span class="p">[</span><span class="n">left</span><span class="p">])</span>
            <span class="n">left_index</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">left</span>   <span class="o">=</span> <span class="n">left_start</span> <span class="o">+</span> <span class="n">left_index</span>
            <span class="n">l_done</span> <span class="o">=</span> <span class="n">left</span> <span class="o">==</span> <span class="n">right_start</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">arr</span><span class="p">[</span><span class="n">right</span><span class="p">])</span>
            <span class="n">right_index</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">r_done</span> <span class="o">=</span> <span class="p">(</span><span class="n">right_start</span> <span class="o">+</span> <span class="n">right_index</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">right_end</span>
        <span class="k">if</span> <span class="n">l_done</span> <span class="ow">or</span> <span class="n">r_done</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">l_done</span><span class="p">:</span>
                <span class="c1"># 左边没数了，右边的数全塞到result里去</span>
                <span class="n">result</span> <span class="o">+=</span> <span class="n">arr</span><span class="p">[</span><span class="n">right</span><span class="p">:</span><span class="n">right_end</span><span class="p">]</span>
                <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">arr</span><span class="p">[</span><span class="n">right_end</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># 右边没数了，左边剩下的数全塞到result里去</span>
                <span class="n">result</span> <span class="o">+=</span> <span class="n">arr</span><span class="p">[</span><span class="n">left</span><span class="p">:</span><span class="n">right_start</span><span class="p">]</span>
            <span class="n">left_start</span><span class="p">,</span> <span class="n">left_end</span><span class="p">,</span> <span class="n">right_start</span><span class="p">,</span> <span class="n">right_end</span> \
                        <span class="o">=</span> <span class="n">gen_indexs</span><span class="p">(</span><span class="n">right_end</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">length</span><span class="p">)</span>
            <span class="n">left_index</span>  <span class="o">=</span> <span class="mi">0</span>
            <span class="n">right_index</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
    <span class="kn">import</span> <span class="nn">time</span>
<span class="c1">#     np.random.seed(7)</span>
<span class="c1">#     length = 20000</span>
<span class="c1">#     arr = list(np.random.randint(0, length*5, size=(length,)))</span>
    <span class="n">arr</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">17</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">22</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">65</span><span class="p">]</span><span class="c1">#,2,13,4,6,17,33,8,0,4,17,22]</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">merge_sort</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="p">,</span> <span class="n">s</span><span class="p">[:</span><span class="mi">100</span><span class="p">])</span>
</pre></div>
<p>output</p>
<pre><code>5.626678466796875e-05
[0, 1, 1, 2, 3, 5, 6, 7, 8, 9, 9, 17, 22, 65]
</code></pre>
<p>以上是我对着动画实现的一个版本，很繁琐，而且只是直观地把动画演示了一遍，即先两两组合，对比，再四四对比，直到最后只有两个大数组，比一次。直到我看到这个思路，我把它实现出来如下：</p><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mergesort</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">end</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">mid</span> <span class="o">=</span> <span class="p">(</span><span class="n">start</span> <span class="o">+</span> <span class="n">end</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="n">mergesort</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">mid</span><span class="p">)</span> <span class="c1"># left</span>
        <span class="n">mergesort</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">mid</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span> <span class="c1"># right</span>
        <span class="n">sort</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">mid</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span> 
        <span class="c1"># 最里层：([0:1],[1:2]) -&gt; (start, mid, end) 为(0,1,2)</span>
        <span class="c1"># 所以退出条件是 end - start &gt; 1</span>

<span class="k">def</span> <span class="nf">sort</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">left</span><span class="p">,</span> <span class="n">mid</span><span class="p">,</span> <span class="n">right</span><span class="p">):</span>
    <span class="n">p1</span> <span class="o">=</span> <span class="n">left</span>
    <span class="n">p2</span> <span class="o">=</span> <span class="n">mid</span>
    <span class="n">temp</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># 本轮排序的结果</span>
    <span class="c1"># 左右两个数组分别按顺序取出最前一个来比较大小</span>
    <span class="c1"># 小数拿到临时数组里去，游标加1</span>
    <span class="k">while</span> <span class="n">p1</span> <span class="o">&lt;</span> <span class="n">mid</span> <span class="ow">and</span> <span class="n">p2</span> <span class="o">&lt;</span> <span class="n">right</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">arr</span><span class="p">[</span><span class="n">p2</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">arr</span><span class="p">[</span><span class="n">p1</span><span class="p">]:</span>
            <span class="n">temp</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">arr</span><span class="p">[</span><span class="n">p1</span><span class="p">])</span>
            <span class="n">p1</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">temp</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">arr</span><span class="p">[</span><span class="n">p2</span><span class="p">])</span>
            <span class="n">p2</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="c1"># 不管是左边还是右边，剩下的都是已经排好的（大数），直接接到数组后面</span>
    <span class="k">if</span> <span class="n">p1</span> <span class="o">&lt;</span> <span class="n">mid</span><span class="p">:</span>
        <span class="n">temp</span> <span class="o">+=</span> <span class="n">arr</span><span class="p">[</span><span class="n">p1</span><span class="p">:</span><span class="n">mid</span><span class="p">]</span>
    <span class="k">elif</span> <span class="n">p2</span> <span class="o">&lt;</span> <span class="n">right</span><span class="p">:</span>
        <span class="n">temp</span> <span class="o">+=</span> <span class="n">arr</span><span class="p">[</span><span class="n">p2</span><span class="p">:</span><span class="n">right</span><span class="p">]</span>

    <span class="n">arr</span><span class="p">[</span><span class="n">left</span><span class="p">:</span><span class="n">right</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span>
</pre></div>
<p>sort部分没变，还是两边比较，永远取小的一个，直到排成一排变成一组。主体变成了mergesort()的递归。用文字描述的话，就是这个方法就做了一件事：把当前数组左右分开，然后用永远取最前一个来当最小值的方式（sort方法）完成排序。
等于是直接就走到了我实现的方法的最后一步，而用递归的方式，让更小的单元完成排序，比如每8个，每4个，每2个，真实发生排序的时候，仍然是我写的代码的第一层，就是两两排序。但是代码简洁抽象好多。</p><p>如果把递归理解为异步的话：</p><div class="highlight"><pre><span></span><span class="k">await</span> <span class="nx">sort_lert</span><span class="p">()</span>
<span class="k">await</span> <span class="nx">sort_right</span><span class="p">()</span>
<span class="nx">sort</span><span class="p">(</span><span class="nx">left</span><span class="p">,</span> <span class="nx">right</span><span class="p">)</span>
</pre></div>
<p>即代码真走到第3行了的话，所有的数据已经排好序了</p><h2>基数排序</h2>
<figure class="vertical-figure" style="flex: 35.714285714285715" ><img width="220" height="308" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/72b4116eb1e49a3b1eaa9097a2890b60.jpg" alt=""/></figure><p>看图，为什么从个位向高位依次排过去为什么就能保证后面高位的排序不会影响低序的，直观来理解的话，就是</p><ol>
<li>如果高位数字不一样，那么低位顺序是没意义的，按高位大小排即可</li>
<li>如果高位数字一样，那么低位已经排好序了</li>
<li>按这个逻辑由低位向高位排，按归纳法，可以推到适用普遍情况的</li>
</ol>
<p>这里就有一个逻辑bug了，我本来就是要根据大小排序比如1万个数字，结果你说要先把这1万个数字根据个位数大小排一遍，再根据十位数大小排一遍，我无数次地排这1万个数字，为何不直接按大小把它排好算了呢？</p><p>这就是这个算法存在的意义吧，根据位数排序数次快的很，因为你不需要排它，你只需要做10个容器，编号为0-9，你要排序的位数上，数字是几就把整个数字丢到对应编号的容器里，自然就实现了排序，因为0-9本身就是个排好了序的数组。</p><blockquote>
<p>你甚至可以用字典，key就是0到9，但数组天生自带了数字Index，何乐而不为？</p></blockquote>
<p>演示：385, 17, 45, 26, 72, 1265, 用个位数字排序，排好后的容器（数组）应该是：</p><div class="highlight"><pre><span></span><span class="p">[</span>
    <span class="p">[],</span>
    <span class="p">[],</span>
    <span class="p">[</span><span class="mi">72</span><span class="p">],</span>
    <span class="p">[],</span>
    <span class="p">[],</span>
    <span class="p">[</span><span class="mi">835</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">1265</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">26</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">17</span><span class="p">],</span>
    <span class="p">[],</span>
    <span class="p">[]</span>
<span class="p">]</span>
</pre></div>
<p>其实这也是排序，和接下来要讲的插入排序很像。它没有查找的过程，时间复杂度为0。上面剧透的shell排序还没讲，又剧透了另一个。</p><p>别的就没啥好说的了，由低位到高位循环就是了。</p><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_number</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    提取指定位数数字的方法：</span>
<span class="sd">    个位：527 % 10^1 // 10^0 = 7</span>
<span class="sd">    十位：527 % 10^2 // 10^1 = 2</span>
<span class="sd">    百位：527 % 10^3 // 10^2 = 5</span>
<span class="sd">    千位：527 % 10^4 // 10^3 = 0</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="n">num</span> <span class="o">%</span> <span class="mi">10</span><span class="o">**</span><span class="p">(</span><span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">10</span><span class="o">**</span><span class="n">index</span>

<span class="k">def</span> <span class="nf">digit_length</span><span class="p">(</span><span class="n">number</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">math</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">number</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">10</span> <span class="k">else</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">number</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">digit_sort</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    对第index个数字进行排序</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">):</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">([])</span> <span class="c1"># [[]] * 10 会造成引用传递</span>
    <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">arr</span><span class="p">:</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">get_number</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">num</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">digit</span> <span class="k">for</span> <span class="n">sublist</span> <span class="ow">in</span> <span class="n">results</span> <span class="k">for</span> <span class="n">digit</span> <span class="ow">in</span> <span class="n">sublist</span><span class="p">]</span>  <span class="c1"># flatten the 2-d array</span>

<span class="k">def</span> <span class="nf">radix_sort</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="n">length</span> <span class="o">=</span> <span class="n">digit_length</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">arr</span><span class="p">))</span> <span class="c1"># 演示如何从数学上取得数字的长度（几十万次迭代效率只有毫米级的差别）</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">length</span><span class="p">):</span>
        <span class="n">arr</span> <span class="o">=</span> <span class="n">digit_sort</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">arr</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
    <span class="kn">import</span> <span class="nn">time</span>
<span class="c1">#     np.random.seed(7)</span>
<span class="c1">#     length = 20000</span>
<span class="c1">#     arr = list(np.random.randint(0, length*50, size=(length,)))</span>
    <span class="n">arr</span> <span class="o">=</span> <span class="p">[</span><span class="mi">954</span><span class="p">,</span><span class="mi">354</span><span class="p">,</span><span class="mi">309</span><span class="p">,</span><span class="mi">411</span><span class="p">]</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">radix_sort</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="p">,</span> <span class="n">s</span><span class="p">[:</span><span class="mi">100</span><span class="p">])</span>
</pre></div>
<p>output:</p>
<pre><code>0.0008242130279541016
[309, 354, 411, 954]
</code></pre>
<h2>插入排序</h2>
<p>准备一个空数组，依次把原数组的每一个数插入到该数组里的适当位置。上面说的基数排序里的按位初排就有点类似插入排序，只不过基数排序里不需要比较大小（即235， 15， 1375）这样的数，如果看个位，都是在索引5的位置，且无序），而且插入的位置是固定的，所以没有时间复杂度。</p><p>而插入排序则实实在在地要在排入的数组里遍历才能找到正确的插入位置，越排到后面，新数组就越长，时间复杂度也就越来越大了。</p><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">insert_sort</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="n">rst</span> <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">arr</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
        <span class="n">found</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">rst</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">j</span> <span class="o">&gt;</span> <span class="n">i</span><span class="p">:</span>
                <span class="n">rst</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="c1"># 排到第一个比它大的前面</span>
                <span class="n">found</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">break</span><span class="p">;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">found</span><span class="p">:</span>
            <span class="n">rst</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">rst</span>

<span class="n">insert_sort</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">34</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
<p>output</p>
<pre><code>[0, 1, 5, 6, 9, 34]
</code></pre>
<h2>希尔排序</h2>
<ol>
<li><code>归并排序</code>是化整为零，两两比较后再组合，分组越来越大，最终变成一组</li>
<li>希尔排序是一开始就对半分（注：如果不能整除，如11//2=5, 这样会有3组），每一组相同位置的数做比较，实现一轮过后分组间<code>同位置的数</code>是顺序排列的</li>
<li>每组元素再减半，就上一条来说是(5//2=2，即上一层一组5个，下一轮每组就只有2个了)，以此往复，让组数越来越多，组内元素却越来越少，极端情况就是每组只有1个了，再参考前面总结的“<strong>分组间同位置的数是顺序排列的</strong>”这一结论，说明整个数组已经排好序了（退出条件get）。这个思路妙不妙？</li>
</ol>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">shell_sort</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="n">group</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>

    <span class="k">while</span> <span class="n">group</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)):</span>
            <span class="n">right</span>   <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">current</span> <span class="o">=</span> <span class="n">i</span>
            <span class="k">while</span> <span class="n">current</span> <span class="o">&gt;=</span> <span class="n">group</span> <span class="ow">and</span> <span class="n">arr</span><span class="p">[</span><span class="n">current</span> <span class="o">-</span> <span class="n">group</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">right</span><span class="p">:</span>
                <span class="n">arr</span><span class="p">[</span><span class="n">current</span><span class="p">]</span> <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">current</span> <span class="o">-</span> <span class="n">group</span><span class="p">]</span>
                <span class="n">current</span> <span class="o">-=</span> <span class="n">group</span>
            <span class="n">arr</span><span class="p">[</span><span class="n">current</span><span class="p">]</span> <span class="o">=</span> <span class="n">right</span>
        <span class="n">group</span> <span class="o">//=</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">arr</span>

<span class="n">shell_sort</span><span class="p">([</span><span class="mi">34</span><span class="p">,</span><span class="mi">24</span><span class="p">,</span><span class="mi">538</span><span class="p">,</span><span class="mi">536</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
<p>output</p>
<pre><code>[1, 24, 34, 536, 538]
</code></pre>
<hr />
<p>最后，生成可重复的随机数测几轮， quick sort要快一些：</p><div class="highlight"><pre><span></span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
    <span class="kn">import</span> <span class="nn">time</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
    <span class="n">length</span> <span class="o">=</span> <span class="mi">20000</span>
    <span class="n">arr</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">length</span><span class="o">*</span><span class="mi">50</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">length</span><span class="p">,)))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">length</span><span class="si">}</span><span class="s1"> random integers sort comparation:&#39;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;-------------round </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">------------&#39;</span><span class="p">)</span>
        <span class="c1"># insert is too slow</span>
        <span class="c1"># or my implementation is not so good</span>
<span class="c1">#         start = time.time()</span>
<span class="c1">#         s1 = insert_sort(arr)</span>
<span class="c1">#         print(f&quot;insert_sort\t {time.time()-start:.5f} seconds&quot;)</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">s2</span> <span class="o">=</span> <span class="n">quick_sort</span><span class="p">(</span><span class="n">arr</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;quick_sort</span><span class="se">\t</span><span class="s2"> </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">s3</span> <span class="o">=</span> <span class="n">shell_sort</span><span class="p">(</span><span class="n">arr</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;shell_sort</span><span class="se">\t</span><span class="s2"> </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">s4</span> <span class="o">=</span> <span class="n">heap_sort</span><span class="p">(</span><span class="n">arr</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;heap_sort</span><span class="se">\t</span><span class="s2"> </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">s5</span> <span class="o">=</span> <span class="n">merge_sort</span><span class="p">(</span><span class="n">arr</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;merge_sort</span><span class="se">\t</span><span class="s2"> </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">s6</span> <span class="o">=</span> <span class="n">radix_sort</span><span class="p">(</span><span class="n">arr</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;radix_sort</span><span class="se">\t</span><span class="s2"> </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;first 10 numbers:</span><span class="se">\n</span><span class="si">{</span><span class="n">s2</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="si">{</span><span class="n">s3</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="si">{</span><span class="n">s4</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="si">{</span><span class="n">s5</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="si">{</span><span class="n">s6</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
<p>output</p>
<pre><code>20000 random integers sort comparation:
-------------round 1------------
quick_sort	 0.07970 seconds
shell_sort	 0.17623 seconds
heap_sort	 0.32919 seconds
merge_sort	 0.20177 seconds
radix_sort	 0.18000 seconds
-------------round 2------------
quick_sort	 0.05894 seconds
shell_sort	 0.15423 seconds
heap_sort	 0.28844 seconds
merge_sort	 0.20043 seconds
radix_sort	 0.19310 seconds
-------------round 3------------
quick_sort	 0.06169 seconds
shell_sort	 0.18299 seconds
heap_sort	 0.33159 seconds
merge_sort	 0.20836 seconds
radix_sort	 0.20003 seconds
-------------round 4------------
quick_sort	 0.05780 seconds
shell_sort	 0.15414 seconds
heap_sort	 0.26780 seconds
merge_sort	 0.18810 seconds
radix_sort	 0.17084 seconds
</code></pre>
</div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/%E5%87%A0%E5%A4%A7%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95python%E5%AE%9E%E7%8E%B0/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/%E6%88%91%E7%9A%84%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/" target="_self">我的知识图谱入门笔记</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/%E6%88%91%E7%9A%84%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/" target="_self">
                <time class="text-uppercase">
                    June 17 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><h1>Knowledge Graph</h1>
<ul>
<li>信息是指外部的客观事实。举例：这里有一瓶水，它现在是7°。</li>
<li>知识是对外部客观规律的归纳和总结。举例：水在零度的时候会结冰。</li>
</ul>
<p>换句话说，知识图谱是由一条条知识组成，每条知识表示为一个SPO三元组(Subject-Predicate-Object)。</p><p>$\boxed{Subject} \xrightarrow{Predicate} \boxed{Object}$</p><h1>语义网络(Semantic Network)</h1>
<p>语义网络由相互连接的节点和边组成，节点表示概念或者对象，边表示他们之间的关系(is-a关系，比如：猫是一种哺乳动物；part-of关系，比如：脊椎是哺乳动物的一部分)</p><p>。在表现形式上，语义网络和知识图谱相似，但语义网络更侧重于描述概念与概念之间的关系，（有点像生物的层次分类体系——界门纲目科属种），而知识图谱则更偏重于描述实体之间的关联。</p><h1>RDF(Resoure Description Framework)</h1>
<p>RDF(Resource Description Framework)，即资源描述框架，是W3C制定的，用于描述实体/资源的标准数据模型。RDF图中一共有三种类型，International Resource Identifiers(IRIs)，blank nodes 和 literals。下面是SPO每个部分的类型约束：</p><ul>
<li>Subject可以是IRI或blank node。可以理解为<code>URI</code></li>
<li>Predicate是IRI。</li>
<li>Object三种类型都可以。</li>
</ul>
<p>也就是说字面量不能做主语？</p><p>将罗纳尔多的原名与中文名关联起来的RDF表示：</p><p>$\boxed{www.kg.com/person/1} \xrightarrow{kg:chineseName} \boxed{罗纳尔多·路易斯·纳扎里奥·达·利马}$</p><blockquote>
<p>可见，主语的指代性要强很多，所以字面量（宾语）用作主语会丧失这种精确性（唯一性）。</p></blockquote>
<ul>
<li>&quot;<code>www.kg.com/person/1</code>&quot;是一个IRI，用来唯一的表示“罗纳尔多”这个实体。&quot;kg:chineseName&quot;也是一个IRI，用来表示“中文名”这样一个属性。&quot;kg:&quot;是RDF文件中所定义的prefix，如下所示。</li>
<li>@<code>prefix kg</code>: <a href="http://www.kg.com/ontology/">http://www.kg.com/ontology/</a> 即，kg:chineseName其实就是&quot;http:// www.kg.com/ontology/chineseName&quot;的缩写。</li>
</ul>
<p>这样知识图谱的正确表示其实是：</p><figure  style="flex: 81.6618911174785" ><img width="1140" height="698" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/055b61a7e0a74762f15ece471ec22ee8.png" alt=""/></figure><p>而不是网传的简单的画几个对象连几根线，也就是说，能用URI表示的，尽量都用URI表示。</p><h1>Identifying graph-shaped problems(应用场景)</h1>
<ul>
<li><p>does our problem involve understanding <code>relationships</code> between entities?</p><ul>
<li>Recommendations</li>
<li>Next best action</li>
<li>Fraud detection</li>
<li>Identity resolution</li>
<li>Data lineage</li>
</ul>
</li>
<li><p>does our problem involve a lot of <code>self-referencing</code> to the same type of entity?</p><ul>
<li>Organisational hierachies</li>
<li>Social influencers</li>
<li>Friends of friends</li>
<li>Churn detection</li>
</ul>
</li>
<li><p>does the problem explore <code>relationships of varying or unknown depth</code>?</p><ul>
<li>Supply chain visibility</li>
<li>Bill of  Materials(BOM)</li>
<li>Network management</li>
</ul>
</li>
<li><p>does our problem involve discovering lots of <code>different routers or paths</code>?</p><ul>
<li>Logistics and routing</li>
<li>Infrastructure management</li>
<li>Dependency tracing</li>
</ul>
</li>
</ul>
<h1>Neo4j</h1>
<figure  style="flex: 88.44507845934379" ><img width="1240" height="701" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/1da110e1163eb31331ea2c21528a4ae2.png" alt=""/></figure><ol>
<li><code>:person</code>, <code>:Car</code>, <code>:Vehicle</code> are <code>Label</code></li>
<li>even <code>relationship</code> can also have(own) properties</li>
</ol>
<h2>AsciiArt</h2>
<h3>for Nodes</h3>
<p><code>(p:Person:Mammal{name:'walker'})</code></p><h3>for Relationships</h3>
<p><code>- [:HIRED {type: 'fulltime'}] -&gt;</code></p><h2>CRUD</h2>
<h3>Create</h3>
<figure  style="flex: 87.94326241134752" ><img width="1240" height="705" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/1664c809c2f78ab7dc14392c1ef00ac9.png" alt=""/></figure><h4>Constraints</h4>

<pre><code>CREATE  CONSTRAINT ON (p:Person)
ASSERT p.name IS UNIQUE
</code></pre>
<p>所以如下语句会报错：</p>
<pre><code>CREATE (a:Person {name: &quot;Ann&quot;})
CREATE (a) - [:HAS_PET] -&gt; (:Dog {name: &quot;Sam&quot;})
</code></pre>
<p>要用<code>merge</code></p>
<pre><code>MERGE (a:Person {name: &quot;Ann&quot;})
CREATE (a) - [:HAS_PET] -&gt; (:Dog {name: &quot;Sam&quot;})
</code></pre>
<h4>Set</h4>
<p>属性可以用<code>JSON</code>格式写，也可以用<code>SET</code>语法（下面的查询语句也是一样）</p>
<pre><code>MERGE (a:Person {name: &quot;Ann&quot;})
ON CREATE SET
    a.twitter = &quot;@ann&quot;
MERGE (a) - [:HAS_PET] -&gt; (:Dog {name: &quot;Sam&quot;})
</code></pre>
<p>同时，看到了吗？<code>create</code>只能出现一次（同一个对象的话）</p><h3>Read</h3>
<blockquote>
<p>who drives a car owned by a lover?</p></blockquote>
<div class="highlight"><pre><span></span><span class="k">MATCH</span><span class="w"></span>
<span class="p">(</span><span class="n">p1</span><span class="p">:</span><span class="n">Person</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">[:</span><span class="n">DRIVES</span><span class="p">]</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">(</span><span class="k">c</span><span class="p">:</span><span class="n">Car</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">[:</span><span class="n">OWNED_BY</span><span class="p">]</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">(</span><span class="n">p2</span><span class="p">:</span><span class="n">Person</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="p">[:</span><span class="n">LOVES</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">(</span><span class="n">P1</span><span class="p">)</span><span class="w"></span>
<span class="k">RETURN</span><span class="w"></span>
<span class="n">p1</span><span class="w"></span>
</pre></div>
<p>其中，因为问的是lover，没有指向性，所以如果是两个互相相爱，后半截也可以是p2指向p1</p>
<pre><code>match p = (n) -[*1..2] -&gt; (m) where n.name='特朗普' return p
match p =  ({name: '特朗普'}) - [*1..2] -&gt; () return p # 简化
match p = (n)-[m]-&gt;(q) where m.name = '丈夫' return n,q skip 10 limit 5
match p = (n)-[:丈夫]-&gt;(q) return n,q skip 10 limit 5 # 简化
</code></pre>
<p>解读：</p><ol>
<li>上面写法很简略，注意观察一下</li>
<li>又一次演示了直接用json来做where和单独用<code>where</code>关键字的写法区别（要多命名一个变量）</li>
<li>p跟n的区别，p是返了整个网络，如果<code>return n</code>，那么就是n自身(一个节点）。</li>
</ol>
<p>4, 但是如果<code>return n, m</code>，那么又把一层网络给select出来了 
5. [*1..2]表示跟踪两层
6. 如果不需要对n,m进行where,set操作，可以不设置变量
7. <code>relationship</code>也可以过滤，也有name等属性
8. <code>limit</code>, <code>skip</code> 等用法</p><h4>Tabular Results</h4>
<div class="highlight"><pre><span></span><span class="k">MATCH</span><span class="w"> </span><span class="p">(</span><span class="n">p</span><span class="p">:</span><span class="n">Person</span><span class="w"> </span><span class="err">{</span><span class="n">name</span><span class="p">:</span><span class="w"> </span><span class="ss">&quot;Tom Hanks&quot;</span><span class="err">}</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">[</span><span class="n">r</span><span class="p">:</span><span class="n">ACTED_IN</span><span class="o">|</span><span class="n">DIRECTED</span><span class="p">]</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">(</span><span class="n">m</span><span class="p">:</span><span class="n">Movie</span><span class="p">)</span><span class="w"></span>
<span class="k">RETURN</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="n">r</span><span class="p">,</span><span class="n">m</span><span class="w"></span>
<span class="k">RETURN</span><span class="w"> </span><span class="n">p</span><span class="p">.</span><span class="n">name</span><span class="p">,</span><span class="w"> </span><span class="k">type</span><span class="p">(</span><span class="n">r</span><span class="p">),</span><span class="w"> </span><span class="n">m</span><span class="p">.</span><span class="n">title</span><span class="w"></span>
</pre></div>
<p>前者返回Graph，后者返回表格数据</p><h3>Update</h3>
<blockquote>
<p>P.S. <code>where</code>是对属性做限制，所以查询条件既可以写在属性里，也可以用<code>where</code>语句来做过滤.</p></blockquote>
<p>要对查询结果进行修改，用<code>set</code>（有则改，无则加）</p>
<pre><code>MATCH
(:Person {name: &quot;张三&quot;}) - [:LOVES] -&gt; (p:Person)
RETURN
p

MATCH
(p1:Person) - [:LOVES] -&gt; (p2:Person)
WHERE
p1.name = &quot;张三&quot;
SET
p2.age = 33  # set by property
# or
p2 += {age: 33, height: 180}  # set by JSON
RETURN
p2
</code></pre>
<p>可以把<code>Neo4j</code>理解为命名实体识别(<code>NER</code>)，即你创造一句话，为句子里的每个实体打上标签，然后你想要谁就用实体标签把它取出来。</p><p>比如“张三爱李四”：</p>
<pre><code>step1: 写框架
CREATE () - [] -&gt; ()
step2: 填节点和关联
CREATE (:Person {name: &quot;张三&quot;}) - [:LOVES] -&gt; (:Person {name: &quot;李四&quot;})
</code></pre>
<p>而你要问张三爱谁:</p>
<pre><code>MATCH
(:Person {name: &quot;张三&quot;}) - [:LOVES] -&gt; (p:Person)
RETURN
p
</code></pre>
<p>看到查询语句了吗？除了<code>CREATE</code>, <code>MATCH</code>等关键词，句子顺序是完全一样的，也就是说，一直是在“<strong>陈述</strong>”一件事。</p><p>而事实上，这个<code>NER</code>在知识图谱中表示为<code>RDF</code>。</p><h3>Delete</h3>
<div class="highlight"><pre><span></span><span class="k">match</span><span class="w"> </span><span class="n">p</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="k">where</span><span class="w"> </span><span class="n">n</span><span class="p">.</span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;小明&#39;</span><span class="w"> </span><span class="n">detach</span><span class="w"> </span><span class="k">delete</span><span class="w"> </span><span class="n">n</span><span class="w"></span>
</pre></div>
<h2>Query</h2>
<h3>最短距离</h3>
<div class="highlight"><pre><span></span><span class="k">MATCH</span><span class="w"></span>
<span class="w">  </span><span class="p">(</span><span class="n">martin</span><span class="p">:</span><span class="n">Person</span><span class="w"> </span><span class="err">{</span><span class="n">name</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;Martin Sheen&#39;</span><span class="err">}</span><span class="p">),</span><span class="w"></span>
<span class="w">  </span><span class="p">(</span><span class="n">oliver</span><span class="p">:</span><span class="n">Person</span><span class="w"> </span><span class="err">{</span><span class="n">name</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;Oliver Stone&#39;</span><span class="err">}</span><span class="p">),</span><span class="w"></span>
<span class="w">  </span><span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">shortestPath</span><span class="p">((</span><span class="n">martin</span><span class="p">)</span><span class="o">-</span><span class="p">[</span><span class="o">*</span><span class="p">..</span><span class="mi">15</span><span class="p">]</span><span class="o">-</span><span class="p">(</span><span class="n">oliver</span><span class="p">))</span><span class="w"> </span><span class="o">#</span><span class="w"> </span><span class="mi">1</span><span class="w"></span>
<span class="w">  </span><span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">allShortestPath</span><span class="p">(....)</span><span class="w"> </span><span class="o">#</span><span class="w"> </span><span class="mi">2</span><span class="w"></span>
<span class="w">  </span><span class="k">WHERE</span><span class="w"> </span><span class="k">none</span><span class="p">(</span><span class="n">r</span><span class="w"> </span><span class="k">IN</span><span class="w"> </span><span class="n">relationships</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="w"> </span><span class="k">WHERE</span><span class="w"> </span><span class="k">type</span><span class="p">(</span><span class="n">r</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;FATHER&#39;</span><span class="p">)</span><span class="w"> </span><span class="o">#</span><span class="w"> </span><span class="mi">3</span><span class="w"></span>
<span class="k">RETURN</span><span class="w"> </span><span class="n">p</span><span class="w"></span>
</pre></div>
<ol>
<li>限定了15层</li>
<li>Finds <code>all</code> the shortest paths between two nodes.</li>
<li>排除了关系<code>type</code>为<strong>FATHER</strong>的</li>
</ol>
<p>给你们看一下一个<code>relationships</code>长啥样：</p><div class="highlight"><pre><span></span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="nt">&quot;identity&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">36629</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="nt">&quot;start&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">31343</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="nt">&quot;end&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">33922</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;author-&gt;title&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="nt">&quot;properties&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;author-&gt;title&quot;</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
<h3>模糊匹配(%)</h3>
<div class="highlight"><pre><span></span><span class="k">match</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">-</span><span class="p">[</span><span class="o">*</span><span class="mi">1</span><span class="p">..</span><span class="mi">2</span><span class="p">]</span><span class="o">-&gt;</span><span class="p">(</span><span class="n">b</span><span class="p">:</span><span class="n">content</span><span class="p">)</span><span class="w"> </span><span class="k">where</span><span class="w"> </span><span class="n">a</span><span class="p">.</span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;李白&#39;</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">b</span><span class="p">.</span><span class="n">name</span><span class="w"> </span><span class="o">=~</span><span class="w"> </span><span class="s1">&#39;.*明月.*&#39;</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="w"></span>
</pre></div>
<p><code>.*</code>就相当于sql里的<code>%</code>吧</p><h3>get by id</h3>

<pre><code>MATCH (n)
WHERE id(n) IN [0, 3, 5]
RETURN n
</code></pre>
<ol>
<li>id(n) -&gt; search with id</li>
<li>multiple id use <code>in</code></li>
</ol>
<h3>outer join (optional relationships)</h3>
<div class="highlight"><pre><span></span><span class="k">match</span><span class="w"> </span><span class="p">(</span><span class="n">a</span><span class="p">:</span><span class="n">author</span><span class="w"> </span><span class="err">{</span><span class="n">name</span><span class="p">:</span><span class="s1">&#39;李白&#39;</span><span class="err">}</span><span class="p">)</span><span class="w"></span>
<span class="n">optional</span><span class="w"> </span><span class="k">match</span><span class="w"> </span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="c1">--&gt;(b)</span>
<span class="k">return</span><span class="w"> </span><span class="n">b</span><span class="w"></span>
</pre></div>
<p>在实例中，b包含了两种实例：</p><ol>
<li>title</li>
<li>introduce</li>
</ol>
<p>等同于sql中user表outer join了两个表(title, introduce)</p></div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/%E6%88%91%E7%9A%84%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/HMM%E3%80%81NER%E3%80%81PoS%E3%80%81Viterbi%E7%AC%94%E8%AE%B0/" target="_self">HMM、NER、PoS、Viterbi笔记</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/HMM%E3%80%81NER%E3%80%81PoS%E3%80%81Viterbi%E7%AC%94%E8%AE%B0/" target="_self">
                <time class="text-uppercase">
                    June 14 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><p>开局一句话，隐马尔可夫，就是在“溯源”，即产生你这个现象的源头在哪。</p><ul>
<li>比如你掷出的这个显示为6的骰子，是来自于六面体的还是四面体的，或是来自于普通的还是灌铅了的</li>
<li>又比如你一句话里的某一个词，它是处于开始位置还是中间位置，或是它是一个人名还是一个地点或是一个介词</li>
</ul>
<p>任何一种表现形式，都有一个它的“原因”或“属性”。 现在正式开始，来自我能理解的网络资料，我的课程，以及一些思考</p><p>首先几个基础概念：</p><h1>命名实体识别(NER)</h1>
<p><strong>实体</strong>：人物(PER)，地点(LOC)，等
<strong>BIOES</strong>: 开始(Begin)， 中间(Inner)， 结尾(E)，单个(Single)，其它(Other)</p><p>比如人名：张北京，就可以被识别为$\Rightarrow$ B-PER, I-PER, E-PER</p><h1>Part-of-Speech Tagging（词性标注）</h1>
<p>词性标注是为输入文本中的每个词性标注词分配词性标记的过程。标记算法的输入是一系列(标记化的)单词和标记集，输出是一系列标记，每个标记一个。</p><p>标记是一项消除歧义的任务;单词是模糊的，有不止一个可能的词性(歧义)，我们的目标是为这种情况找到正确的标签。例如，book可以是动词(book that flight)，也可以是名词(hand me that book)。That可以是一个限定词(Does that flight serve dinner)，也可以是一个补语连词(I thought that your flight was earlier)。后置标记的目标是解决这些分辨率模糊，为上下文选择合适的标记</p><h1>Sequence model</h1>
<p>Sequence models are central to NLP: they are models where there is some sort of <code>dependence through time</code> between your inputs.</p><ul>
<li>The classical example of a sequence model is the <code>Hidden Markov Model</code> for <strong>part-of-speech tagging</strong>. (词性标注)</li>
<li>Another example is the <code>conditional random field</code>.</li>
</ul>
<p>HMM模型的典型应用是词性标注</p><figure  style="flex: 94.51219512195122" ><img width="1240" height="656" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/8246ff19ee962171c5d3b15abd234eec.png" alt=""/></figure><p>词性标注语料库是统计标注算法的关键训练(和测试)集。三个主要的标注语料库始终用于训练和测试英语词性标注器。</p><ol>
<li>布朗语料库是1961年在美国出版的500篇不同体裁的书面文本的100万单词样本。</li>
<li>《华尔街日报》语料库收录了1989年发表在《华尔街日报》上的100万个单词。</li>
<li>总机语料库由1990-1991年收集的200万字电话对话组成。语料库的创建是通过在文本上运行一个自动的词性标记，然后由人工注释器手工更正每个标记。</li>
</ol>
<h1>HMM</h1>
<p>HMM是一个序列模型(<code>sequence model</code>)。序列模型或序列分类器是一个模型，其工作是为序列中的每个单元分配一个标签或类，从而将一个观察序列(观察状态)映射到一个标签序列(隐藏状态)。HMM是一种概率序列模型：给定一个单位序列(单词、字母、语素、句子等等)，它计算可能的标签序列的概率分布，并选择最佳标签序列。</p><ul>
<li>3个骰子，6面体，4面体，8面体(D6, D4, D8)</li>
<li>每次随机选出一个骰子投掷，得到一个数字</li>
<li>共十次，得到10个数字</li>
</ul>
<ol>
<li><code>可见状态链</code>：10次投掷得到10个数字(1,3,5...)$\Rightarrow$对应你看得的10个单词</li>
<li><code>隐含状态链</code>：每一次投掷都有可能拿到三种骰子之一，(D6, D6, D4...) $\Rightarrow$对应为每个单词的词性</li>
<li>转换概率（<code>transition probability</code>）：隐含状态之间的概率($\Rightarrow$对应为语法)：<ul>
<li>每一次拿到某种骰子之后，下一次拿到三种骰子的概率（[1/3,1/3,1/3],...)</li>
<li>或者说主动决策下一次用哪个骰子的概率[a,b,c...] (相加为1)</li>
</ul>
</li>
<li>可见状态之间没有转换概率</li>
<li>输出概率（<code>emission probability</code>）：隐含状态和可见状态之间的概率，比如D4下1的概率为1/4，D6下为1/6 (表现概率，激发概率，多种翻译)</li>
</ol>
<figure  style="flex: 106.16438356164383" ><img width="1240" height="584" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/1c9b14ebecf2d171b7f511296c425412.png" alt=""/></figure><p>应用HMM模型时候，往往是缺失了一部分信息的，</p><ul>
<li>有时候你知道骰子有几种，每种骰子是什么，但是不知道掷出来的骰子序列；</li>
<li>有时候你只是看到了很多次掷骰子的结果，剩下的什么都不知道。</li>
</ul>
<p>如何应用算法去估计这些缺失的信息，就成了一个很重要的问题，这也是HMM模型能做的几件事：</p><h2>Decoding</h2>
<p>解码的过程就是在给出一串序列和已知HMM模型的情况下，找到最可能的隐性状态序列。</p><p>比如结果是：1 6 3 5 2 7 3 5 2 4, 求最可能的骰子序列</p><h3>Viterbi algorithm</h3>
<ol>
<li>掷出1的最大概率是4面体： P1(D4) = P(1|D4) * P(D4) = 1/4 * 1/3</li>
<li>掷出6的最大概率是 P2(D6) = P(6|D6) * P(D6) = 1/6 * 1/3</li>
<li>连续1，6的概率就成了1的概率 * 2的概率 P2(D6) = P1(D4) * P2(D6) = 1/216</li>
<li>1,6,3 =&gt; P3(D4) = P2(D6) * P(3|D4) * P(D4) = $\frac{1}{216} \cdot \frac{1}{3} \cdot \frac{1}{4}$</li>
<li>and so on</li>
<li>但这个例子忽略了转移概率，即P(D6|D4), P(D4|D6,D4)，或者说默认了转移概率就是1/3，即每次挑中三个骰子的机率均等。</li>
</ol>
<h2>Evaluation</h2>
<p>根据条件和序列结果求这一序列的概率是多少，比如三种骰子，投出了1，6，3的结果：</p><figure  style="flex: 169.86301369863014" ><img width="1240" height="365" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/3008ce6fdd893286e56d1b7f9ad1a342.png" alt=""/></figure><ul>
<li>第1列表示第一次投掷得到1的可能性和为0.18</li>
<li>第2列为1 6的的可能性和为0.05</li>
<li>第3列为1 6 3的可能性和为0.03</li>
</ul>
<p>如果远低于或远高于这个概率，必然有做过手脚的骰子。</p><h2>转移概率的矩阵表示</h2>
<p>这次假定不同的骰子是用来作弊的，作弊者会根据情况来挑选骰子，这样转移概率就不可能是均等的了：</p><figure  style="flex: 83.33333333333333" ><img width="500" height="300" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/8259518e5781e3ce798778e8da69de85.png" alt=""/></figure><p>很幸运，这么复杂的概率转移图，竟然能用矩阵表达：</p><p>$$A = 
\begin{bmatrix}
0.15 &amp; 0.45 &amp; 0.4 \\
0.25 &amp; 0.35 &amp; 0.4 \\
0.10 &amp; 0.55 &amp; 0.35
\end{bmatrix}
$$</p>
<p>既然是3行3列，显然$A_{ij}$就是从i切换到j的概率，比如$A_{12}$ 就应该是这个人把骰子从作弊骰子1切换到2的概率。</p><figure  style="flex: 102.04081632653062" ><img width="500" height="245" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/653c3cb7f8c2f3a541d170421fe489bf.png" alt=""/></figure><p>相应地，发射概率（即不同骰子摇出的点数的概率）也能表示为矩阵：</p><p>$$B = 
\begin{bmatrix}
0.16 &amp; 0.16 &amp; 0.16 &amp; 0.16 &amp; 0.16 &amp; 0.16 \\
0.02 &amp; 0.02 &amp; 0.02 &amp; 0.02 &amp; 0.02 &amp; 0.90 \\
0.40 &amp; 0.20 &amp; 0.25 &amp; 0.05 &amp; 0.05 &amp; 0.05 \\
\end{bmatrix}
$$</p>
<p>现在有了转移概率和发射概率，我们再来看看前面的掷出1，6，3的骰子的概率：
骰子设为D1 D2 D3, 每一轮的可能性为P1 P2 P3, 则P = P3D1 + P3D2 + P3D3 即第3轮时3种骰子能投出3的概率和</p><p>我来推导一下P3D1怎么来的，上面的表格是我从别人的博客里复制的，这里就不做一个一模一样的图了，我们一步步来吧：</p><ul>
<li>第一次投掷每个骰子的概率应该是隐含了各为1/3吧？(这个好像叫&quot;<code>初始隐状态</code>&quot; $\pi$)</li>
<li>P1D1 = 0.16 * 0.33, 即1/3概率拿到D1，0.16概率投出1，同理：<ul>
<li>P1D2 = 0.02 * 0.33</li>
<li>P1D3 = 0.40 * 0.33</li>
</ul>
</li>
<li>P2D1 =<ul>
<li>P1D1 * $A_{00}$ * $B_{05}$ = P1D1 * 0.15 * 0.16 即P1D1前提下，乘上D1换到D1的概率，再乘上D1选出6的概率</li>
<li>$+$</li>
<li>P1D2 * $A_{10}$ * $B_{05}$ = P1D1 * 0.25 * 0.16 即P1D2前提下，乘上D2换到D1的概率，再乘上D1选出6的概率</li>
<li>$+$</li>
<li>P1D3 * $A_{20}$ * $B_{05}$ = P1D1 * 0.10 * 0.16 即P1D3前提下，乘上D3换到D1的概率，再乘上D1选出6的概率</li>
<li>以此类推得到P2D2, P2D3</li>
</ul>
</li>
<li>P3D2 = （<em>D1的概率太平均，这次换个D2来演示</em>）<ul>
<li>P2D1 * $A_{01}$ * $B_{12}$ = P2D1 * 0.45 * 0.02 即P2D1前提下，乘上D1换到D2的概率，再乘上D2选出3的概率</li>
<li>$+$</li>
<li>P2D2 * $A_{11}$ * $B_{12}$ = P2D1 * 0.35 * 0.02 即P2D2前提下，乘上D2换到D2的概率，再乘上D2选出3的概率</li>
<li>$+$</li>
<li>P2D3 * $A_{21}$ * $B_{12}$ = P2D1 * 0.35 * 0.02 即P2D3前提下，乘上D3换到D2的概率，再乘上D2选出3的概率</li>
<li>以此类推得到P3D1, P3D2</li>
</ul>
</li>
<li>P = P3D1 + P3D2 + P3D3</li>
</ul>
<p>$$
\sum_{r\in R}\prod_t^TP(v(t)|w_r(t)) | w_r(t-1))
$$</p>
<ul>
<li>v: visible 可见序列</li>
<li>w: 隐性状态序列</li>
<li>R: 所有隐状态的可能性</li>
</ul>
<ol>
<li>t-1隐状态前提下得到t的概率（转移概率）如D2换到D3的概率</li>
<li>上一概率前提下得到v(t)的概率，如D3扔出1的概率</li>
<li>一种隐状态下出序列的结果为累乘</li>
<li>所有隐状态下出该序列的结果为3的累加</li>
</ol>
<p>简单来说：</p><ol>
<li>可见序列$v(t)$的概率依赖当前$t$下的隐状态（比如是不是作弊了的骰子）$w_r(t)$<ul>
<li>得到：$P(v(t)\ \color{red}|\ w_r(t))$</li>
</ul>
</li>
<li>当前隐状态$w_r(t)$又有两个特征:<ol>
<li>由$w_r(t-1)$转换而来的: $P(v(t)|w_r(t))\color{red}{|}w_r(t-1)$</li>
<li>$T$是链式的，概率累乘： $\color{red}{\prod_t^T}P(v(t)|w_r(t)) | w_r(t-1))$</li>
</ol>
</li>
<li>最后一步时的隐状态显然是几种之一，累加起来就是所有可能性：<ul>
<li>$\color{red}{\sum_{r\in R}}\prod_t^TP(v(t)|w_r(t)) | w_r(t-1))$</li>
</ul>
</li>
</ol>
<h1>应用</h1>
<ol>
<li>初始概率</li>
</ol>
<p>以<code>BMES</code>为例（参考NER），把其认为是隐状态，然后认为每个词（里的字）是由隐状态产生的。</p><p>即<code>B</code>对应的字可能有“<code>中</code>”，“<code>国</code>”，等等，能作为词语打头的字都可能由隐状态<code>B</code>产生，其它状态依次类推。</p><p>就像我们三种骰子的初始概率，完全取决于每种骰子占总数的多少一样，HHM应用到语言模型里，初始概率就是先把文字全部用<code>BMES</code>表示，然后分别数出个数，与总数做个对比。（此时已经可以判断出<code>M</code>和<code>E</code>的概率只能是0了。</p><ol start="2">
<li>转移概率</li>
</ol>
<p>应该是4个循环吧，每次把当前状态后面跟上四个状态的情况都数出来，就是一个隐状态到其它四个状态的转移概率，四行拼到一起就是一个转移概率的矩阵，类似上面的三种骰子互相切换的矩阵。</p><p>也可以用字典，比如 BE BS BB BM等共16个键，两两遍历整个字符串完后，16个count就出来了，group后就能得到概率了。</p><ol start="3">
<li>观测概率（发射概率）</li>
</ol>
<p>这个就是每一个隐状态下对应不同表面文字的概率了，比如：{s:{&quot;周&quot;: 0.3357, &quot;爬&quot;:0.00003}...}</p><p>要知道，三种概率里面是有很多0的，意思就是在现有的语法体系里面不可能出现的场景，比如第一个字不可能是M和E，B后面不可能跟S，B，而M后面不可能跟B，S，以及S后面不可能跟M，E等，再比如假如哪个字永远不可能是第一个字，那么它的观测概率在S里面就永远是0，等等。</p><p>这里要计算的话，因为隐状态是用文字推断出来的，所以这个映射关系还在，那么整理一下两个数组就能把每个隐状态能对应的文字全部映射上了。</p><hr />
<p>以下是我课程里的笔记，理解了上面的内容，理解下面是没有任何障碍的。</p><h1>viterbi in NLP</h1>
<p>$\overbrace{
  0
  \xrightarrow[农]{2.5}
  1
  \xrightarrow[产]{4.0}
  2
}^{1.4}
\xrightarrow[物]{2.3}
3$</p><p>$0
\xrightarrow[农]{2.5}
\underbrace{
  1
  \xrightarrow[产]{4.0}
  2
  \xrightarrow[物]{2.3}
  3
}_{2.1}$</p><blockquote>
<p>数字画圈的写法 $\enclose{circle}{3}$ 这个生成器暂不支持</p></blockquote>
<ul>
<li>node: $\enclose{circle}{2}$ ，圆圈，就是位置索引</li>
<li>edge: 词， 箭头，很好理解：string[0,1] = '农'</li>
<li>Each edge weight is a <code>negative log probality</code><ul>
<li>-log(P(农)) = 2.5</li>
<li>-log(P(产)) = 4.0</li>
<li>-log(P(农产)) = 1.4</li>
<li>-log(P(产物)) = 2.1</li>
</ul>
</li>
<li>Each path is a segmentation for the sentence</li>
<li>Each path weight is a sentence <code>unigram</code> negative log probability<ul>
<li>-log(P(农产)) + -log(P(物)) = 1.4 + 2.3 = 3.7</li>
<li>农 + 产 + 物 = 2.5 + 4.0 + 2.3 = 8.8</li>
<li>农 + 产物 = 2.5 + 2.1 = 4.6</li>
</ul>
</li>
</ul>
<h2>two step</h2>
<p>1.前向，从左往右，找到<strong>最佳路径</strong>的分数
2.后向，从右往左，创建一条最佳路径</p><h3>forward algorithm</h3>
<p>pseudo code</p><div class="highlight"><pre><span></span><span class="n">best_score</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>
<span class="k">for</span> <span class="n">each</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">graph</span> <span class="p">(</span><span class="n">ascending</span> <span class="n">order</span><span class="p">)</span>
  <span class="n">best_score</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="err">∞</span>
  <span class="k">for</span> <span class="n">each</span> <span class="n">incoming</span> <span class="n">edge</span> <span class="n">of</span> <span class="n">node</span>
    <span class="n">score</span><span class="o">=</span><span class="n">best_score</span><span class="p">[</span><span class="n">edgeprev_node</span><span class="p">]</span><span class="o">+</span><span class="n">edge</span><span class="o">.</span><span class="n">score</span>
    <span class="k">if</span> <span class="n">score</span> <span class="o">&lt;</span> <span class="n">best_score</span><span class="p">[</span><span class="n">node</span><span class="p">]</span>
      <span class="n">best_score</span><span class="p">[</span><span class="n">node</span><span class="p">]</span><span class="o">=</span><span class="n">score</span>
      <span class="n">best_edge</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span><span class="n">edge</span>
</pre></div>
<p>example:
<figure  style="flex: 82.65765765765765" ><img width="734" height="444" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/5aa5426eb70b4c6cd0b8c4b1dacda749.png" alt=""/></figure></p><ul>
<li>初始节点打分0，其它节点打分为$\infty$</li>
<li>每个节点打分由其(<code>incoming edge</code>)(即来源箭头)和来源节点的打分构成</li>
<li>如果有多个来源，则计算出该来源的得分，与该节点当前的得分做对比，取得分低的那个</li>
<li>把该节点的分值和来源edge存到该节点上（edge就是词）。</li>
</ul>
<ol>
<li>简单来说，还是和之前的骰子一样，每一次算出到当前节点的最低分数的路径。</li>
<li>上图中，我们就把e1, e2, e5选出来了，这个过程中，删除了e3, e4这几条路径</li>
<li>best_score=(0.0, 2.5, 1.4, 3.7), best_edge = (NULL, e1, e2, e5)</li>
<li>用字典来把Node映射上去：{0:(0.0, NULL), 1:(2.5, e1), 2:(1.4, e2), 3:(3.7, e5)}</li>
</ol>
<h3>backward algorithm</h3>
<div class="highlight"><pre><span></span><span class="n">best_path</span><span class="o">=</span><span class="p">[]</span>
<span class="n">next_edge</span><span class="o">=</span><span class="n">best_edge</span><span class="p">[</span><span class="n">best_edge</span><span class="o">.</span><span class="n">length</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="k">while</span> <span class="n">next_edge</span> <span class="o">!=</span> <span class="n">NULL</span>
  <span class="n">add</span> <span class="n">next_edge</span> <span class="n">to</span> <span class="n">best_path</span>
  <span class="n">next_edge</span> <span class="o">=</span><span class="n">best_edge</span><span class="p">[</span><span class="n">next_edge</span><span class="o">.</span><span class="n">prev_node</span><span class="p">]</span>
<span class="n">reverse</span> <span class="n">best</span> <span class="n">path</span>
</pre></div>
<p>举例：
<figure  style="flex: 102.56410256410257" ><img width="800" height="390" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/c97b27230bd42318efb72375c245c50b.png" alt=""/></figure></p><ul>
<li>从图片可知，<code>path</code>就是<code>edge</code></li>
<li>初始path是空，[]</li>
<li>从<code>forward</code>的结果字典里找到node 3的best_edge，就是e5 [e5]</li>
<li>e5的来源的是node 2</li>
<li>从字典里找到2的best_edge，是e2 [e5, e2]</li>
<li>e2的来源是node 0</li>
<li>0的best_edge是NULL，结束递归</li>
<li>reverse: [e2, e5]</li>
</ul>
<figure  style="flex: 53.25581395348837" ><img width="458" height="430" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/c7650195782b845d9be6e36dac55f277.png" alt=""/></figure><p>这个很好理解</p><ol>
<li>0到农，到农产，到农产物的概率，表示为0.0+ -log(p(农/农产/农产物))</li>
<li>在农的前提下，就有农到产，和农到产物：best(1) + -log(P(产/产物))</li>
<li>在产的前提下，就只有best(2) + -log(P(物))了</li>
</ol>
<p>应用到NLP：</p><figure  style="flex: 73.99193548387096" ><img width="734" height="496" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/5e643e29e4fe62ef780ba545dc3a04fb.png" alt=""/></figure><p>这里就是把node, egde具体了一下：</p><ol>
<li>多包了一层for-each，意思是前面的代码是处理一行的</li>
<li>node对应是单词结尾(word_end)，其实就是一个index，前面说过了</li>
<li>edge对应是单词(word)，前面也说过了，即<code>string[5,7]</code>的意思</li>
<li>score由uni-gram来计算</li>
<li>计算上，就是找到以基准字当作单词结尾，然后前面的字跟它拼起来的所有可能性，找最低分：<ul>
<li>比如abcdefg, 如果当前是e，那么分别比较：abced, bcde, cde, de</li>
</ul>
</li>
<li>接上例，输出结果应该这么解读：<ul>
<li>以b为结尾的单词，最有可能的是xxx, 它的得分是，它的索引是，</li>
<li>以c为结尾的单词，最有可能是bc或是abc，它的得分是，bc/abc的索引是(1,2)，这样</li>
</ul>
</li>
</ol>
<figure  style="flex: 90.2439024390244" ><img width="592" height="328" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/9de76a49c53e036f86c0c2e7a950c8cd.png" alt=""/></figure><ol>
<li>显然这里已经知道edge不知道是一个词，而且是一个词的首尾边界</li>
<li>也知道存到best_edges里面的其实就是词的位置索引</li>
<li>反向的时候，从最后一个索引找到得分最低的词，再从这个单词向前找，一直找到<ul>
<li>所以next_edge[0]其实就是当前单词词首，[1]就是词尾</li>
<li>所以把当前单词存进去后，向前搜索就要以next_edge[0]为字典，找对应的best_edge</li>
<li>再从best_edge里面解析出最合适的单词的首尾索引，存到结果数组里</li>
</ul>
</li>
</ol>
</div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/HMM%E3%80%81NER%E3%80%81PoS%E3%80%81Viterbi%E7%AC%94%E8%AE%B0/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/Mac%E8%BF%9C%E7%A8%8BWindows-10%E9%87%8C%E7%94%A8Anaconda%E8%A3%85%E7%9A%84Jupyter-lab/" target="_self">Mac远程Windows-10里用Anaconda装的Jupyter-lab</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/Mac%E8%BF%9C%E7%A8%8BWindows-10%E9%87%8C%E7%94%A8Anaconda%E8%A3%85%E7%9A%84Jupyter-lab/" target="_self">
                <time class="text-uppercase">
                    June 13 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><p>家里台式机配置比笔记本好多了，但又习惯了苹果本，怎么在小本本上直接跑windows上的jupyter呢？</p><p>首先，给Windows 10 装上<a href="https://docs.microsoft.com/en-us/windows-server/administration/openssh/openssh_install_firstuse">OpenSSH</a></p><p>如果你不是用的Anaconda等虚拟环境而是把python和jupyter lab装在了本机以及写在了path里，理论上你用ssh连上windows后在shell里直接<code>jupyter lab</code>就好了，可是我是用了Anaconda的，ssh进去以及windows自身的命令行环境里都是执行不了conda和jupyter的</p><blockquote>
<p>可能仅仅只是path的原因，但应该没这么简单，考虑到端口转发已经能实现我的目的了，就不深究了。</p></blockquote>
<p>这时使用<code>ssh</code>的本地端口转发功能可以达到目的：</p><div class="highlight"><pre><span></span>$ ssh -L <span class="m">2121</span>:host2:21 host3
</pre></div>
<p>即把<code>host3</code>的端口<code>21</code>转发到<code>host2</code>的2121上去，当然，大多数情况下<code>host2</code>就是本机，那么<code>localhost</code>就好了：</p><div class="highlight"><pre><span></span>$ ssh -L <span class="m">8000</span>:localhost:8889 windows-server
</pre></div>
<p>当然，<code>8889</code>是你在windows上运行<code>--no-browser</code>的jupyter lab设定的端口：</p><div class="highlight"><pre><span></span>jupyter lab --no-browser --post<span class="o">=</span><span class="m">8889</span>
</pre></div>
</div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/Mac%E8%BF%9C%E7%A8%8BWindows-10%E9%87%8C%E7%94%A8Anaconda%E8%A3%85%E7%9A%84Jupyter-lab/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/Semi-supervised-Learning/" target="_self">Semi supervised Learning</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/Semi-supervised-Learning/" target="_self">
                <time class="text-uppercase">
                    June 07 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><p>李宏毅机器学习2021spring的家庭作业里面有一个<code>Semi-supervised Learning</code>的任务。</p><p>具体来说，就是一个图片分类的任务（11个食品类别），但只给了你几百个有标注的图片，同时，还给了你几千张没有标的图片（用来训练，而不是测试）。</p><p>思路也很简单，既然样本量过小，我们就得自己扩充样本量，但这次不是用数据增广(<code>Augumentation</code>)，而是自己造样本：</p><ol>
<li>用小样本训练一个模型，用这个模型来predict没有标注的图片（文本有补述）</li>
<li>对预测输出的11个类别softmax后，观察最大值，如果大于你设定的某个threshold，比如0.68，就把该图片和最大值所映射的类别当成一组真值添加到训练集里去</li>
<li>我用的是<code>torch.utils.data</code>里的<code>TensorDataset</code>来构建手动创建的增强数据集，然后用了<code>ConcatDataset</code>与原训练集拼接：</li>
</ol>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">TensorDataset</span>

<span class="k">def</span> <span class="nf">get_pseudo_labels</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.65</span><span class="p">):</span>
    <span class="c1"># This functions generates pseudo-labels of a dataset using given model.</span>
    <span class="c1"># It returns an instance of DatasetFolder containing images whose prediction confidences exceed a given threshold.</span>
    <span class="c1"># You are NOT allowed to use any models trained on external data for pseudo-labeling.</span>
    <span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>

    <span class="c1"># Construct a data loader.</span>
    <span class="n">data_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Make sure the model is in eval mode.</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="c1"># Define softmax function.</span>
    <span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Iterate over the dataset by batches.</span>
    <span class="n">images</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([])</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([])</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">data_loader</span><span class="p">):</span>
        <span class="n">img</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">batch</span>

        <span class="c1"># Forward the data</span>
        <span class="c1"># Using torch.no_grad() accelerates the forward process.</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>

        <span class="c1"># Obtain the probability distributions by applying softmax on logits.</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>

        <span class="c1"># ---------- TODO ----------</span>
        <span class="c1"># 在这里根据阈值判断是否保留</span>
        <span class="c1"># Filter the data and construct a new dataset.</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">prob</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">probs</span><span class="p">):</span>
            <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">prob</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">images</span><span class="p">,</span> <span class="n">img</span><span class="p">[</span><span class="n">idx</span><span class="p">]))</span>   <span class="c1"># 用索引选出对应的图片</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">targets</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">c</span><span class="p">)))</span> <span class="c1"># 用最大值索引当class</span>

    <span class="n">dataset</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>  <span class="c1"># 拼成tensor dataset</span>

    <span class="c1"># # Turn off the eval mode.</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">dataset</span>
</pre></div>
<p>使用：</p><div class="highlight"><pre><span></span><span class="n">pseudo_set</span> <span class="o">=</span> <span class="n">get_pseudo_labels</span><span class="p">(</span><span class="n">unlabeled_set</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>

<span class="c1"># Construct a new dataset and a data loader for training.</span>
<span class="c1"># This is used in semi-supervised learning only.</span>
<span class="n">concat_dataset</span> <span class="o">=</span> <span class="n">ConcatDataset</span><span class="p">([</span><span class="n">train_set</span><span class="p">,</span> <span class="n">pseudo_set</span><span class="p">])</span> <span class="c1"># 拼接两个dataset(只要有感兴趣的两组数组即可)</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">concat_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
<p>看来，所谓的半监督仍然是有监督，对于没有标注的数据，仍然要想办法用已有数据去为它打标，接下来就是普通的监督学习了。</p><hr />
<p>最后，在实际的demo代码中，能看到并不是我最初理解的“先用小样本训练好一个模型”，再用它来过滤un-labeled样本，增广到训练集去，即对训练集的增广是一劳永逸的（像别的增广方案一样）</p><p>而是每一个epoch里面都<strong>重新</strong>去增广一次，这个思路更类似于GAN（生成对抗网络），<code>generator</code>和<code>discriminator</code>是一起训练的。</p><p>也所以，第一次去增广的时候，其实就是一个初始化的model，也就是说，一个比较垃圾的数据集（当然，初始化的model未必能预测出置信度高的结果，以至于并不会有太多pseudo labels进入训练集）</p><p>因此，相比较纯监督学习，假如训练集是2000条，那么整个epoch轮次里，都是2000条数据在训练；而半监督学习里，可能是200, 220, 350, 580, 1000, 1500...这样累增的样本量（随着模型越来越好，置信度应该是越来越高的），如果epoch数量不够，可能并没有在相同2000左右的样本量下得到足够的训练</p></div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/Semi-supervised-Learning/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/RNN%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E6%8E%A8%E5%AF%BC/" target="_self">RNN梯度消失与梯度爆炸推导</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/RNN%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E6%8E%A8%E5%AF%BC/" target="_self">
                <time class="text-uppercase">
                    May 09 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><figure  style="flex: 82.77777777777777" ><img width="596" height="360" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/794a90e7b252e2aa03842eeee7fad8de.png" alt=""/></figure>
$$
\large
\begin{aligned}
h_t &amp;=\sigma(z_t) = \sigma(Ux_t+Wh_{t-1} + b) \
y_t &amp;= \sigma(Vh_t + c)
\end{aligned}
$$<h2>梯度消失与爆炸</h2>
<p>假设一个只有 3 个输入数据的序列，此时我们的隐藏层 h1、h2、h3 和输出 y1、y2、y3 的计算公式：</p><p>$$
\large
\begin{aligned}
h_1 &amp;= \sigma(Ux_1 + Wh_0 + b) \\
h_2 &amp;= \sigma(Ux_2 + Wh_1 + b) \\
h_3 &amp;= \sigma(Ux_3 + Wh_2 + b) \\
y_1 &amp;= \sigma(Vh_1 + c) \\
y_2 &amp;= \sigma(Vh_2 + c) \\
y_3 &amp;= \sigma(Vh_3 + c)
\end{aligned}
$$</p>
<p>RNN 在时刻 t 的损失函数为 Lt，总的损失函数为 $L = L1 + L2 + L3 \Longrightarrow  \sum_{t=1}^TL_T$</p><p>t = 3 时刻的损失函数 L3 对于网络参数 U、W、V 的梯度如下：</p><p>$$
\begin{aligned}
\frac{\partial L_3}{\partial V} &amp;= \frac{\partial L_3}{\partial y_3} \frac{\partial y_3}{\partial V} \\
\frac{\partial L_3}{\partial U} &amp;= \frac{\partial L_3}{\partial y_3} \frac{\partial y_3}{\partial h_3} \frac{\partial h_3}{\partial U} + \frac{\partial L_3}{\partial y_3} \frac{\partial y_3}{\partial h_3} \frac{\partial h_3}{\partial h_2} \frac{\partial h_2}{\partial U} + \frac{\partial L_3}{\partial y_3} \frac{\partial y_3}{\partial h_3} \frac{\partial h_3}{\partial h_2} \frac{\partial h_2}{\partial h_1} \frac{\partial h_1}{\partial U} \\
\frac{\partial L_3}{\partial W} &amp;= \frac{\partial L_3}{\partial y_3} \frac{\partial y_3}{\partial h_3} \frac{\partial h_3}{\partial W} 
+ \frac{\partial L_3}{\partial y_3} \frac{\partial y_3}{\partial h_3} \frac{\partial h_3}{\partial h_2} \frac{\partial h_2}{\partial W} 
+ \frac{\partial L_3}{\partial y_3} \frac{\partial y_3}{\partial h_3} \frac{\partial h_3}{\partial h_2} \frac{\partial h_2}{\partial h_1} \frac{\partial h_1}{\partial W} \\
\end{aligned}
$$</p>
<p>其实主要就是因为：</p><ul>
<li>对V求偏导时，$h_3$是常数</li>
<li>对U求偏导时：<ul>
<li>$h_3$里有U，所以要继续对h3应用<code>chain rule</code></li>
<li>$h_3$里的$W, b$是常数，但是$h_2$里又有U，继续<code>chain rule</code></li>
<li>以此类推，直到$h_0$</li>
</ul>
</li>
<li>对W求偏导时一样</li>
</ul>
<p>所以：</p><ol>
<li>参数矩阵 V (对应输出 $y_t$) 的梯度很显然并没有长期依赖</li>
<li>U和V显然就是连乘($\prod$)后累加($\sum$)</li>
</ol>
<p>$$
\begin{aligned}
\frac{\partial L_t}{\partial U} = \sum_{k=0}^{t} \frac{\partial L_t}{\partial y_t} \frac{\partial y_t}{\partial h_t}
(\prod_{j=k+1}^{t}\frac{\partial h_j}{\partial h_{j-1}})
\frac{\partial h_k}{\partial U} \\
\frac{\partial L_t}{\partial W} = \sum_{k=0}^{t} \frac{\partial L_t}{\partial y_t} \frac{\partial y_t}{\partial h_t}
(\prod_{j=k+1}^{t}\frac{\partial h_j}{\partial h_{j-1}})
\frac{\partial h_k}{\partial W}
\end{aligned}
$$</p>
<p>其中的连乘项就是导致 RNN 出现梯度消失与梯度爆炸的罪魁祸首，连乘项可以如下变换：</p><ul>
<li>$h_j = tanh(Ux_j + Wh_{j-1} + b)$</li>
<li>$\prod_{j=k+1}^{t}\frac{\partial h_j}{\partial h_{j-1}} =\prod_{j=k+1}^{t} tanh' \times W$</li>
</ul>
<p>tanh' 表示 tanh 的导数，可以看到 RNN 求梯度的时候，实际上用到了 (tanh' × W) 的连乘。当 (tanh' × W) &gt; 1 时，多次连乘容易导致梯度爆炸；当 (tanh' × W) &lt; 1 时，多次连乘容易导致梯度消失。</p></div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/RNN%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E6%8E%A8%E5%AF%BC/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/RNN%E4%B8%ADbidirectional%E5%92%8Cnum_layer%E5%AF%B9output%E5%92%8Chidden%E5%BD%A2%E7%8A%B6%E7%9A%84%E5%BD%B1%E5%93%8D/" target="_self">RNN中bidirectional和num_layer对output和hidden形状的影响</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/RNN%E4%B8%ADbidirectional%E5%92%8Cnum_layer%E5%AF%B9output%E5%92%8Chidden%E5%BD%A2%E7%8A%B6%E7%9A%84%E5%BD%B1%E5%93%8D/" target="_self">
                <time class="text-uppercase">
                    April 13 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><h2>Batch first</h2>
<p>首先，我们要习惯接受<code>batch_first=False</code>（就是默认值）的思维，因为NLP中批量处理句子，是每一句取第一个词，第二个词，以此类推。
按我们习惯的把数据放在同一批（即<code>batch_first=True</code>）的思路虽然可以做到（善用切片即可），但是绕了弯路。但是如果第1批都是第1个字，第2批全是第2个字，这会自然很多（<strong>行优先</strong>）。</p><p>所以至少<code>Pytorch</code>内部，你设了True，内部也是按False来处理的，只是给了你一个语法糖（当然你组织数据就必须按True来组织了。</p><p>看个实例：</p><figure  style="flex: 79.69151670951157" ><img width="1240" height="778" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/d6a0d9ea71601f8b89c0e8465751baf4.png" alt=""/></figure><ol>
<li>假定批次是64，句长截为70，在还没有向量化的数据中，那么显然一次的输入应该为(70x64)，批次在第2位</li>
<li>注意第一行，全是2，这是设定的<code>&lt;bos&gt;</code>，这已经很好地表示了在行优先的系统里（比如<code>Matlab</code>就是列优先），会自然而且把<strong>每句话</strong>的第一个词读出来的设定了。</li>
</ol>

<pre><code># 我用的torchtext的Field进行演示， SRC是一个Field
[SRC.vocab.itos[i] for i in range(1,4)]  
['&lt;pad&gt;', '&lt;bos&gt;', '&lt;eos&gt;']
</code></pre>
<ol start="3">
<li>可见，2是开始，3是结束，1是空格（当然这是我设置的）</li>
<li>同时也能注意到，最后一行有的是3，有的是1，有的都不是，就说明句子是以70为长度进行截断的，自然结束的是3，补<code>&lt;pad&gt;</code>的是1，截断的那么那个字是多少就是多少</li>
<li>竖向取一条就是一整句话，打印出来就是箭头指向的那一大坨（共70个数字）</li>
<li>对它进行<code>index_to_string</code>(itos)，则还原出了这句话</li>
<li>nn.Embedding做了两件事：</li>
</ol>
<ul>
<li>根据vocabulary进行one-hot（稀疏）$\rightarrow$ 所以你要告诉它词典大小</li>
<li>然后再embedding成指定的低维向量（稠密）</li>
<li>所以70个数字就成了70x300，拼上维度，就是70x64x300</li>
</ul>
<p>既然讲到这了，多讲两行，假定hidden_dim=256, 一个<code>nn.RNN</code>会输出的<code>outputs</code>和<code>hidden</code>的形状如下：</p>
<pre><code>&gt;&gt;&gt; outputs.shape
torch.Size([70, 64, 256])
&gt;&gt;&gt; hidden.shape
torch.Size([1, 64, 256])
</code></pre>
<ol>
<li>即300维进去，256维出来，但是因为句子有70的长度，那就是70个output，hidden是从前传到后的，当然是最后一个</li>
<li>也因此，如果你不需要叠加多层RNN，你只需要最后一个字的output就行了<code>outputs[-1,:,:]</code>, 这个结果送到全连接层里去进行分类。</li>
</ol>
<h2>自己写一个RNN</h2>
<p>其实就是要自己把上述形状变化做对就行了。就是几个线性变换，所以我们用<code>nn.Linear</code>来拼接:</p><ol>
<li>input: 2x5x3 $\Rightarrow$ 5个序列，每一个2个词，每个词用3维向量表示</li>
<li>hidden=10, 无embedding，num_class=7</li>
<li>期待形状：</li>
</ol>
<ul>
<li>output: 2x5x7</li>
<li>hidden:1x5x10</li>
</ul>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">RNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">i2h</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">i2o</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">):</span>
        <span class="n">combined</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="nb">input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
        <span class="c1"># input shape: (2, 5, 3)</span>
        <span class="c1"># hidden shape: (2, 5, 10)</span>
        <span class="c1"># combine shape (2, 5, 13)</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">i2h</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">i2o</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span>

    <span class="k">def</span> <span class="nf">initHidden</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>

<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">num_class</span> <span class="o">=</span> <span class="mi">7</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">,(</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>

<span class="n">rnn</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span><span class="n">hidden_size</span><span class="p">,</span><span class="n">num_class</span><span class="p">)</span>
<span class="n">out</span><span class="p">,</span> <span class="n">hid</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">))</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">hid</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
<p>output:</p>
<pre><code>(torch.Size([2, 5, 7]), torch.Size([2, 5, 10]))
</code></pre>
<p>可见，output是一样的，hidden的形状不一样，事实上每一个字确实是会产生hidden的，但是pytorch并没有把它返出来（消费掉就没用了）。这里就pass了，我们主要是看一下双向和多层的情况下形状的变化，下面我们用pytorch自己的RNN来测试。</p><h1>num_layers</h1>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">num_layers</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span> <span class="c1"># 几层就需要初始几个hidden</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># input: 5x3 -&gt; 1x12 # N个批次， 5个序列(比如5个字，每个字由3个数字的向量组成)</span>
<span class="n">o</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span> <span class="c1"># 5个output, 一个final hidden</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;output shape&#39;</span><span class="p">,</span> <span class="n">o</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;hidden shape&#39;</span><span class="p">,</span> <span class="n">h</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<p>输出：</p>
<pre><code>output shape torch.Size([5, 2, 12])  # 2个批次，5个词，12维度输出
hidden shape torch.Size([3, 2, 12]) # 3层会输出3个hidden，2个批次
</code></pre>
<p>加上embedding, RNN改成GRU</p><div class="highlight"><pre><span></span><span class="c1"># 这次加embedding</span>
<span class="c1"># 顺便把 RNN 改 GRU</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">embed_size</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">3</span>
<span class="c1"># 要求词典长度不超过5，输出向量长度为10</span>
<span class="n">emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span> 
<span class="c1"># 输入为embeding维度，输出（和隐层）为8维度</span>
<span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># 这次设了num_layers=2，就要求有两个hidden了</span>
<span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
<span class="c1"># 因为数据会用embedding包一次，所以input没有了维度要求（只有大小要求，每个数字要小于字典长度）</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">))</span> 
<span class="n">e</span> <span class="o">=</span> <span class="n">emb</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;input.shape:&#39;</span><span class="p">,</span> <span class="n">x0</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;embedding.shape:&#39;</span><span class="p">,</span> <span class="n">e</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (3,4)会扩展成（3,4,10), 10维是rnn的input维度，正好</span>
<span class="n">o</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;output.shape:</span><span class="si">{</span><span class="n">o</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">, hidden.shape:</span><span class="si">{</span><span class="n">h</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

<pre><code>input.shape: torch.Size([5, 3])
embedding.shape: torch.Size([5, 3, 10])
output.shape:torch.Size([5, 3, 8]), hidden.shape:torch.Size([2, 3, 8])
</code></pre>
<p>唯一要注意的变化就是input，因为embedding是把字典大小的维度转换成指定大小的维度，暗含了你里面的每一个数字都是字典的索引，所以你组装demo数据的时候，要生成小于字典大小(<code>vocab_size</code>）的数字作为输入。</p><h2>bidirectional</h2>
<p>这次加<strong>bidirectional</strong></p><ul>
<li>batch_first = False</li>
<li>x (5, 3) -&gt; 3个序列，每个序列5个数</li>
<li>embedding(5, 10) -&gt; 输入字典长5，输出向量长10 -&gt; (5, 3, 10) -&gt; 3个序列，每个序列5个10维向量</li>
<li>hidden必须为8维，4个（num_layers=2, bidirection),3个批次 -&gt; (4,3,8)</li>
<li>rnn(10, 8) -&gt; 输入10维，输出8维</li>
</ul>
<div class="highlight"><pre><span></span><span class="c1"># 这次加 bidirection</span>

<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">embed_size</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">num_layers</span> <span class="o">=</span> <span class="mi">2</span>
<span class="c1"># 要求词典长度不超过5，输出向量长度为10</span>
<span class="n">emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span> 
<span class="c1"># 输入为embeding维度，输出（和隐层）为8维度</span>
<span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># 这次设了num_layers=2，就要求有两个hidden了</span>
<span class="c1"># 加上双向，就有4个了，这里乘以2</span>
<span class="c1"># h0 = (torch.rand(2, batch_size, hidden_size), torch.rand(2, batch_size, hidden_size))</span>
<span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">num_layers</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
<span class="c1"># 因为数据会用embedding包一次，所以input没有了维度要求（只有大小要求，每个数要小于字典长度）</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">))</span> 
<span class="n">e</span> <span class="o">=</span> <span class="n">emb</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;input.shape:&#39;</span><span class="p">,</span> <span class="n">x0</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;embedding.shape:&#39;</span><span class="p">,</span> <span class="n">e</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (3,4)会扩展成（3,4,10), 10维是rnn的input维度，正好</span>
<span class="c1"># hidden = torch.cat((h0[-2,:,:], h0[-1,:,:]),1)</span>
<span class="n">o</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;output.shape:</span><span class="si">{</span><span class="n">o</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">, hidden.shape:</span><span class="si">{</span><span class="n">h</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

<pre><code>input.shape: torch.Size([5, 3])
embedding.shape: torch.Size([5, 3, 10])
output.shape:torch.Size([5, 3, 16]), hidden.shape:torch.Size([4, 3, 8])
</code></pre>
<p>可见，双向会使输出多一倍，可以用<code>[:hidden_size], [hidden_size:]</code>分别取出来，我们<strong>验证</strong>一下，用框架生成一个双向的GRU，然后手动生成一个正向的一个负向的，复制参数，看一下输出：</p><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="c1"># 制作一个正序和反序的input</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">17</span><span class="p">)</span>
<span class="n">random_input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">normal_</span><span class="p">(),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">reverse_input</span> <span class="o">=</span> <span class="n">random_input</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">:,</span> <span class="p">:]</span>

<span class="n">bi_grus</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">reverse_gru</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">gru</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">reverse_gru</span><span class="o">.</span><span class="n">weight_ih_l0</span> <span class="o">=</span> <span class="n">bi_grus</span><span class="o">.</span><span class="n">weight_ih_l0_reverse</span>
<span class="n">reverse_gru</span><span class="o">.</span><span class="n">weight_hh_l0</span> <span class="o">=</span> <span class="n">bi_grus</span><span class="o">.</span><span class="n">weight_hh_l0_reverse</span>
<span class="n">reverse_gru</span><span class="o">.</span><span class="n">bias_ih_l0</span> <span class="o">=</span> <span class="n">bi_grus</span><span class="o">.</span><span class="n">bias_ih_l0_reverse</span>
<span class="n">reverse_gru</span><span class="o">.</span><span class="n">bias_hh_l0</span> <span class="o">=</span> <span class="n">bi_grus</span><span class="o">.</span><span class="n">bias_hh_l0_reverse</span>
<span class="n">gru</span><span class="o">.</span><span class="n">weight_ih_l0</span> <span class="o">=</span> <span class="n">bi_grus</span><span class="o">.</span><span class="n">weight_ih_l0</span>
<span class="n">gru</span><span class="o">.</span><span class="n">weight_hh_l0</span> <span class="o">=</span> <span class="n">bi_grus</span><span class="o">.</span><span class="n">weight_hh_l0</span>
<span class="n">gru</span><span class="o">.</span><span class="n">bias_ih_l0</span> <span class="o">=</span> <span class="n">bi_grus</span><span class="o">.</span><span class="n">bias_ih_l0</span>
<span class="n">gru</span><span class="o">.</span><span class="n">bias_hh_l0</span> <span class="o">=</span> <span class="n">bi_grus</span><span class="o">.</span><span class="n">bias_hh_l0</span>

<span class="n">bi_output</span><span class="p">,</span> <span class="n">bi_hidden</span> <span class="o">=</span> <span class="n">bi_grus</span><span class="p">(</span><span class="n">random_input</span><span class="p">)</span>
<span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="n">gru</span><span class="p">(</span><span class="n">random_input</span><span class="p">)</span>
<span class="n">reverse_output</span><span class="p">,</span> <span class="n">reverse_hidden</span> <span class="o">=</span> <span class="n">reverse_gru</span><span class="p">(</span><span class="n">reverse_input</span><span class="p">)</span>  <span class="c1"># 分别取[(4,3,2,1,0),:,:] -&gt; 即倒序送入input</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;bi_output:&#39;</span><span class="p">,</span> <span class="n">bi_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">bi_output</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">bi_output</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>                <span class="c1"># 双向输出中的后半截</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">reverse_output</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="p">))</span> <span class="c1"># 反向输出</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>                   <span class="c1"># 单独一个rnn的输出 </span>
<span class="nb">print</span><span class="p">(</span><span class="n">bi_output</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>                <span class="c1"># 双向输出中的前半截</span>
</pre></div>

<pre><code>bi_output: torch.Size([5, 1, 2])
tensor([[-0.2336, -0.3068],
        [ 0.0660, -0.6004],
        [ 0.0859, -0.5620],
        [ 0.2164, -0.5750],
        [ 0.1229, -0.3608]])
tensor([-0.3068, -0.6004, -0.5620, -0.5750, -0.3608])
tensor([-0.3068, -0.6004, -0.5620, -0.5750, -0.3608])
tensor([-0.2336,  0.0660,  0.0859,  0.2164,  0.1229])
tensor([-0.2336,  0.0660,  0.0859,  0.2164,  0.1229])
</code></pre>
<p>现在你们应该知道<code>bidirectional</code>的双倍输出是怎么回事了，再来看看hidden</p><div class="highlight"><pre><span></span><span class="n">hidden</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">reverse_hidden</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">bi_hidden</span><span class="o">.</span><span class="n">shape</span>
<span class="n">bi_hidden</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">reverse_hidden</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">hidden</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">data</span>
</pre></div>

<pre><code>(torch.Size([1, 1, 1]), torch.Size([1, 1, 1]), torch.Size([2, 1, 1]))
(tensor([ 0.1229, -0.3068]), tensor([-0.3068]), tensor([0.1229]))
</code></pre>
<ul>
<li>正向的输出就是单向rnn</li>
<li>反向的输出就是把数据反传的单向rnn</li>
<li>双向rnn出来的第最后一个hidden（后半截）就是反向完成后的hidden</li>
</ul>
<figure  style="flex: 83.6996336996337" ><img width="914" height="546" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/67bb7c8df3810d83a7f07909fbd601f9.png" alt=""/></figure><p>由打印出来的数据可知：</p><ul>
<li>最后一个hidden，就是反向RNN的最后一个hidden（时间点在开头）</li>
<li>也是双向RNN里的第一个输出（<strong>的最后一个元素</strong>）</li>
<li>也是单向RNN（但是数据反传）（或者正向，但逆时序）里的最后一个输出</li>
</ul>
<hr />
<p>双向RNN里</p><ul>
<li>倒数第二个hidden，是正向的最后一个hidden（时间点在结尾）</li>
<li>它也是output里面的值，它是双向输出里的最后一个的<strong>第一个元素</strong></li>
</ul>
<p>总的来说</p><ul>
<li>output由正反向输出横向拼接（所有）</li>
<li>hidden由正反向hidden竖向拼接（top layer)</li>
</ul>
<figure  style="flex: 71.92575406032482" ><img width="1240" height="862" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/671265db99cdb4c1fcda808d82a08794.png" alt=""/></figure></div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/RNN%E4%B8%ADbidirectional%E5%92%8Cnum_layer%E5%AF%B9output%E5%92%8Chidden%E5%BD%A2%E7%8A%B6%E7%9A%84%E5%BD%B1%E5%93%8D/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
</section>

<div class="container">
    <section id="prism__page__pagination" class="prism-pagination" class="col-md-8 offset-md-2">
        <ul>
            
            <li class="next">
                <a class="no-link" href="/page/3/" target="_self"><i class="fa fa-chevron-left" aria-hidden="true"></i>Newer</a>
            </li>
            
            
            <li class="prev">
                <a class="no-link" href="/page/5/" target="_self">Older<i class="fa fa-chevron-right" aria-hidden="true"></i></a>
            </li>
            
        </ul>
    </section>
</div>


</main>

            <footer id="prism__footer">
                <section>
                    <div>
                        <nav class="social-links">
                            <ul><li><a class="no-link" title="Twitter" href="https://twitter.com/walkerwzy" target="_blank" rel="noopener noreferrer nofollow"><i class="gi gi-twitter"></i></a></li><li><a class="no-link" title="GitHub" href="https://github.com/walkerwzy" target="_blank" rel="noopener noreferrer nofollow"><i class="gi gi-github"></i></a></li><li><a class="no-link" title="Weibo" href="https://weibo.com/1071696872" target="_blank" rel="noopener noreferrer nofollow"><i class="gi gi-weibo"></i></a></li></ul>
                        </nav>
                    </div>

                    <section id="prism__external_links">
                        <ul>
                            
                            <li>
                                <a class="no-link" target="_blank" href="https://github.com/AlanDecode/Maverick" rel="noopener noreferrer nofollow">Maverick</a>：🏄‍ Go My Own Way.
                                <span>|</span>
                            </li>
                            
                            <li>
                                <a class="no-link" target="_blank" href="https://www.imalan.cn" rel="noopener noreferrer nofollow">Triple NULL</a>：Home page for AlanDecode.
                                <span>|</span>
                            </li>
                            
                        </ul>
                    </section>

                    <div class="copyright">
                        <p class="copyright-text">
                            <span class="brand">walker's code blog</span>
                            <span>Copyright © 2022 AlanDecode</span>
                        </p>
                        <p class="copyright-text powered-by">
                            | Powered by <a href="https://github.com/AlanDecode/Maverick" class="no-link" target="_blank" rel="noopener noreferrer nofollow">Maverick</a> | Theme <a href="https://github.com/Reedo0910/Maverick-Theme-Prism" target="_blank" class="no-link" rel="noopener noreferrer nofollow">Prism</a>
                        </p>
                    </div>
                    <div class="footer-addon">
                        
                    </div>
                </section>
                <script>
                    var site_build_date = "2019-12-06T12:00+08:00"

                </script>
                <script src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/prism-efa8685153.js"></script>
            </footer>
        </div>
    </div>
    </div>

    <script src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/ExSearch/jquery.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/ExSearch/ExSearch-493cb9cd89.js"></script>

    <!--katex-->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/katex.min.js"></script>
    <script>
        mathOpts = {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "\\[", right: "\\]", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false }
            ]
        };

    </script>
    <script defer src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/auto-render.min.js" onload="renderMathInElement(document.body, mathOpts);"></script>

    
</body>

</html>