<!DOCTYPE HTML>
<html lang="english">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="renderer" content="webkit">
    <meta name="HandheldFriendly" content="true">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Maverick,AlanDecode,Galileo,blog" />
    <meta name="generator" content="Maverick 1.2.1" />
    <meta name="template" content="Prism" />
    <link rel="alternate" type="application/rss+xml" title="walker's code blog &raquo; RSS 2.0" href="/feed/index.xml" />
    <link rel="alternate" type="application/atom+xml" title="walker's code blog &raquo; ATOM 1.0" href="/feed/atom/index.xml" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/prism-b9d78ff38a.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/ExSearch/ExSearch-182e5a8869.css">
    <link href="https://fonts.googleapis.com/css?family=Fira+Code&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css">
    <script>
        var ExSearchConfig = {
            root: "",
            api: "https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/285ab69ade0ed03d76402217cdc4c128.json"
        }

    </script>
    
<title>walker's code blog</title>
<meta name="author" content="AlanDecode" />
<meta name="description" content="coder, reader" />
<meta property="og:title" content="walker's code blog" />
<meta property="og:description" content="coder, reader" />
<meta property="og:site_name" content="walker's code blog" />
<meta property="og:type" content="website" />
<meta property="og:url" content="/page/4/" />
<meta property="og:image" content="walker's code blog" />
<meta name="twitter:title" content="walker's code blog" />
<meta name="twitter:description" content="coder, reader" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/android-chrome-512x512.png" />


    
</head>

<body>
    <div class="container prism-container">
        <header class="prism-header" id="prism__header">
            <h1 class="text-uppercase brand"><a class="no-link" href="/" target="_self">walker's code blog</a></h1>
            <p>coder, reader</p>
            <nav class="prism-nav"><ul><li><a class="no-link text-uppercase " href="/" target="_self">Home</a></li><li><a class="no-link text-uppercase " href="/archives/" target="_self">Archives</a></li><li><a class="no-link text-uppercase " href="/about/" target="_self">About</a></li><li><a href="#" target="_self" class="search-form-input no-link text-uppercase">Search</a></li></ul></nav>
        </header>
        <div class="prism-wrapper" id="prism__wrapper">
            
<main>    
    

<section id="prism__post-list" class="prism-section row">
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/Mac%E8%BF%9C%E7%A8%8BWindows-10%E9%87%8C%E7%94%A8Anaconda%E8%A3%85%E7%9A%84Jupyter-lab/" target="_self">Mac远程Windows-10里用Anaconda装的Jupyter-lab</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/Mac%E8%BF%9C%E7%A8%8BWindows-10%E9%87%8C%E7%94%A8Anaconda%E8%A3%85%E7%9A%84Jupyter-lab/" target="_self">
                <time class="text-uppercase">
                    June 13 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><p>家里台式机配置比笔记本好多了，但又习惯了苹果本，怎么在小本本上直接跑windows上的jupyter呢？</p><p>首先，给Windows 10 装上<a href="https://docs.microsoft.com/en-us/windows-server/administration/openssh/openssh_install_firstuse">OpenSSH</a></p><p>如果你不是用的Anaconda等虚拟环境而是把python和jupyter lab装在了本机以及写在了path里，理论上你用ssh连上windows后在shell里直接<code>jupyter lab</code>就好了，可是我是用了Anaconda的，ssh进去以及windows自身的命令行环境里都是执行不了conda和jupyter的</p><blockquote>
<p>可能仅仅只是path的原因，但应该没这么简单，考虑到端口转发已经能实现我的目的了，就不深究了。</p></blockquote>
<p>这时使用<code>ssh</code>的本地端口转发功能可以达到目的：</p><div class="highlight"><pre><span></span>$ ssh -L <span class="m">2121</span>:host2:21 host3
</pre></div>
<p>即把<code>host3</code>的端口<code>21</code>转发到<code>host2</code>的2121上去，当然，大多数情况下<code>host2</code>就是本机，那么<code>localhost</code>就好了：</p><div class="highlight"><pre><span></span>$ ssh -L <span class="m">8000</span>:localhost:8889 windows-server
</pre></div>
<p>当然，<code>8889</code>是你在windows上运行<code>--no-browser</code>的jupyter lab设定的端口：</p><div class="highlight"><pre><span></span>jupyter lab --no-browser --post<span class="o">=</span><span class="m">8889</span>
</pre></div>
</div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/Mac%E8%BF%9C%E7%A8%8BWindows-10%E9%87%8C%E7%94%A8Anaconda%E8%A3%85%E7%9A%84Jupyter-lab/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/Semi-supervised-Learning/" target="_self">Semi-supervised-Learning</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/Semi-supervised-Learning/" target="_self">
                <time class="text-uppercase">
                    June 07 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><p>李宏毅机器学习2021spring的家庭作业里面有一个<code>Semi-supervised Learning</code>的任务。</p><p>具体来说，就是一个图片分类的任务（11个食品类别），但只给了你几百个有标注的图片，同时，还给了你几千张没有标的图片（用来训练，而不是测试）。</p><p>思路也很简单，既然样本量过小，我们就得自己扩充样本量，但这次不是用数据增广(<code>Augumentation</code>)，而是自己造样本：</p><ol>
<li>用小样本训练一个模型，用这个模型来predict没有标注的图片（文本有补述）</li>
<li>对预测输出的11个类别softmax后，观察最大值，如果大于你设定的某个threshold，比如0.68，就把该图片和最大值所映射的类别当成一组真值添加到训练集里去</li>
<li>我用的是<code>torch.utils.data</code>里的<code>TensorDataset</code>来构建手动创建的增强数据集，然后用了<code>ConcatDataset</code>与原训练集拼接：</li>
</ol>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">TensorDataset</span>

<span class="k">def</span> <span class="nf">get_pseudo_labels</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.65</span><span class="p">):</span>
    <span class="c1"># This functions generates pseudo-labels of a dataset using given model.</span>
    <span class="c1"># It returns an instance of DatasetFolder containing images whose prediction confidences exceed a given threshold.</span>
    <span class="c1"># You are NOT allowed to use any models trained on external data for pseudo-labeling.</span>
    <span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>

    <span class="c1"># Construct a data loader.</span>
    <span class="n">data_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Make sure the model is in eval mode.</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="c1"># Define softmax function.</span>
    <span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Iterate over the dataset by batches.</span>
    <span class="n">images</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([])</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([])</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">data_loader</span><span class="p">):</span>
        <span class="n">img</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">batch</span>

        <span class="c1"># Forward the data</span>
        <span class="c1"># Using torch.no_grad() accelerates the forward process.</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>

        <span class="c1"># Obtain the probability distributions by applying softmax on logits.</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>

        <span class="c1"># ---------- TODO ----------</span>
        <span class="c1"># 在这里根据阈值判断是否保留</span>
        <span class="c1"># Filter the data and construct a new dataset.</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">prob</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">probs</span><span class="p">):</span>
            <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">prob</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">images</span><span class="p">,</span> <span class="n">img</span><span class="p">[</span><span class="n">idx</span><span class="p">]))</span>   <span class="c1"># 用索引选出对应的图片</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">targets</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">c</span><span class="p">)))</span> <span class="c1"># 用最大值索引当class</span>

    <span class="n">dataset</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>  <span class="c1"># 拼成tensor dataset</span>

    <span class="c1"># # Turn off the eval mode.</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">dataset</span>
</pre></div>
<p>使用：</p><div class="highlight"><pre><span></span><span class="n">pseudo_set</span> <span class="o">=</span> <span class="n">get_pseudo_labels</span><span class="p">(</span><span class="n">unlabeled_set</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>

<span class="c1"># Construct a new dataset and a data loader for training.</span>
<span class="c1"># This is used in semi-supervised learning only.</span>
<span class="n">concat_dataset</span> <span class="o">=</span> <span class="n">ConcatDataset</span><span class="p">([</span><span class="n">train_set</span><span class="p">,</span> <span class="n">pseudo_set</span><span class="p">])</span> <span class="c1"># 拼接两个dataset(只要有感兴趣的两组数组即可)</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">concat_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
<p>看来，所谓的半监督仍然是有监督，对于没有标注的数据，仍然要想办法用已有数据去为它打标，接下来就是普通的监督学习了。</p><hr />
<p>最后，在实际的demo代码中，能看到并不是我最初理解的“先用小样本训练好一个模型”，再用它来过滤un-labeled样本，增广到训练集去，即对训练集的增广是一劳永逸的（像别的增广方案一样）</p><p>而是每一个epoch里面都<strong>重新</strong>去增广一次，这个思路更类似于GAN（生成对抗网络），<code>generator</code>和<code>discriminator</code>是一起训练的。</p><p>也所以，第一次去增广的时候，其实就是一个初始化的model，也就是说，一个比较垃圾的数据集（当然，初始化的model未必能预测出置信度高的结果，以至于并不会有太多pseudo labels进入训练集）</p><p>因此，相比较纯监督学习，假如训练集是2000条，那么整个epoch轮次里，都是2000条数据在训练；而半监督学习里，可能是200, 220, 350, 580, 1000, 1500...这样累增的样本量（随着模型越来越好，置信度应该是越来越高的），如果epoch数量不够，可能并没有在相同2000左右的样本量下得到足够的训练</p></div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/Semi-supervised-Learning/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/RNN%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E6%8E%A8%E5%AF%BC/" target="_self">RNN梯度消失与梯度爆炸推导</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/RNN%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E6%8E%A8%E5%AF%BC/" target="_self">
                <time class="text-uppercase">
                    May 09 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><figure  style="flex: 82.77777777777777" ><img width="596" height="360" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/794a90e7b252e2aa03842eeee7fad8de.png" alt=""/></figure>
$
\large
\begin{aligned}
h_t &amp;=\sigma(z_t) = \sigma(Ux_t+Wh_{t-1} + b) \
y_t &amp;= \sigma(Vh_t + c)
\end{aligned}
$<h2>梯度消失与爆炸</h2>
<p>假设一个只有 3 个输入数据的序列，此时我们的隐藏层 h1、h2、h3 和输出 y1、y2、y3 的计算公式：</p><p>$
\large
\begin{aligned}
h_1 &amp;= \sigma(Ux_1 + Wh_0 + b) \
h_2 &amp;= \sigma(Ux_2 + Wh_1 + b) \
h_3 &amp;= \sigma(Ux_3 + Wh_2 + b) \
y_1 &amp;= \sigma(Vh_1 + c) \
y_2 &amp;= \sigma(Vh_2 + c) \
y_3 &amp;= \sigma(Vh_3 + c)
\end{aligned}
$</p><p>RNN 在时刻 t 的损失函数为 Lt，总的损失函数为 $L = L1 + L2 + L3 \Longrightarrow  \sum_{t=1}^TL_T$</p><p>t = 3 时刻的损失函数 L3 对于网络参数 U、W、V 的梯度如下：</p><p>$$
\begin{aligned}
\frac{\partial L_3}{\partial V} &amp;= \frac{\partial L_3}{\partial y_3} \frac{\partial y_3}{\partial V} \\
\frac{\partial L_3}{\partial U} &amp;= \frac{\partial L_3}{\partial y_3} \frac{\partial y_3}{\partial h_3} \frac{\partial h_3}{\partial U} + \frac{\partial L_3}{\partial y_3} \frac{\partial y_3}{\partial h_3} \frac{\partial h_3}{\partial h_2} \frac{\partial h_2}{\partial U} + \frac{\partial L_3}{\partial y_3} \frac{\partial y_3}{\partial h_3} \frac{\partial h_3}{\partial h_2} \frac{\partial h_2}{\partial h_1} \frac{\partial h_1}{\partial U} \\
\frac{\partial L_3}{\partial W} &amp;= \frac{\partial L_3}{\partial y_3} \frac{\partial y_3}{\partial h_3} \frac{\partial h_3}{\partial W} 
\+ \frac{\partial L_3}{\partial y_3} \frac{\partial y_3}{\partial h_3} \frac{\partial h_3}{\partial h_2} \frac{\partial h_2}{\partial W} 
\+ \frac{\partial L_3}{\partial y_3} \frac{\partial y_3}{\partial h_3} \frac{\partial h_3}{\partial h_2} \frac{\partial h_2}{\partial h_1} \frac{\partial h_1}{\partial W} \\
\end{aligned}
$$</p>
<p>其实主要就是因为：</p><ul>
<li>对V求偏导时，$h_3$是常数</li>
<li>对U求偏导时：<ul>
<li>$h_3$里有U，所以要继续对h3应用<code>chain rule</code></li>
<li>$h_3$里的$W, b$是常数，但是$h_2$里又有U，继续<code>chain rule</code></li>
<li>以此类推，直到$h_0$</li>
</ul>
</li>
<li>对W求偏导时一样</li>
</ul>
<p>所以：</p><ol>
<li>参数矩阵 V (对应输出 $y_t$) 的梯度很显然并没有长期依赖</li>
<li>U和V显然就是连乘($\prod$)后累加($\sum$)</li>
</ol>
<p>$$
\begin{aligned}
\frac{\partial L_t}{\partial U} = \sum_{k=0}^{t} \frac{\partial L_t}{\partial y_t} \frac{\partial y_t}{\partial h_t}
(\prod_{j=k+1}^{t}\frac{\partial h_j}{\partial h_{j-1}})
\frac{\partial h_k}{\partial U} \\
\frac{\partial L_t}{\partial W} = \sum_{k=0}^{t} \frac{\partial L_t}{\partial y_t} \frac{\partial y_t}{\partial h_t}
(\prod_{j=k+1}^{t}\frac{\partial h_j}{\partial h_{j-1}})
\frac{\partial h_k}{\partial W}
\end{aligned}
$$</p>
<p>其中的连乘项就是导致 RNN 出现梯度消失与梯度爆炸的罪魁祸首，连乘项可以如下变换：</p><ul>
<li>$h_j = tanh(Ux_j + Wh_{j-1} + b)$</li>
<li>$\prod_{j=k+1}^{t}\frac{\partial h_j}{\partial h_{j-1}} =\prod_{j=k+1}^{t} tanh' \times W$</li>
</ul>
<p>tanh' 表示 tanh 的导数，可以看到 RNN 求梯度的时候，实际上用到了 (tanh' × W) 的连乘。当 (tanh' × W) &gt; 1 时，多次连乘容易导致梯度爆炸；当 (tanh' × W) &lt; 1 时，多次连乘容易导致梯度消失。</p></div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/RNN%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E6%8E%A8%E5%AF%BC/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/RNN%E4%B8%ADbidirectional%E5%92%8Cnum_layer%E5%AF%B9output%E5%92%8Chidden%E5%BD%A2%E7%8A%B6%E7%9A%84%E5%BD%B1%E5%93%8D/" target="_self">RNN中bidirectional和num_layer对output和hidden形状的影响</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/RNN%E4%B8%ADbidirectional%E5%92%8Cnum_layer%E5%AF%B9output%E5%92%8Chidden%E5%BD%A2%E7%8A%B6%E7%9A%84%E5%BD%B1%E5%93%8D/" target="_self">
                <time class="text-uppercase">
                    April 13 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><h2>Batch first</h2>
<p>首先，我们要习惯接受<code>batch_first=False</code>（就是默认值）的思维，因为NLP中批量处理句子，是每一句取第一个词，第二个词，以此类推。
按我们习惯的把数据放在同一批（即<code>batch_first=True</code>）的思路虽然可以做到（善用切片即可），但是绕了弯路。但是如果第1批都是第1个字，第2批全是第2个字，这会自然很多（<strong>行优先</strong>）。</p><p>所以至少<code>Pytorch</code>内部，你设了True，内部也是按False来处理的，只是给了你一个语法糖（当然你组织数据就必须按True来组织了。</p><p>看个实例：</p><figure  style="flex: 79.69151670951157" ><img width="1240" height="778" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/d6a0d9ea71601f8b89c0e8465751baf4.png" alt=""/></figure><ol>
<li>假定批次是64，句长截为70，在还没有向量化的数据中，那么显然一次的输入应该为(70x64)，批次在第2位</li>
<li>注意第一行，全是2，这是设定的<code>&lt;bos&gt;</code>，这已经很好地表示了在行优先的系统里（比如<code>Matlab</code>就是列优先），会自然而且把<strong>每句话</strong>的第一个词读出来的设定了。</li>
</ol>

<pre><code># 我用的torchtext的Field进行演示， SRC是一个Field
[SRC.vocab.itos[i] for i in range(1,4)]  
['&lt;pad&gt;', '&lt;bos&gt;', '&lt;eos&gt;']
</code></pre>
<ol start="3">
<li>可见，2是开始，3是结束，1是空格（当然这是我设置的）</li>
<li>同时也能注意到，最后一行有的是3，有的是1，有的都不是，就说明句子是以70为长度进行截断的，自然结束的是3，补<code>&lt;pad&gt;</code>的是1，截断的那么那个字是多少就是多少</li>
<li>竖向取一条就是一整句话，打印出来就是箭头指向的那一大坨（共70个数字）</li>
<li>对它进行<code>index_to_string</code>(itos)，则还原出了这句话</li>
<li>nn.Embedding做了两件事：</li>
</ol>
<ul>
<li>根据vocabulary进行one-hot（稀疏）$\rightarrow$ 所以你要告诉它词典大小</li>
<li>然后再embedding成指定的低维向量（稠密）</li>
<li>所以70个数字就成了70x300，拼上维度，就是70x64x300</li>
</ul>
<p>既然讲到这了，多讲两行，假定hidden_dim=256, 一个<code>nn.RNN</code>会输出的<code>outputs</code>和<code>hidden</code>的形状如下：</p>
<pre><code>&gt;&gt;&gt; outputs.shape
torch.Size([70, 64, 256])
&gt;&gt;&gt; hidden.shape
torch.Size([1, 64, 256])
</code></pre>
<ol>
<li>即300维进去，256维出来，但是因为句子有70的长度，那就是70个output，hidden是从前传到后的，当然是最后一个</li>
<li>也因此，如果你不需要叠加多层RNN，你只需要最后一个字的output就行了<code>outputs[-1,:,:]</code>, 这个结果送到全连接层里去进行分类。</li>
</ol>
<h2>自己写一个RNN</h2>
<p>其实就是要自己把上述形状变化做对就行了。就是几个线性变换，所以我们用<code>nn.Linear</code>来拼接:</p><ol>
<li>input: 2x5x3 $\Rightarrow$ 5个序列，每一个2个词，每个词用3维向量表示</li>
<li>hidden=10, 无embedding，num_class=7</li>
<li>期待形状：</li>
</ol>
<ul>
<li>output: 2x5x7</li>
<li>hidden:1x5x10</li>
</ul>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">RNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">i2h</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">i2o</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">):</span>
        <span class="n">combined</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="nb">input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
        <span class="c1"># input shape: (2, 5, 3)</span>
        <span class="c1"># hidden shape: (2, 5, 10)</span>
        <span class="c1"># combine shape (2, 5, 13)</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">i2h</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">i2o</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span>

    <span class="k">def</span> <span class="nf">initHidden</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>

<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">num_class</span> <span class="o">=</span> <span class="mi">7</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">,(</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>

<span class="n">rnn</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span><span class="n">hidden_size</span><span class="p">,</span><span class="n">num_class</span><span class="p">)</span>
<span class="n">out</span><span class="p">,</span> <span class="n">hid</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">))</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">hid</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
<p>output:</p>
<pre><code>(torch.Size([2, 5, 7]), torch.Size([2, 5, 10]))
</code></pre>
<p>可见，output是一样的，hidden的形状不一样，事实上每一个字确实是会产生hidden的，但是pytorch并没有把它返出来（消费掉就没用了）。这里就pass了，我们主要是看一下双向和多层的情况下形状的变化，下面我们用pytorch自己的RNN来测试。</p><h1>num_layers</h1>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">num_layers</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span> <span class="c1"># 几层就需要初始几个hidden</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># input: 5x3 -&gt; 1x12 # N个批次， 5个序列(比如5个字，每个字由3个数字的向量组成)</span>
<span class="n">o</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span> <span class="c1"># 5个output, 一个final hidden</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;output shape&#39;</span><span class="p">,</span> <span class="n">o</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;hidden shape&#39;</span><span class="p">,</span> <span class="n">h</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<p>输出：</p>
<pre><code>output shape torch.Size([5, 2, 12])  # 2个批次，5个词，12维度输出
hidden shape torch.Size([3, 2, 12]) # 3层会输出3个hidden，2个批次
</code></pre>
<p>加上embedding, RNN改成GRU</p><div class="highlight"><pre><span></span><span class="c1"># 这次加embedding</span>
<span class="c1"># 顺便把 RNN 改 GRU</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">embed_size</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">3</span>
<span class="c1"># 要求词典长度不超过5，输出向量长度为10</span>
<span class="n">emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span> 
<span class="c1"># 输入为embeding维度，输出（和隐层）为8维度</span>
<span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># 这次设了num_layers=2，就要求有两个hidden了</span>
<span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
<span class="c1"># 因为数据会用embedding包一次，所以input没有了维度要求（只有大小要求，每个数字要小于字典长度）</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">))</span> 
<span class="n">e</span> <span class="o">=</span> <span class="n">emb</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;input.shape:&#39;</span><span class="p">,</span> <span class="n">x0</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;embedding.shape:&#39;</span><span class="p">,</span> <span class="n">e</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (3,4)会扩展成（3,4,10), 10维是rnn的input维度，正好</span>
<span class="n">o</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;output.shape:</span><span class="si">{</span><span class="n">o</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">, hidden.shape:</span><span class="si">{</span><span class="n">h</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

<pre><code>input.shape: torch.Size([5, 3])
embedding.shape: torch.Size([5, 3, 10])
output.shape:torch.Size([5, 3, 8]), hidden.shape:torch.Size([2, 3, 8])
</code></pre>
<p>唯一要注意的变化就是input，因为embedding是把字典大小的维度转换成指定大小的维度，暗含了你里面的每一个数字都是字典的索引，所以你组装demo数据的时候，要生成小于字典大小(<code>vocab_size</code>）的数字作为输入。</p><h2>bidirectional</h2>
<p>这次加<strong>bidirectional</strong></p><ul>
<li>batch_first = False</li>
<li>x (5, 3) -&gt; 3个序列，每个序列5个数</li>
<li>embedding(5, 10) -&gt; 输入字典长5，输出向量长10 -&gt; (5, 3, 10) -&gt; 3个序列，每个序列5个10维向量</li>
<li>hidden必须为8维，4个（num_layers=2, bidirection),3个批次 -&gt; (4,3,8)</li>
<li>rnn(10, 8) -&gt; 输入10维，输出8维</li>
</ul>
<div class="highlight"><pre><span></span><span class="c1"># 这次加 bidirection</span>

<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">embed_size</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">num_layers</span> <span class="o">=</span> <span class="mi">2</span>
<span class="c1"># 要求词典长度不超过5，输出向量长度为10</span>
<span class="n">emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span> 
<span class="c1"># 输入为embeding维度，输出（和隐层）为8维度</span>
<span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># 这次设了num_layers=2，就要求有两个hidden了</span>
<span class="c1"># 加上双向，就有4个了，这里乘以2</span>
<span class="c1"># h0 = (torch.rand(2, batch_size, hidden_size), torch.rand(2, batch_size, hidden_size))</span>
<span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">num_layers</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
<span class="c1"># 因为数据会用embedding包一次，所以input没有了维度要求（只有大小要求，每个数要小于字典长度）</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">))</span> 
<span class="n">e</span> <span class="o">=</span> <span class="n">emb</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;input.shape:&#39;</span><span class="p">,</span> <span class="n">x0</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;embedding.shape:&#39;</span><span class="p">,</span> <span class="n">e</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (3,4)会扩展成（3,4,10), 10维是rnn的input维度，正好</span>
<span class="c1"># hidden = torch.cat((h0[-2,:,:], h0[-1,:,:]),1)</span>
<span class="n">o</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;output.shape:</span><span class="si">{</span><span class="n">o</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">, hidden.shape:</span><span class="si">{</span><span class="n">h</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

<pre><code>input.shape: torch.Size([5, 3])
embedding.shape: torch.Size([5, 3, 10])
output.shape:torch.Size([5, 3, 16]), hidden.shape:torch.Size([4, 3, 8])
</code></pre>
<p>可见，双向会使输出多一倍，可以用<code>[:hidden_size], [hidden_size:]</code>分别取出来，我们<strong>验证</strong>一下，用框架生成一个双向的GRU，然后手动生成一个正向的一个负向的，复制参数，看一下输出：</p><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="c1"># 制作一个正序和反序的input</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">17</span><span class="p">)</span>
<span class="n">random_input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">normal_</span><span class="p">(),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">reverse_input</span> <span class="o">=</span> <span class="n">random_input</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">:,</span> <span class="p">:]</span>

<span class="n">bi_grus</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">reverse_gru</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">gru</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">reverse_gru</span><span class="o">.</span><span class="n">weight_ih_l0</span> <span class="o">=</span> <span class="n">bi_grus</span><span class="o">.</span><span class="n">weight_ih_l0_reverse</span>
<span class="n">reverse_gru</span><span class="o">.</span><span class="n">weight_hh_l0</span> <span class="o">=</span> <span class="n">bi_grus</span><span class="o">.</span><span class="n">weight_hh_l0_reverse</span>
<span class="n">reverse_gru</span><span class="o">.</span><span class="n">bias_ih_l0</span> <span class="o">=</span> <span class="n">bi_grus</span><span class="o">.</span><span class="n">bias_ih_l0_reverse</span>
<span class="n">reverse_gru</span><span class="o">.</span><span class="n">bias_hh_l0</span> <span class="o">=</span> <span class="n">bi_grus</span><span class="o">.</span><span class="n">bias_hh_l0_reverse</span>
<span class="n">gru</span><span class="o">.</span><span class="n">weight_ih_l0</span> <span class="o">=</span> <span class="n">bi_grus</span><span class="o">.</span><span class="n">weight_ih_l0</span>
<span class="n">gru</span><span class="o">.</span><span class="n">weight_hh_l0</span> <span class="o">=</span> <span class="n">bi_grus</span><span class="o">.</span><span class="n">weight_hh_l0</span>
<span class="n">gru</span><span class="o">.</span><span class="n">bias_ih_l0</span> <span class="o">=</span> <span class="n">bi_grus</span><span class="o">.</span><span class="n">bias_ih_l0</span>
<span class="n">gru</span><span class="o">.</span><span class="n">bias_hh_l0</span> <span class="o">=</span> <span class="n">bi_grus</span><span class="o">.</span><span class="n">bias_hh_l0</span>

<span class="n">bi_output</span><span class="p">,</span> <span class="n">bi_hidden</span> <span class="o">=</span> <span class="n">bi_grus</span><span class="p">(</span><span class="n">random_input</span><span class="p">)</span>
<span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="n">gru</span><span class="p">(</span><span class="n">random_input</span><span class="p">)</span>
<span class="n">reverse_output</span><span class="p">,</span> <span class="n">reverse_hidden</span> <span class="o">=</span> <span class="n">reverse_gru</span><span class="p">(</span><span class="n">reverse_input</span><span class="p">)</span>  <span class="c1"># 分别取[(4,3,2,1,0),:,:] -&gt; 即倒序送入input</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;bi_output:&#39;</span><span class="p">,</span> <span class="n">bi_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">bi_output</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">bi_output</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>                <span class="c1"># 双向输出中的后半截</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">reverse_output</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="p">))</span> <span class="c1"># 反向输出</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>                   <span class="c1"># 单独一个rnn的输出 </span>
<span class="nb">print</span><span class="p">(</span><span class="n">bi_output</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>                <span class="c1"># 双向输出中的前半截</span>
</pre></div>

<pre><code>bi_output: torch.Size([5, 1, 2])
tensor([[-0.2336, -0.3068],
        [ 0.0660, -0.6004],
        [ 0.0859, -0.5620],
        [ 0.2164, -0.5750],
        [ 0.1229, -0.3608]])
tensor([-0.3068, -0.6004, -0.5620, -0.5750, -0.3608])
tensor([-0.3068, -0.6004, -0.5620, -0.5750, -0.3608])
tensor([-0.2336,  0.0660,  0.0859,  0.2164,  0.1229])
tensor([-0.2336,  0.0660,  0.0859,  0.2164,  0.1229])
</code></pre>
<p>现在你们应该知道<code>bidirectional</code>的双倍输出是怎么回事了，再来看看hidden</p><div class="highlight"><pre><span></span><span class="n">hidden</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">reverse_hidden</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">bi_hidden</span><span class="o">.</span><span class="n">shape</span>
<span class="n">bi_hidden</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">reverse_hidden</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">hidden</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">data</span>
</pre></div>

<pre><code>(torch.Size([1, 1, 1]), torch.Size([1, 1, 1]), torch.Size([2, 1, 1]))
(tensor([ 0.1229, -0.3068]), tensor([-0.3068]), tensor([0.1229]))
</code></pre>
<ul>
<li>正向的输出就是单向rnn</li>
<li>反向的输出就是把数据反传的单向rnn</li>
<li>双向rnn出来的第最后一个hidden（后半截）就是反向完成后的hidden</li>
</ul>
<figure  style="flex: 83.6996336996337" ><img width="914" height="546" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/67bb7c8df3810d83a7f07909fbd601f9.png" alt=""/></figure><p>由打印出来的数据可知：</p><ul>
<li>最后一个hidden，就是反向RNN的最后一个hidden（时间点在开头）</li>
<li>也是双向RNN里的第一个输出（<strong>的最后一个元素</strong>）</li>
<li>也是单向RNN（但是数据反传）（或者正向，但逆时序）里的最后一个输出</li>
</ul>
<hr />
<p>双向RNN里</p><ul>
<li>倒数第二个hidden，是正向的最后一个hidden（时间点在结尾）</li>
<li>它也是output里面的值，它是双向输出里的最后一个的<strong>第一个元素</strong></li>
</ul>
<p>总的来说</p><ul>
<li>output由正反向输出横向拼接（所有）</li>
<li>hidden由正反向hidden竖向拼接（top layer)</li>
</ul>
<figure  style="flex: 71.92575406032482" ><img width="1240" height="862" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/671265db99cdb4c1fcda808d82a08794.png" alt=""/></figure></div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/RNN%E4%B8%ADbidirectional%E5%92%8Cnum_layer%E5%AF%B9output%E5%92%8Chidden%E5%BD%A2%E7%8A%B6%E7%9A%84%E5%BD%B1%E5%93%8D/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/%E5%87%A0%E7%A7%8D%E6%95%99%E6%9D%90%E9%87%8C%E6%B1%82%E8%A7%A3Ax%3D0%E7%AC%94%E8%AE%B0/" target="_self">几种教材里求解Ax=0笔记</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/%E5%87%A0%E7%A7%8D%E6%95%99%E6%9D%90%E9%87%8C%E6%B1%82%E8%A7%A3Ax%3D0%E7%AC%94%E8%AE%B0/" target="_self">
                <time class="text-uppercase">
                    March 05 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><p>如果一个矩阵化简为
$$
A=
\left[
    \begin{array}{cccc|c}
    1&amp;2&amp;2&amp;2&amp;0 \
    0&amp;0&amp;1&amp;2&amp;0 \
    0&amp;0&amp;0&amp;0&amp;0
    \end{array}
\right] \tag{0}
$$</p><p>求解$\bf{A}\it\vec{x}=0$</p><p>对比在不同教材中的解题思路。</p><h2>可汗学院解法</h2>
<p>先继续化简为<code>Reduced Row Echelon Form</code> (RREF)
$$
\left[
    \begin{array}{cccc|c}
    1&amp;2&amp;0&amp;-2&amp;0 \
    0&amp;0&amp;1&amp;2&amp;0 \
    0&amp;0&amp;0&amp;0&amp;0
    \end{array}
\right] \tag{1. 1}
$$</p><p>还原为方程组:
$$ 
\begin{cases}
    x_1=-2x_2+2x_4 \
    x_3=-2x_4\
\end{cases} \tag{1.2}
$$</p><p>用$x_2$和$x_4$来表示$x_1$和$x_3$，填满矩阵相应位置即可得解：
$$
\left[\begin{smallmatrix} x_1\x_2\x_3\x_4 \end{smallmatrix}\right]=
x_2 \left[\begin{smallmatrix} -2\1\0\0 \end{smallmatrix}\right] +
x_4 \left[\begin{smallmatrix} 2\0\-2\1 \end{smallmatrix}\right] \tag{1.3}
$$
如果不是太直观的话，其实就是把以下方程写成了矩阵的形式：
$$
\begin{cases}
    x_1=-2x_2+2x_4 \
    x_2=x_2\
    x_3=-2x_4\
    x_4=x_4
\end{cases}\tag{1. 4}
$$</p><hr />
<h2>剑桥教材解法</h2>
<blockquote>
<p>《Mathematics for Machine Learning》</p></blockquote>
<p>by Marc Peter Deisenroth, A Aldo Faisal, Cheng Soon Ong,
Cambridge University</p><p>化简为<code>RREF</code>后，观察到$c_1$和$c_3$列可组成一个单位矩阵（<code>identity matrix</code>）$\left[\begin{smallmatrix} 1&amp;0\0&amp;1 \end{smallmatrix}\right]$</p><blockquote>
<p>如果是解$\bf{A}\it\vec{x}=b$，此时可用此矩阵求出特解，但此处是0，所以此步省略，直接求通解</p></blockquote>
<p>我们用$c_1$和$c_3$来表示其它列：
$$
\begin{cases}
c_2=2c_1 \
c_4=-2c_1+2c_3
\end{cases} \tag{2.1}
$$
我们利用$c_2-c_2=0, c_4-c_4=0$来构造0值（通解都是求0）：
$$
\begin{cases}
2c_1-\color{green}{c_2}=0 \
-2c_1+2c_3-\color{green}{c_4}=0
\end{cases} \tag{2.2}
$$
补齐方程，整理顺序（以便直观地看到系数）得：
$$
\begin{cases}
\color{red}2c_1\color{red}{-1}c_2+\color{red}{0}c_3+\color{red}{0}c_4=0 \
\color{red}{-2}c_1+\color{red}0c_2+\color{red}2c_3\color{red}{-1}c_4=0
\end{cases} \tag{2. 3}
$$</p><p>因为矩阵乘向量可以理解为矩阵和<code>列向量</code>$\vec{c}$与向量$x$的点积之和$\sum_{i=1}^4 x_ic_i$，所以红色的系数部分其实就是$(x_1, x_2, x_3, x_4)$，得解：
$$
\left{x\in\mathbb{R}^4:x=\lambda_1\left[\begin{smallmatrix} 2\-1\0\0 \end{smallmatrix}\right]+\lambda_2\left[\begin{smallmatrix} 2\0\-2\1 \end{smallmatrix}\right],\lambda_1,\lambda_2\in\mathbb{R}\right} \tag{2.4}
$$</p><blockquote>
<p>与<strong>可汗学院</strong>的解得到的两个向量比较下，是一样的，都是$[2,-1,0,0]^T$和$[2,0,-2,1]^T$。</p></blockquote>
<hr />
<p>##麻省理工教材解法</p><blockquote>
<p>《Introduction to Linear Alegebra》</p></blockquote>
<p>by Gilbert Strang, 
Massachusetts Institute of Technology</p><p>无需继续化简为<code>RREF</code>，直接对方程组：
$$ 
\begin{cases}
    x_1=-2x_2+2x_4 \
    x_3=-2x_4\
\end{cases} \tag{3.1}
$$
使用特解。考虑到$x_1,x_3$为主元（<code>pivot</code>），那么分别设$[\begin{smallmatrix} x_2 \ x_4 \end{smallmatrix}]$ 为$[\begin{smallmatrix} 1 \ 0 \end{smallmatrix}]$ 和$[\begin{smallmatrix} 0 \ 1 \end{smallmatrix}]$ 。
两种情况各代入一次，解出$x_1,x_3$，仍然是$[2,\color{red}{-1},0,\color{red}0]^T$和$[2,\color{red}0,-2,\color{red}1]^T$，红色标识了代入值，黑色即为代入后的解。</p><p><code>MIT</code>不止提供了这一个思路，解法二如下：</p><p>这次需要化简为<code>RREF</code>，然后互换第<code>2</code>列和第<code>3</code>列（<strong><code>记住这次互换</code></strong>），还记得剑桥的方法里发现$c_1,c_3$能组成一个单位矩阵吗？这里的目的是通过移动列，直接在表现形式上变成单位矩阵：
$$
\left[
    \begin{array}{cc:cc}
    1&amp;0&amp;2&amp;-2\
    0&amp;1&amp;0&amp;2\
    \hdashline
    0&amp;0&amp;0&amp;0
    \end{array}
\right] \tag{3.2}
$$
这里把用虚线反矩阵划成了四个区，左上角为一个<code>Identity Matrix</code>，我们记为<code>I</code>，右上角为自由列，我们记为<code>F</code>，矩阵（这次我们标记为<strong>R</strong>）变成了
$$
\bf{\it{R}}=
\begin{bmatrix}
I&amp;F\
0&amp;0
\end{bmatrix} \tag{3. 3}
$$
求解$\bf{\it{R}}\it\vec{x}=0$，得到$x=\left[\begin{smallmatrix} -F\I \end{smallmatrix}\right]$，把<strong>F</strong>和<strong>I</strong>分别展开(<code>记得F要乘上-1</code>)：
$$
\begin{bmatrix}
-2&amp;2\
0&amp;-2\
1&amp;0\
0&amp;1
\end{bmatrix} \tag{3.4}
$$
还记得前面加粗提示的交换了两列吗？我们交换了两列，倒置后，我们要把第<code>2, 3</code><strong>行</strong>给交换一下：
$$
\begin{bmatrix}
-2&amp;2\
1&amp;0\
0&amp;-2\
0&amp;1
\end{bmatrix} \tag{3.5}
$$</p><p>是不是又得到了两个熟悉的$[2,-1,0,0]^T$和$[2,0,-2,1]^T$。？</p><blockquote>
<p>当时看到Gilbert教授简单粗暴地用$[\begin{smallmatrix} 1 \ 0 \end{smallmatrix}]$ 和$[\begin{smallmatrix} 0 \ 1 \end{smallmatrix}]$ 直接代入求出解，道理都不跟你讲，然后又给你画大饼，又是F又是I的，觉得可能他的课程不适合初学者，LOL。不过，这些Gilbert教授在此演示的解法并不适用于$\bf{A}\it\vec{x}=b$。</p></blockquote>
<p>在此特用笔记把几本教材里的思路都记录一下。</p></div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/%E5%87%A0%E7%A7%8D%E6%95%99%E6%9D%90%E9%87%8C%E6%B1%82%E8%A7%A3Ax%3D0%E7%AC%94%E8%AE%B0/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/scikit-learn%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0%28%E4%B8%80%29/" target="_self">scikit-learn官网教程笔记(一)</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/scikit-learn%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0%28%E4%B8%80%29/" target="_self">
                <time class="text-uppercase">
                    March 02 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><p>当初翻scikit learn文档的时候，越翻越多，干脆把它的教程拿出来看了看，只有前面的部分，主要想看看scikit learn角度整理的知识体系，果然，一开始就是从监督和非监督讲起：</p><h2>Machine learning</h2>
<p>In general, a learning problem considers a set of n samples of data and then tries to predict properties of unknown data.</p><p>Learning problems fall into a few categories:</p><ul>
<li><a href="https://scikit-learn.org/stable/supervised_learning.html#supervised-learning">supervised learning</a>, in which the data comes with additional attributes that we want to predict. This problem can be either:<ul>
<li><strong>classification</strong>: samples belong to two or more classes and we want to learn from already labeled data how to predict the class of unlabeled data.</li>
<li><strong>regression</strong>: if the desired output consists of one or more continuous variables, then the task is called regression.</li>
</ul>
</li>
<li><a href="https://scikit-learn.org/stable/unsupervised_learning.html#unsupervised-learning">unsupervised learning</a>, in which the training data consists of a set of input vectors x without any corresponding target values.<ul>
<li><strong>clustering</strong>: The goal in such problems may be to discover groups of similar examples within the data</li>
<li><strong>density estimation</strong>: to determine the <em>distribution</em> of data within the input space</li>
<li><strong>down dimensional</strong>: project the data from a high-dimensional space down to two or three dimensions for the purpose of visualization.</li>
</ul>
</li>
</ul>
<h2>dataset</h2>
<p><a href="https://scikit-learn.org/stable/datasets/loading_other_datasets.html#external-datasets">see more</a> dataset load method</p><h3>data, targets</h3>
<p>A <code>dataset</code> is a dictionary-like object that holds all the data and some metadata about the data. This data is stored in the <code>.data</code> member, which is a n_samples, n_features array. In the case of supervised problem, one or more response variables are stored in the <code>.target</code> member.</p><h2>estimator</h2>
<p>In scikit-learn, an <code>estimator</code> for classification is a Python object that <strong>implements</strong> the methods <code>fit(X, y)</code> and <code>predict(T)</code>,  an estimator is any object that learns from data</p><p>An example of an estimator is the class <code>sklearn.svm.SVC</code>, which implements <code>support vector classification</code>.</p><ul>
<li>estimator.param1 表示传入的参数</li>
<li>estimator.param1_   表示estimated param</li>
</ul>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="n">iris</span>   <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">digits</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_digits</span><span class="p">()</span>

<span class="c1"># digits.data.shape,  (1797, 64)</span>
<span class="c1"># digits.images.shape, (1797, 8, 8)</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="c1"># first, we treat the estimator as a black box, and set params manually</span>
<span class="c1"># or we can use `grid search` and `cross validation` to determine the best params</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">100.</span><span class="p">)</span>

<span class="c1"># train(or learn) from all (except last one) digits</span>
<span class="c1"># validate with the last digit</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:])</span>
</pre></div>

<pre><code>array([8])
</code></pre>
<figure  style="flex: 50.81967213114754" ><img width="496" height="488" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/5f41cd44fe4cf94f9dbfee9954b82ec6.png" alt=""/></figure><div class="highlight"><pre><span></span><span class="c1"># better practise</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># flatten the images</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># Create a classifier: a support vector classifier</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="c1"># Split data into 50% train and 50% test subsets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Learn the digits on the train subset</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predict the value of the digit on the test subset</span>
<span class="n">predicted</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
<h2>classification_report</h2>
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report">classification_report</a> builds a text report showing the main classification metrics.</p><h2>confusion matrix</h2>
<p><a href="https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix">plot_confusion_matrix</a> can e used to visually represent a confusion matrix:</p><p>$
\begin{array}{c|c}
&amp; Positive &amp; Negative \
\hline
True &amp; TP &amp; TN \
\hline
False &amp; FP &amp; FN
\end{array}
$</p><p>混淆矩阵还有另一种写法，即横纵轴都以positive，和negative表示，而不是如上的一个是指标，一个是判断（正确，错误）。这些都不重要，自己看清楚不要臆测就好了。
如果：
$
\begin{array}{c|c}
&amp; Positive &amp; Negative \
\hline
True &amp;3 &amp; 4 \
\hline
Fa lse &amp;1 &amp; 2
\end{array}
$</p><p>解读：</p><ul>
<li>预测Positive共4例，成功3，失败1</li>
<li>预测Negative共6例，成功4，失败2</li>
</ul>
<p>所以反推正样本3+2=5，负样1+4=5，共10例</p><p>大原则，我们看到的时候已经是结果，所以只能从结果反推真实情况，比如正确的<code>正</code>和错误的<code>负</code>，加起来就是样本的<code>正</code>，等等</p><ul>
<li>准确率：7/10 （TP+TN/total) 根据上文的文字描述，其实就是判断成功的次数</li>
<li>精确率：3/4 (TP/TP+FP) 即<strong>只关注一个指标</strong>(等于是竖向统计），比如正例，或负例，然后观察它错了多少。<ul>
<li>本例中，只预测了4个正，就错了一个</li>
</ul>
</li>
<li>召回率：3/5 (TP/TP+FN) 仍然只关注一个指标，比如正例，但是召回率关心你把所有的“正例“找出来多少<ul>
<li>也就是说，如果你把所有的样本判断为正例，召回率可达100%</li>
</ul>
</li>
</ul>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Classification report for classifier </span><span class="si">{</span><span class="n">clf</span><span class="si">}</span><span class="s2">:</span><span class="se">\n</span><span class="s2">&quot;</span>
      <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>

<pre><code>Classification report for classifier SVC(gamma=0.001):
              precision    recall  f1-score   support

           0       1.00      0.99      0.99        88
           1       0.99      0.97      0.98        91
           2       0.99      0.99      0.99        86
           3       0.98      0.87      0.92        91
           4       0.99      0.96      0.97        92
           5       0.95      0.97      0.96        91
           6       0.99      0.99      0.99        91
           7       0.96      0.99      0.97        89
           8       0.94      1.00      0.97        88
           9       0.93      0.98      0.95        92

    accuracy                           0.97       899
   macro avg       0.97      0.97      0.97       899
weighted avg       0.97      0.97      0.97       899
</code></pre>
<div class="highlight"><pre><span></span><span class="n">disp</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">figure_</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Confusion matrix:</span><span class="se">\n</span><span class="si">{</span><span class="n">disp</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
<figure  style="flex: 54.00696864111498" ><img width="620" height="574" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/2088af7c0ab08ef8b680eab7d06d4bcd.png" alt=""/></figure><h2>Conventions</h2>
<h3>Type Casting</h3>
<p>Unless otherwise specified, input will be cast to float64:</p><div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">random_projection</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">X</span><span class="o">.</span><span class="n">dtype</span>
<span class="n">dtype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">transformer</span> <span class="o">=</span> <span class="n">random_projection</span><span class="o">.</span><span class="n">GaussianRandomProjection</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">X_new</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">X_new</span><span class="o">.</span><span class="n">dtype</span>
<span class="n">dtype</span><span class="p">(</span><span class="s1">&#39;float64&#39;</span><span class="p">)</span>
</pre></div>
<p>the example above, the <code>float32</code> X is casst to <code>float64</code> by <code>fit_transform(X)</code></p><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">()</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="mi">3</span><span class="p">])))</span>

<span class="c1"># fit string</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="mi">3</span><span class="p">])))</span>
<span class="c1"># [&#39;setosa&#39;, &#39;setosa&#39;, &#39;setosa&#39;]</span>
</pre></div>

<pre><code>[0, 0, 0]
['setosa', 'setosa', 'setosa']
</code></pre>
<h3>Refitting and updating parameters</h3>
<p>Hyper-parameters of an estimator can be updated after it has been <strong>constructed</strong> via the <code>set_params()</code>. then you call <code>fit()</code>, the learned will be overwrite.</p><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>    <span class="c1"># 注意换了种load方式</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">()</span>
<span class="n">clf</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">5</span><span class="p">]))</span>

<span class="n">clf</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">5</span><span class="p">]))</span>
</pre></div>

<pre><code>linear [0 0 0 0 0]
rbf [0 0 0 0 0]
</code></pre>
<h2>Multiclass vs. multilabel fitting</h2>
<p>When using multiclass classifiers, the learning and prediction task that is performed is <strong>dependent on</strong> the format of the target data fit upon:</p><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.multiclass</span> <span class="kn">import</span> <span class="n">OneVsRestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelBinarizer</span>

<span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>

<span class="c1"># 注意一行的写法</span>
<span class="n">classif</span> <span class="o">=</span> <span class="n">OneVsRestClassifier</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">SVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> 
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;1d:&#39;</span><span class="p">,</span> <span class="n">classif</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="c1"># one-hot</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">LabelBinarizer</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;y:&#39;</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;one-hot:&#39;</span><span class="p">,</span> <span class="n">classif</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>  <span class="c1"># 可以看到，已经开始不准确了b</span>
</pre></div>

<pre><code>1d: [0 0 1 1 2]
y: [[1 0 0]
 [1 0 0]
 [0 1 0]
 [0 1 0]
 [0 0 1]]
one-hot: [[1 0 0]
 [1 0 0]
 [0 1 0]
 [0 0 0]
 [0 0 0]]
</code></pre>
<h3>multiple label</h3>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MultiLabelBinarizer</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]</span>  <span class="c1"># 一个instance被赋予多个label，（甚至第4个有3个label)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">MultiLabelBinarizer</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">classif</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>

<pre><code>array([[1, 1, 0, 0, 0],
       [1, 0, 1, 0, 0],
       [0, 1, 0, 1, 0],
       [1, 0, 1, 0, 0],
       [1, 0, 1, 0, 0]])
</code></pre>
<h2>KNN (k nearest neighbors classification)</h2>
<p>KNN是一种用身边最近的n个数据点哪个类别最多来推断自己类别的的方法，所以本质上还是有标签的（周边数据点都是打标的）</p><blockquote>
<p>我还写过一种无监督的<strong>聚类</strong>方法，叫<code>k-means</code>，就因为都有个<code>k</code>，一度都让我混淆了起来。其实没有关系，<code>k-means</code>是随机选k个点当作中心点，找出与它们最近的点来聚类，然后再每个点取中心，这么迭代N次之后，聚类好的数据也会越来越远。这里只是作个旁记。</p></blockquote>

<pre><code>import numpy as np
from sklearn import datasets
iris_X, iris_y = datasets.load_iris(return_X_y=True)
# Split iris data in train and test data
# A random permutation, to split the data randomly
np.random.seed(0)
indices = np.random.permutation(len(iris_X))
iris_X_train = iris_X[indices[:-10]]
iris_y_train = iris_y[indices[:-10]]
iris_X_test = iris_X[indices[-10:]]
iris_y_test = iris_y[indices[-10:]]
# Create and fit a nearest-neighbor classifier
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(iris_X_train, iris_y_train)
print(knn.predict(iris_X_test))
print(iris_y_test)
</code></pre>

<pre><code>[1 2 1 0 0 0 2 1 2 0]
[1 1 1 0 0 0 2 1 2 0]
</code></pre>
<h3>The curse of dimensionality</h3>
<p>nearest neighbor算法，维数越高，需要的数据越多，才能保证在一点的附近有足够多的neighbor。所以一般来说当特征很多时KNN的效果会下降。当然也有例外，某次做一个20个特征的KNN时候，结果居然比随机森林还要好_(:з」∠)_场面一度十分尴尬… <a href="https://www.zhihu.com/question/27836140/answer/145952018">参考</a></p><h2>Shrinkage</h2>
<p>If there are <strong>few</strong> data points per dimension, noise in the observations induces <strong>high variance</strong>:</p><h1>train with very few data</h1>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span> <span class="mf">.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
<span class="n">regr</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span> 

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span> 
    <span class="n">this_X</span> <span class="o">=</span> <span class="mf">.1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">+</span> <span class="n">X</span>
    <span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">this_X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test</span><span class="p">))</span> 
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">this_X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
<figure  style="flex: 75.50607287449392" ><img width="746" height="494" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/22a14395fafb62dcbaabdca600191fd6.png" alt=""/></figure><h2>Ridge regression:</h2>
<p>A solution in high-dimensional statistical learning is to shrink the regression coefficients to zero: any two randomly chosen set of observations are likely to be uncorrelated. This is called Ridge regression.</p><p>This is an example of <code>bias/variance</code> tradeoff:</p><p>the <strong>larger</strong> the ridge <code>alpha</code> parameter,</p><ul>
<li>the <strong>higher</strong> the <code>bias</code></li>
<li>the <strong>lower</strong> the <code>variance</code>.</li>
</ul>
<p>lasso 回归和岭回归（ridge regression）其实就是在标准线性回归的基础上分别加入 L1 和 L2 正则化（regularization）。相比直接把一些特征的系数置零，只是把它们的“贡献”变小，即乘一下较低的权重（惩罚，imposing a penalty on the size of the coefficients）。</p><p>Lasso 更多用于估计稀疏样本的系数。</p><p>以下关于几个加了正则的demo和调优是整理笔记整理岔了，不是官方教程里的，但是也是我的学习笔记，正好演示一些demo和cross validation的用法就不删了。</p><ul>
<li>L1-norm (Lasso)</li>
<li>L2-norm (Ridge)</li>
<li>(Elastic Net) (l1+l2)</li>
</ul>
<p>Lasso:<br />
$$J(\theta) = \frac{1}{2}\sum_{i}^{m}(y^{(i)} - \theta^Tx^{(i)})^2 + 
            \color{red} {\lambda \sum_{j}^{n}|\theta_j|}$$</p><p>Ridge:<br />
$$J(\theta) = \frac{1}{2}\sum_{i}^{m}(y^{(i)} - \theta^Tx^{(i)})^2 +
            \color{blue} {\lambda \sum_{j}^{n}\theta_j^2}$$</p><p>ElasticNet:<br />
$$J(\theta) = \frac{1}{2}\sum_{i}^{m}(y^{(i)} - \theta^Tx^{(i)})^2 + 
            \lambda(\rho 
            \color{red}{\sum_{j}^{n}|\theta_j|} + 
            (1-\rho)\color{blue}{ \sum_{j}^{n}\theta_j^2})$$</p><h4>岭回归demo</h4>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_boston</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">boston</span> <span class="o">=</span> <span class="n">load_boston</span><span class="p">()</span>   <span class="c1"># 还记得上一节课 load_iris() 吗？</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">boston</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">boston</span><span class="o">.</span><span class="n">target</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>   <span class="c1"># 用岭回归构建模型</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>                 <span class="c1"># 拟合</span>
<span class="n">train_score</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span> <span class="c1"># 模型对训练样本得准确性</span>
<span class="n">test_score</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>    <span class="c1"># 模型对测试集的准确性</span>

<span class="nb">print</span><span class="p">(</span><span class="n">boston</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="mi">5</span><span class="p">],</span> <span class="n">boston</span><span class="o">.</span><span class="n">target</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;train_score: </span><span class="si">{</span><span class="n">train_score</span><span class="si">}</span><span class="s2">, test_score: </span><span class="si">{</span><span class="n">test_score</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>

<pre><code>[[6.3200e-03 1.8000e+01 2.3100e+00 0.0000e+00 5.3800e-01 6.5750e+00
  6.5200e+01 4.0900e+00 1.0000e+00 2.9600e+02 1.5300e+01 3.9690e+02
  4.9800e+00]
 [2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 6.4210e+00
  7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9690e+02
  9.1400e+00]
 [2.7290e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 7.1850e+00
  6.1100e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9283e+02
  4.0300e+00]
 [3.2370e-02 0.0000e+00 2.1800e+00 0.0000e+00 4.5800e-01 6.9980e+00
  4.5800e+01 6.0622e+00 3.0000e+00 2.2200e+02 1.8700e+01 3.9463e+02
  2.9400e+00]
 [6.9050e-02 0.0000e+00 2.1800e+00 0.0000e+00 4.5800e-01 7.1470e+00
  5.4200e+01 6.0622e+00 3.0000e+00 2.2200e+02 1.8700e+01 3.9690e+02
  5.3300e+00]] [24.  21.6 34.7 33.4 36.2]

train_score: 0.723706995939315, test_score: 0.7926416423787221
</code></pre>
<h4>岭回归调优</h4>
<ul>
<li>Ridge regression is a penalized linear regression model for predicting a numerical value</li>
<li>and it can be very effective when applied to classification</li>
<li>the important parameter to tune is the regularization strength (<code>alpha</code>) in (0.1, 1.0) step = 0.1</li>
</ul>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RidgeCV</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_boston</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">boston</span> <span class="o">=</span> <span class="n">load_boston</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">boston</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">boston</span><span class="o">.</span><span class="n">target</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># 用redge cross validation建模而不是Ridge</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">RidgeCV</span><span class="p">(</span><span class="n">alphas</span><span class="o">=</span><span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.005</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.0005</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">],</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">alpha_</span><span class="p">)</span>
</pre></div>

<pre><code>0.01
</code></pre>
<h4>lass demo和调优</h4>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>

<span class="n">lasso_reg</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">lasso_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;lasso score&quot;</span><span class="p">,</span> <span class="n">lasso_reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>

<span class="c1"># 调优</span>
<span class="n">lscv</span> <span class="o">=</span> <span class="n">LassoCV</span><span class="p">(</span><span class="n">alphas</span><span class="o">=</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.005</span><span class="p">,</span> <span class="mf">0.0025</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.00025</span><span class="p">),</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">lscv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Lasso optimal alpha: </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">lscv</span><span class="o">.</span><span class="n">alpha_</span><span class="p">)</span>
</pre></div>

<pre><code>lasso score 0.7956864030940746
Lasso optimal alpha: 0.010
</code></pre>
<h4>弹性网络</h4>
<p>大多情况下应该避免使用纯线性回归，如果特征数比较少，更倾向于<code>Lasso回归</code>或者<code>弹性网络</code>，因为它们会将无用的特征权重降为0，一般来说<code>弹性网络</code>优于<code>Lasso回归</code></p><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNet</span>

<span class="n">e_net</span> <span class="o">=</span> <span class="n">ElasticNet</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">e_net</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;e_net score:&quot;</span><span class="p">,</span> <span class="n">e_net</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>

<span class="c1"># 调优</span>
<span class="n">encv</span> <span class="o">=</span> <span class="n">ElasticNetCV</span><span class="p">(</span><span class="n">alphas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.005</span><span class="p">,</span> <span class="mf">0.0025</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">),</span> 
                    <span class="n">l1_ratio</span><span class="o">=</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">),</span> 
                    <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">encv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ElasticNet optimal alpha: </span><span class="si">%.3f</span><span class="s1"> and L1 ratio: </span><span class="si">%.4f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">encv</span><span class="o">.</span><span class="n">alpha_</span><span class="p">,</span> <span class="n">encv</span><span class="o">.</span><span class="n">l1_ratio_</span><span class="p">))</span>
</pre></div>

<pre><code>e_net score: 0.7926169728251697
ElasticNet optimal alpha: 0.001 and L1 ratio: 0.5000
</code></pre>
<p>回到教程，对前例（数据过少引起的过拟合），加入了惩罚项后：</p><div class="highlight"><pre><span></span><span class="n">regr</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">.1</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span> 
    <span class="n">this_X</span> <span class="o">=</span> <span class="mf">.1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">+</span> <span class="n">X</span>
    <span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">this_X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test</span><span class="p">))</span> 
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">this_X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span> 

<span class="c1"># 观察图像的不同，其实可以理解为样本过少时的”过拟合“，引入忽略的指标后虽然对训练集的准确率大打折扣，但确实降低了方差</span>
</pre></div>
<figure  style="flex: 76.02459016393442" ><img width="742" height="488" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/97bc036cb6f5c3ce99362eb4bbba4c3d.png" alt=""/></figure><h4>Diabetes dataset</h4>
<p>换个数据源，<code>estimator</code>并不需要更换，如果需要换超参，前文也已经讲过了：</p><div class="highlight"><pre><span></span><span class="n">diabetes_X</span><span class="p">,</span> <span class="n">diabetes_y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_diabetes</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">diabetes_X_train</span> <span class="o">=</span> <span class="n">diabetes_X</span><span class="p">[:</span><span class="o">-</span><span class="mi">20</span><span class="p">]</span>
<span class="n">diabetes_X_test</span>  <span class="o">=</span> <span class="n">diabetes_X</span><span class="p">[</span><span class="o">-</span><span class="mi">20</span><span class="p">:]</span>
<span class="n">diabetes_y_train</span> <span class="o">=</span> <span class="n">diabetes_y</span><span class="p">[:</span><span class="o">-</span><span class="mi">20</span><span class="p">]</span>
<span class="n">diabetes_y_test</span>  <span class="o">=</span> <span class="n">diabetes_y</span><span class="p">[</span><span class="o">-</span><span class="mi">20</span><span class="p">:]</span>

<span class="c1"># observe the alpha and the score:</span>

<span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>  <span class="c1"># log10(-4)到log10(-1)共6个数做alpha</span>
<span class="nb">print</span><span class="p">(</span><span class="n">alphas</span><span class="p">)</span>
<span class="nb">print</span><span class="p">([</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">regr</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">diabetes_X_train</span><span class="p">,</span> <span class="n">diabetes_y_train</span><span class="p">)</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">diabetes_X_test</span><span class="p">,</span> <span class="n">diabetes_y_test</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">%&#39;</span>
       <span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">])</span>
</pre></div>

<pre><code>[0.0001     0.00039811 0.00158489 0.00630957 0.02511886 0.1       ]
['58.51%', '58.52%', '58.55%', '58.56%', '58.31%', '57.06%']
</code></pre>
<h2>Lasso regression</h2>
<p>Lasso = least absolute shrinkage and selection operator</p><p>相比Ridge, Lasso会真的把一些feature系数置0 (<strong>sparse method</strong>)，适用奥卡姆剃刀原理(<strong>Occam’s razor</strong>: prefer simpler models)</p><div class="highlight"><pre><span></span><span class="n">regr</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">Lasso</span><span class="p">()</span>
<span class="n">scores</span> <span class="o">=</span> <span class="p">[</span><span class="n">regr</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
              <span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">diabetes_X_train</span><span class="p">,</span> <span class="n">diabetes_y_train</span><span class="p">)</span>
              <span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">diabetes_X_test</span><span class="p">,</span> <span class="n">diabetes_y_test</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">]</span>
<span class="n">best_alpha</span> <span class="o">=</span> <span class="n">alphas</span><span class="p">[</span><span class="n">scores</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">scores</span><span class="p">))]</span>
<span class="n">regr</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">best_alpha</span>   <span class="c1"># 不链式调用的话不需要用set_params</span>
<span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">diabetes_X_train</span><span class="p">,</span> <span class="n">diabetes_y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
</pre></div>

<pre><code>[   0.         -212.43764548  517.19478111  313.77959962 -160.8303982
   -0.         -187.19554705   69.38229038  508.66011217   71.84239008]
</code></pre>
<h3>Different algorithms for the same problem</h3>
<p>Different algorithms can be used to solve the same mathematical problem. For instance the <code>Lasso</code> object in scikit-learn solves the lasso regression problem using a <code>coordinate descent</code> method, that is efficient on <strong>large datasets</strong>. However, scikit-learn also provides the <code>LassoLars</code> object using the <code>LARS</code> algorithm, which is very efficient for problems in which the weight vector estimated is very <strong>sparse</strong> (i.e. problems with <strong>very few</strong> observations).</p><h2>Classification</h2>
<h3>Logistic Regerssion</h3>
<figure  style="flex: 66.66666666666667" ><img width="400" height="300" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/386b1a1250985ccfdacd014b942dff98.png" alt=""/></figure><div class="highlight"><pre><span></span><span class="n">log</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">1e5</span><span class="p">)</span>
<span class="n">log</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris_X_train</span><span class="p">,</span> <span class="n">iris_y_train</span><span class="p">)</span>
</pre></div>

<pre><code>LogisticRegression(C=100000.0)
</code></pre>
<ul>
<li>The <code>C</code> parameter controls the <strong>amount of regularization</strong> in the LogisticRegression object:<ul>
<li>a large value for <code>C</code> results in <strong>less regularization</strong>.</li>
</ul>
</li>
<li><code>penalty=&quot;l2&quot;</code> gives <strong>Shrinkage</strong> (i.e. non-sparse coefficients),</li>
<li><code>penalty=&quot;l1&quot;</code> gives <strong>Sparsity</strong>.</li>
</ul>
<p><strong>DEMO</strong>: 比较<code>KNN</code>和<code>LogisticRegression</code></p><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">neighbors</span><span class="p">,</span> <span class="n">linear_model</span>

<span class="n">X_digits</span><span class="p">,</span> <span class="n">y_digits</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_digits</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">X_digits</span> <span class="o">=</span> <span class="n">X_digits</span> <span class="o">/</span> <span class="n">X_digits</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>

<span class="n">n_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_digits</span><span class="p">)</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">X_digits</span><span class="p">[:</span><span class="nb">int</span><span class="p">(</span><span class="mf">.9</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">)]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y_digits</span><span class="p">[:</span><span class="nb">int</span><span class="p">(</span><span class="mf">.9</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">)]</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_digits</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="mf">.9</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">):]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">y_digits</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="mf">.9</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">):]</span>

<span class="n">knn</span> <span class="o">=</span> <span class="n">neighbors</span><span class="o">.</span><span class="n">KNeighborsClassifier</span><span class="p">()</span>
<span class="n">logistic</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;KNN score: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;LogisticRegression score: </span><span class="si">%f</span><span class="s1">&#39;</span>
      <span class="o">%</span> <span class="n">logistic</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
</pre></div>

<pre><code>KNN score: 0.961111
LogisticRegression score: 0.933333
</code></pre>
<h2>Support vector machines (SVMs)</h2>
<p>支持向量机(support vector machines,SVM)是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器。除此之外，SVM算法还包括核函数，核函数可以使它成为非线性分类器。</p><p><code>Support Vector Machines</code> belong to the <strong>discriminant model family</strong>:</p><p>they try to find a combination of samples to <strong>build a plane</strong> maximizing the margin between the two classes. <code>Regularization</code> is set by the <code>C</code> parameter:</p><ul>
<li>a <strong>small</strong> value for C means the margin is calculated using <strong>many or all</strong> of the observations around the separating line (more regularization);</li>
<li>a <strong>large</strong> value for C means the margin is calculated on observations <strong>close to</strong> the separating line (less regularization).</li>
</ul>
<p>SVMs can be used：</p><ul>
<li>in regression –<code>SVR</code> (Support Vector Regression)–,</li>
<li>or in classification –<code>SVC</code> (Support Vector Classification).</li>
</ul>
<p>SVM模型有两个非常重要的参数C与gamma。其中:</p><ul>
<li>C是惩罚系数，即对误差的宽容度。c越高，说明越不能容忍出现误差,容易过拟合。C越小，容易欠拟合。C过大或过小，泛化能力变差</li>
<li>gamma是选择RBF函数作为kernel后，该函数自带的一个参数。隐含地决定了数据映射到新的特征空间后的分布，gamma越大，支持向量越少，gamma值越小，支持向量越多。支持向量的个数影响训练与预测的速度。</li>
</ul>
<h3>Using kernels</h3>
<p>Classes are not always <strong>linearly separable</strong> in feature space. The solution is to <code>build a decision function</code> that is not linear but may be <strong>polynomial</strong> instead.</p><p>This is done using the <code>kernel</code> trick that can be seen as creating a decision energy by positioning kernels on observations:</p><h4>Linear kernal</h4>
<div class="highlight"><pre><span></span><span class="n">svc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)</span>
</pre></div>
<h4>Polynomial kernel</h4>
<div class="highlight"><pre><span></span><span class="n">svc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
<h4>RBF kernel (Radial Basis Function)</h4>
<div class="highlight"><pre><span></span><span class="n">svc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rfb&#39;</span><span class="p">)</span>
</pre></div>
<p><strong>DEMO</strong>: Plot different SVM classifiers in the iris dataset</p><ul>
<li>LinearSVC minimizes the squared hinge loss while SVC minimizes the regular hinge loss.</li>
<li>LinearSVC uses the One-vs-All (also known as One-vs-Rest) multiclass reduction while SVC uses the One-vs-One multiclass reduction.</li>
</ul>
<div class="highlight"><pre><span></span><span class="c1"># 代码片段，定义4个estimator</span>
<span class="n">C</span> <span class="o">=</span> <span class="mf">1.0</span>  <span class="c1"># SVM regularization parameter</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">(</span><span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">),</span>
          <span class="n">svm</span><span class="o">.</span><span class="n">LinearSVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">),</span>
          <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">),</span>
          <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">))</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">clf</span> <span class="ow">in</span> <span class="n">models</span><span class="p">)</span>
</pre></div>
<figure  style="flex: 79.81132075471699" ><img width="846" height="530" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/73af5427b414af6455402c92562d3524.png" alt=""/></figure><h2>cross validation</h2>
<p><a href="https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation">https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation</a></p><p><strong>RAW DEMO</strong>: KFold cross-validation:</p><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">svm</span>

<span class="n">X_digits</span><span class="p">,</span> <span class="n">y_digits</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_digits</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">svc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)</span>

<span class="n">score</span> <span class="o">=</span> <span class="n">svc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_digits</span><span class="p">[:</span><span class="o">-</span><span class="mi">100</span><span class="p">],</span> <span class="n">y_digits</span><span class="p">[:</span><span class="o">-</span><span class="mi">100</span><span class="p">])</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_digits</span><span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="p">:],</span> <span class="n">y_digits</span><span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="p">:])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>

<span class="n">X_folds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">X_digits</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># 分成了3个fold</span>
<span class="n">y_folds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">y_digits</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="c1"># We use &#39;list&#39; to copy, in order to &#39;pop&#39; later on</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">X_folds</span><span class="p">)</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="c1"># 取出最后一个fold # 不对，python居然可以Pop任意一个索引</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> <span class="c1"># 把剩下的fold拼回去</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">y_folds</span><span class="p">)</span>
    <span class="n">y_test</span> <span class="o">=</span> <span class="n">y_train</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
    <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">svc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
<span class="n">scores</span>
</pre></div>

<pre><code>0.98
[0.9348914858096828, 0.9565943238731218, 0.9398998330550918]
</code></pre>
<p>Scikit-learn肯定是提供了官方支持的：</p><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">KFold</span><span class="p">,</span> <span class="n">cross_val_score</span>

<span class="c1"># step 1: 用KFold和fold数做一个KFold对象，然后用这个KFold对象去循环（其实就是一个generator)</span>
<span class="c1"># step 2: 每次循环自己手动计算score</span>
<span class="n">k_fold</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="p">[</span><span class="n">svc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_digits</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">y_digits</span><span class="p">[</span><span class="n">train</span><span class="p">])</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_digits</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">y_digits</span><span class="p">[</span><span class="n">test</span><span class="p">])</span> \
         <span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">k_fold</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X_digits</span><span class="p">)]</span>
<span class="n">scores</span>
</pre></div>
<p>[0.9348914858096828, 0.9565943238731218, 0.9398998330550918]</p>
<pre><code>可见官方api分折和我们手动split，每次从后向前取一折做测试集结果是一致的
当然，打印cross_validation的结果也是有封装的：
```python
# 把KFolder对象传入即可
scores = cross_val_score(svc, X_digits, y_digits, cv=k_fold)
print(scores)

# 定制scoring method:
scores = cross_val_score(svc, X_digits, y_digits, cv=k_fold, scoring='precision_macro')
print(scores)
</code></pre>

<pre><code>[0.93489149 0.95659432 0.93989983]
[0.93969761 0.95911415 0.94041254]
</code></pre>
<h5>Cross-validation generators:</h5>
<p><a href="https://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html">see more</a> cross-validation generators:</p><p>$
\begin{array}{l|l|l}
KFold &amp; StratifiedKFold &amp; GroupKFold \
\hline
ShuffleSplit &amp; StratifiedShuffleSplit &amp; GroupShuffleSplit \
\hline
LeaveOneGroupOut &amp; LeavePGroupOut &amp; LeaveOneOut \
\hline
LeavePOut &amp; PredefinedSplit
\end{array}
$</p><h4>Datatransformation with held out data</h4>
<div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
<span class="o">...</span>     <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">X_train_transformed</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_transformed</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">X_test_transformed</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_transformed</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="mf">0.9333</span><span class="o">...</span>

<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">clf</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">preprocessing</span><span class="o">.</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">)</span>
<span class="n">array</span><span class="p">([</span><span class="mf">0.977</span><span class="o">...</span><span class="p">,</span> <span class="mf">0.933</span><span class="o">...</span><span class="p">,</span> <span class="mf">0.955</span><span class="o">...</span><span class="p">,</span> <span class="mf">0.933</span><span class="o">...</span><span class="p">,</span> <span class="mf">0.977</span><span class="o">...</span><span class="p">])</span>
</pre></div>
<h4>cross_validate v.s. cross_val_score</h4>
<p>The cross_validate function differs from cross_val_score in two ways:</p><ul>
<li>It allows specifying multiple metrics for evaluation.</li>
<li>It returns a dict containing fit-times, score-times (and optionally training scores as well as fitted estimators) in addition to the test score.</li>
</ul>
<div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">recall_score</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">scoring</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;precision_macro&#39;</span><span class="p">,</span> <span class="s1">&#39;recall_macro&#39;</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">scoring</span><span class="p">)</span>  <span class="c1"># 以字典返回validate几个指标</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="p">[</span><span class="s1">&#39;fit_time&#39;</span><span class="p">,</span> <span class="s1">&#39;score_time&#39;</span><span class="p">,</span> <span class="s1">&#39;test_precision_macro&#39;</span><span class="p">,</span> <span class="s1">&#39;test_recall_macro&#39;</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_recall_macro&#39;</span><span class="p">]</span>
<span class="n">array</span><span class="p">([</span><span class="mf">0.96</span><span class="o">...</span><span class="p">,</span> <span class="mf">1.</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.96</span><span class="o">...</span><span class="p">,</span> <span class="mf">0.96</span><span class="o">...</span><span class="p">,</span> <span class="mf">1.</span>        <span class="p">])</span>
</pre></div>
<h2>grid search</h2>
<p><a href="https://scikit-learn.org/stable/modules/grid_search.html#grid-search">https://scikit-learn.org/stable/modules/grid_search.html#grid-search</a></p><p>对不同参数进行组合遍历，目的是为了<code>maximize the cross-validation score</code></p><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span><span class="p">,</span> <span class="n">cross_val_score</span>
<span class="n">Cs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">svc</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="n">Cs</span><span class="p">),</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_digits</span><span class="p">[:</span><span class="mi">1000</span><span class="p">],</span> <span class="n">y_digits</span><span class="p">[:</span><span class="mi">1000</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;best score:&#39;</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">best_score_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;best estimator.c:&#39;</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">C</span><span class="p">)</span>
<span class="c1"># Prediction performance on test set is not as good as on train set</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_digits</span><span class="p">[</span><span class="mi">1000</span><span class="p">:],</span> <span class="n">y_digits</span><span class="p">[</span><span class="mi">1000</span><span class="p">:])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;score:&#39;</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>
</pre></div>

<pre><code>best score: 0.95
best estimator.c: 0.0021544346900318843
score: 0.946047678795483
</code></pre>
<p>与此同时，每个estimator也有自己的CV版本（跟之前串课的笔记呼应上了）</p><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span><span class="p">,</span> <span class="n">datasets</span>
<span class="n">lasso</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LassoCV</span><span class="p">()</span>
<span class="n">X_diabetes</span><span class="p">,</span> <span class="n">y_diabetes</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_diabetes</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_diabetes</span><span class="p">,</span> <span class="n">y_diabetes</span><span class="p">)</span>
<span class="c1"># The estimator chose automatically its lambda:</span>
<span class="n">lasso</span><span class="o">.</span><span class="n">alpha_</span>
</pre></div>

<pre><code>0.003753767152692203
</code></pre>
<h2>Unsupervised learning: seeking representations of the data</h2>
<h3>Clustreing: grouping observations together</h3>
<h3>K-means clustreing</h3>
<p>随机选k个质心计算所有的点的距离，然后再取每个群里的均值做质心，如此往复。结果的随机性很强（前面剧透了，把k-means和knn搞混过）</p><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">cluster</span><span class="p">,</span> <span class="n">datasets</span>
<span class="n">X_iris</span><span class="p">,</span> <span class="n">y_iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">k_means</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">k_means</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_iris</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">k_means</span><span class="o">.</span><span class="n">labels_</span><span class="p">[::</span><span class="mi">10</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_iris</span><span class="p">[::</span><span class="mi">10</span><span class="p">])</span>
</pre></div>

<pre><code>[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2]
[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2]
</code></pre>
<div class="highlight"><pre><span></span><span class="n">k_means</span><span class="o">.</span><span class="n">cluster_centers_</span>
</pre></div>

<pre><code>array([[5.006     , 3.428     , 1.462     , 0.246     ],
       [5.9016129 , 2.7483871 , 4.39354839, 1.43387097],
       [6.85      , 3.07368421, 5.74210526, 2.07105263]])
</code></pre>
<h3>未完结</h3>
<p><a href="https://scikit-learn.org/stable/tutorial/statistical_inference/unsupervised_learning.html">https://scikit-learn.org/stable/tutorial/statistical_inference/unsupervised_learning.html</a></p></div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/scikit-learn%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0%28%E4%B8%80%29/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/%E4%BB%8E%E6%8A%95%E5%BD%B1%E3%80%81%E6%AD%A3%E4%BA%A4%E8%A1%A5%E8%A7%92%E5%BA%A6%E8%AF%81%E6%98%8E%EF%BC%88%E6%8E%A8%E5%AF%BC%EF%BC%89%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E5%85%AC%E5%BC%8F/" target="_self">从投影、正交补角度证明（推导）最小二乘法公式</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/%E4%BB%8E%E6%8A%95%E5%BD%B1%E3%80%81%E6%AD%A3%E4%BA%A4%E8%A1%A5%E8%A7%92%E5%BA%A6%E8%AF%81%E6%98%8E%EF%BC%88%E6%8E%A8%E5%AF%BC%EF%BC%89%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E5%85%AC%E5%BC%8F/" target="_self">
                <time class="text-uppercase">
                    January 25 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><p>学习线性回归的时候，会教我们$X\theta=y$可以直接用<strong>最小二乘法</strong>直接把$\theta$求出来：</p><p>$\theta=(X^TX)^{-1}X^Ty$</p><p>并且还在我<a href="https://www.jianshu.com/p/c2d0c743dc5d">之前的博文</a>里直接应用了一番（那是根据公式来应用，即如何构建正确的A和y，从而应用公式直接求解$\theta$)，里面还引了一篇详实的证明文章。</p><p>首先，在吴恩达的教材里，这个并不叫最小二乘(<code>least suqare</code>），而是叫<code>Normal Equation method</code>，这个不重要，毕竟在可汗学院的教材里，又叫最小二乘了^^。今天补充的内容，就是在回顾之前的笔记的时候，发现了大量的证明和应用这个公式的地方，而且全是在引入了投影(<code>Projection</code>)概念之后。因为那个时候并没有接触机器学习，看了也就看了，现在看到了应用场景，那就闭环了，回顾一下：</p><p>首先，预备知识</p><h2>子空间</h2>
<figure class="vertical-figure" style="flex: 35.609397944199706" ><img width="970" height="1362" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/454cb3771fb0eb69bf0301b42ad8d9f3.png" alt=""/></figure><p>笔记很清楚了，对于一个矩阵
$A= \begin{bmatrix}
   -2 &amp; -1 &amp; -3 \
   4 &amp; 2 &amp; 6 \
\end{bmatrix}$ 它的列空间是自然是C(A)，行空间自然是A的转置的后的<code>列空间</code>，然后各自拥有一个对应的零空间（即求解$Ax=0, A^Tx=0）$</p><p>上图用红框框出来的部分即是具体这个矩阵$A$的四个子空间。同时，拥有如下性质：</p><ol>
<li>$C(A)$与$N(A^T)$正交(<code>orthogonal</code>)，即列空间与左零空间正交</li>
<li>$C(A^T)$与$N(A)$正交，即行空间与零空间正交</li>
</ol>
<h2>正交补</h2>
<p>$V^\bot = {\vec x \in \R^n | \vec x \cdot \vec{v} = 0; for; every\ \vec{v} \in V \text{}}$ 即V的正交补为垂直于V内任意一个向量的所有向量。</p><p>那么:</p><ul>
<li>$C(A) = (N(A^T))^\bot$</li>
<li>$C(A^T) = (N(A))^\bot$</li>
</ul>
<h2>投影是一个线性变换</h2>
<figure  style="flex: 66.66666666666667" ><img width="1240" height="930" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/d73cc50bd39caeb73a1816384ee8e8d7.png" alt=""/></figure>
这里已经看到我们熟悉的$(A^TA)^{-1}A^Tx$了，我们来看一下推导过程：<ol>
<li>$\vec x$在$V \in \R^n$上的投影$Proj_V^{\vec x} = \vec v$必然能表示成该空间的<code>basis</code>{$\vec b_1, \vec b_2, \vec b_3, \dots$}的线性变换：$\vec v \in V = y_1\vec b_1 + y_2\vec b_2 + \cdots + y_k \vec b_k =  A\vec y$</li>
<li>求出$\vec y$则求出了这个投影在哪里</li>
<li>$\vec x$能向$V$投影，自然也能向$V^\bot$投影($\vec w$)</li>
</ol>
<ul>
<li>这里是故意这么说的，强调都是投影，其实在向$V$投影时，在$V^\bot$的投影（$\vec w$）就是那条<strong>垂线</strong></li>
</ul>
<ol start="4">
<li>$V \Rightarrow C(A),; V^\bot \Rightarrow N(A^T), \vec v \in V, \vec w \in V^\bot$</li>
<li>左零空间只不过是转置的零空间，那么零空间的特性是什么呢？即$A\vec x = 0$的空间，那么$\vec w$在左零空间里，意味着: $A^T\vec w = 0$</li>
<li>$\vec w = \vec x - \vec v = \vec x - A\vec y \Rightarrow A^T(\vec x - A\vec y) = 0 \Rightarrow A^T \vec x = A^TA\vec y$</li>
<li>只要$A^TA$可逆的话: $\Rightarrow \vec  y= (A^TA)^{-1}A^T\vec x$</li>
<li>$\therefore Proj_V^{\vec x} = A\vec y = A(A^TA)^{-1}A^T\vec x$</li>
<li>得证$\vec x$在$V$上的投影就是一个线性变换</li>
<li>$\vec y$即是机器学习中我们需要学习到的<strong>系数</strong> = $(A^TA)^{-1}A^T$</li>
</ol>
<h2>最小二乘逼近</h2>
<p>由此到了下一课，<code>the lease squares approximation</code>，讲的就是$A\vec x = \vec b$无解时，意思就是在$\vec b$不存在A的张成子空间中，所以无论进行怎样的<strong>线性变换</strong>，都是不可能得到$\vec b$的，则取$\vec x$在$C(A)$中的投影作为近似的解（证明就不再展开了）
<figure class="vertical-figure" style="flex: 49.5603517186251" ><img width="1240" height="1251" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/d390aad67946ec06a573bddf79d29339.png" alt=""/></figure>
仍然用的是同一个思路，即&quot;垂线在左零空间中&quot;，来构造$A^T\cdot \vec w = \vec 0$</p><h2>应用最小二乘拟合一条回归线</h2>
<p>这里终于讲到了与机器学习最接近的内容：<code>regression</code>
<figure  style="flex: 164.89361702127658" ><img width="1240" height="376" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/2da3910c441eb8061e11ea708824fd7d.png" alt=""/></figure>
可以看到，毫无业务思维的花花肠子，很多机器学习课程里会花大量工夫从感性到理性上给你讲这些内容，因为它的期望从0跟你讲清楚，而在循序渐进的数学理论体系里，这些根本就不需要关联感性认识的，什么每年的房价啊，数学关注的只是建模。</p><p>这个回归实例里，因为需要拟合的是一条直线：$y = b + ax$，那么既有的数据就成了机器学习里的“样本”，但我们这里不需要这么理解，而是直接理解为矩阵，得到
方程组：
$\begin{cases}
b + a = 1 \
b + 2a = 2 \
b + 3a = 2
\end{cases}$
提取矩阵：
$A = \begin{bmatrix}1&amp;1\1&amp; 2\ 1&amp; 3\end{bmatrix}, \vec b = \begin{bmatrix}1\2\ 2\end{bmatrix}$ $\Rightarrow A\vec x = \vec b$</p><p>好了，在上面提到的<a href="https://www.jianshu.com/p/c2d0c743dc5d">这篇博文</a>里，我们不明就里地直接用了公式，已知A和b求变换矩阵M(即这里的$\vec x$)，还当成是机器学习的内容，而现在我们已经知道自己是在做什么，就是找b在$A$的张成子空间里的投影，就能得到最近似的解</p><p>$$\vec x \approx (A^TA)^{-1}A^T\vec b$$</p>
</div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/%E4%BB%8E%E6%8A%95%E5%BD%B1%E3%80%81%E6%AD%A3%E4%BA%A4%E8%A1%A5%E8%A7%92%E5%BA%A6%E8%AF%81%E6%98%8E%EF%BC%88%E6%8E%A8%E5%AF%BC%EF%BC%89%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E5%85%AC%E5%BC%8F/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/%E7%9F%A9%E9%98%B5%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2%E7%9F%A9%E9%98%B5/" target="_self">矩阵最小二乘法求解仿射变换矩阵</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/%E7%9F%A9%E9%98%B5%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2%E7%9F%A9%E9%98%B5/" target="_self">
                <time class="text-uppercase">
                    January 17 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><p>一个矩形三个顶点<code>(0,0), (50, 0), (50, 50)</code>, 变换后为<code>(30, 30), (30, 130), (130, 130)</code>, 求其仿射矩阵。</p><p>我们分别设起始和结束矩阵的坐标为：$(a_x, a_y), (b_x, b_y), (c_x, c_y)$， 变换后的加一个prime（$ ^\prime$)符号，以此类推。<br />
要知道，一个3X2的矩阵是不可能右乘一个矩阵得到一个3X2的矩阵（只能左乘一个3X3的），<br />
然后，每一个新坐标，都是由原坐标的(x, y)经过变换得到(x', y‘），即使是新坐标的X值，也是需要原坐标的(x, y)值参与过来进行变化的（乘以合适的系数），然后还要加上偏移的系数，以<code>x'</code>为例，应该是这样：$a^\prime_x = a_x m_{00} + a_y m_{01} + m_{02} $
我们根据矩阵特征，补一个1，构造这个矩阵看看效果：
$$
\begin{bmatrix}
\color{red}{a_x} &amp; \color{red}{a_y} &amp; \color{red}1 \
b_x &amp; b_y &amp; 1 \
c_x &amp; c_y &amp; 1 \
\end{bmatrix}
\begin{bmatrix}
\color{red}{m_{00}} \ \color{red}{m_{01}} \ \color{red}{m_{02}}
\end{bmatrix} = 
\begin{bmatrix}
\color{red}{a^\prime_x} \ b^\prime_x \ c^\prime_x
\end{bmatrix} \tag{红色部分即为上面的等式}
$$
这只是把三个x给变换出来了，<strong>其实你也可以认为这是把y给变换出来了</strong>（因为原理一样，只是系数不同）。<br />
做到这一步，我们已经知道要如何求y坐标了，即我们只补一列的话，只能得到一个坐标的x值（或y值），要求另一半，根据坐标相乘的原理，看来只能把前三列置零，再把后三列复制进去了（__这样仿射矩阵也就变成6X1了__），其实就是上面矩阵乘法的重复，只不过交错一下形成x,y交错的排列：
$$
\begin{bmatrix}
a_x &amp; a_y &amp; 1 &amp; 0 &amp; 0 &amp; 0 \
0 &amp; 0 &amp; 0 &amp; a_x &amp; a_y &amp; 1 \
b_x &amp; b_y &amp; 1 &amp; 0 &amp; 0 &amp; 0 \
0 &amp; 0 &amp; 0 &amp; b_x &amp; b_y &amp; 1 \
c_x &amp; c_y &amp; 1 &amp; 0 &amp; 0 &amp; 0 \
0 &amp; 0 &amp; 0 &amp; c_x &amp; c_y &amp; 1 
\end{bmatrix}
\begin{bmatrix}
m_{00} \ m_{01} \ m_{02} \ m_{10} \ m_{11} \ m_{12}
\end{bmatrix} = 
\begin{bmatrix}
a^\prime_x \ a^\prime_y \ b^\prime_x \ b^\prime_y \ c^\prime_x \ c^\prime_y \
\end{bmatrix}
$$
原理当然就是把第一个公式补全：
$$
\begin{cases}
    ; a^\prime_x = a_x m_{00} + a_y m{01} + m_{02} \
    ; a^\prime_y = a_x m_{10} + a_y m{11} + m_{12} \
    \
    ; b^\prime_x = b_x m_{00} + b_y m{01} + m_{02} \
    ; b^\prime_y = b_x m_{10} + b_y m{11} + m_{12} \
    \
    ; c^\prime_x = c_x m_{00} + c_y m{01} + m_{02} \
    ; c^\prime_y = c_x m_{10} + c_y m{11} + m_{12} \
\end{cases}
$$
最小二乘的公式如下：
$$
\begin{equation}
\lVert A\beta - Y \rVert{^2_2} \quad A \in \mathbb{R}^{(m\times n+1)}, \beta \in \mathbb{R}^{(n+1)\times 1}, Y \in \mathbb{R}^{m\times 1} \
\hat \beta = (A^TA)^{-1}A^TY
\end{equation}
$$
<a href="https://iewaij.github.io/introDataScience/OLS.html">推导过程见此</a></p><blockquote>
<p>奇异矩阵没有逆矩阵，$(A^TA)^{-1}$会出现无法求解的问题，也就是该方法对数据是有约束的，这个有解，另议。</p></blockquote>
<p>我们把A和Y都做出来了，直接套用公式即可，为了编程方便，我们把前后矩阵设为A和B，仿射矩阵为M，就成了：
$$
M = (A^TA)^{-1}A^TB
$$</p><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">A</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">]]</span>
<span class="n">B</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">30</span><span class="p">,</span> <span class="mi">30</span><span class="p">],</span> <span class="p">[</span><span class="mi">130</span><span class="p">,</span> <span class="mi">30</span><span class="p">],</span> <span class="p">[</span><span class="mi">130</span><span class="p">,</span> <span class="mi">130</span><span class="p">]]</span>

<span class="c1"># 分别整理成上面分析的6x6和6x1的矩阵</span>
<span class="c1"># 先定义变量保留6个坐标的值</span>
<span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">ay</span><span class="p">),</span> <span class="p">(</span><span class="n">bx</span><span class="p">,</span> <span class="n">by</span><span class="p">),</span> <span class="p">(</span><span class="n">cx</span><span class="p">,</span> <span class="n">cy</span><span class="p">)</span> <span class="o">=</span> <span class="n">A</span>
<span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ay1</span><span class="p">),</span> <span class="p">(</span><span class="n">bx1</span><span class="p">,</span> <span class="n">by1</span><span class="p">),</span> <span class="p">(</span><span class="n">cx1</span><span class="p">,</span> <span class="n">cy1</span><span class="p">)</span> <span class="o">=</span> <span class="n">B</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="n">ax</span><span class="p">,</span> <span class="n">ay</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">ay</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="n">bx</span><span class="p">,</span> <span class="n">by</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">bx</span><span class="p">,</span> <span class="n">by</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="n">cx</span><span class="p">,</span> <span class="n">cy</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">cx</span><span class="p">,</span> <span class="n">cy</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="p">])</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ay1</span><span class="p">,</span> <span class="n">bx1</span><span class="p">,</span> <span class="n">by1</span><span class="p">,</span> <span class="n">cx1</span><span class="p">,</span> <span class="n">cy1</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># 比手写6X1矩阵要省事</span>
<span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">A</span><span class="p">)</span> <span class="o">@</span> <span class="n">A</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">B</span> <span class="c1"># 套公式</span>
<span class="n">M</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
<p>输出：</p><div class="highlight"><pre><span></span>array<span class="o">([[</span> <span class="m">2</span>.,  <span class="m">0</span>., <span class="m">30</span>.<span class="o">]</span>,
       <span class="o">[</span> <span class="m">0</span>.,  <span class="m">2</span>., <span class="m">30</span>.<span class="o">]])</span>
</pre></div>
<hr />
<p>上就是最小二乘的一个应用，也给了一篇链接介绍推导，后来我翻阅学习线代时的笔记，其实有从投影方面给的解释，直观易懂，于是<a href="https://www.jianshu.com/p/39db42a6dd5a">另写了篇博文</a>来介绍这个推导。</p></div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/%E7%9F%A9%E9%98%B5%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2%E7%9F%A9%E9%98%B5/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/add_subplots%E6%96%B9%E6%B3%95%E4%BC%A0%E9%80%92%E9%A2%9D%E5%A4%96%E5%8F%82%E6%95%B0/" target="_self">add_subplots方法传递额外参数</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/add_subplots%E6%96%B9%E6%B3%95%E4%BC%A0%E9%80%92%E9%A2%9D%E5%A4%96%E5%8F%82%E6%95%B0/" target="_self">
                <time class="text-uppercase">
                    January 14 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><p>用matplotlib绘制3D图，最快的用法</p><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>

<span class="n">此后即可</span>  <span class="err">`</span><span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="err">`</span><span class="p">,</span> <span class="err">`</span><span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="err">`</span><span class="n">等</span><span class="err">，</span><span class="n">具体用法请翻阅文档</span>
</pre></div>
<p>其次，这样：</p><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
</pre></div>
<p>而我喜欢要同时绘多栏图的时候喜欢用<code>plt.subplots</code>方法，却发现传不进<code>projection</code>参数，仔细看文档，是支持用<code>subplot_kw</code>来为它添加的subplots来进行属性设置的，这样可以保持外层api干净而不必把subplots的所有属性都复制一遍</p><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="o">**</span><span class="p">{</span><span class="s2">&quot;subplot_kw&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;projection&#39;</span><span class="p">:</span> <span class="s1">&#39;3d&#39;</span><span class="p">}})</span>
<span class="c1"># 或</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">subplot_kw</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">))</span>
</pre></div>
<p>看你自己习惯了。
<figure  style="flex: 53.95996518711923" ><img width="1240" height="1149" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/e131d84109722ad314d449f86313f68e.png" alt=""/></figure></p></div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/add_subplots%E6%96%B9%E6%B3%95%E4%BC%A0%E9%80%92%E9%A2%9D%E5%A4%96%E5%8F%82%E6%95%B0/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/%E4%BA%8C%E5%88%86%E6%B3%95%E3%80%81%E7%89%9B%E9%A1%BF%E6%B3%95%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E5%BC%80%E6%A0%B9%E5%8F%B7%E5%92%8C%E8%A7%A3%E6%96%B9%E7%A8%8B/" target="_self">二分法、牛顿法和梯度下降法开根号和解方程</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/%E4%BA%8C%E5%88%86%E6%B3%95%E3%80%81%E7%89%9B%E9%A1%BF%E6%B3%95%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E5%BC%80%E6%A0%B9%E5%8F%B7%E5%92%8C%E8%A7%A3%E6%96%B9%E7%A8%8B/" target="_self">
                <time class="text-uppercase">
                    January 14 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><h2>二分法</h2>
<p>二分法的思路是每次排除一半样本的试错方法，把样本一分为二（A和B），那么目标值不在A就在B里，不断缩小范围。</p><p>就像在玩一个猜价格的游戏，通过告诉你猜高了还是低了，你总能猜到正确价格一样，设计好一个计算差值的函数能大体上判断出你下一轮尝试的值应该在前一半还是后一半，总能迭代到足够接近的结果。</p><p>对于求平方根来说，我们没什么过多的设计，直接对中值取平方，高了就取小的一半，低了就取大的一半，实测小的数字是没问题的，这里仅仅用来演示思路。</p><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>

<span class="k">def</span> <span class="nf">binary_sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-10</span>         <span class="c1"># quit flag</span>
    <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">n</span>
    <span class="n">mid</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">mid</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">-</span> <span class="n">n</span>
    <span class="k">while</span> <span class="nb">abs</span><span class="p">(</span><span class="n">diff</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">epsilon</span><span class="p">:</span>
        <span class="c1"># 值过大则尝试小的一半，否则就尝试大的一半，修改边界值即可</span>
        <span class="k">if</span> <span class="n">diff</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">end</span> <span class="o">=</span> <span class="n">mid</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">mid</span>
        <span class="n">mid</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">mid</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">-</span> <span class="n">n</span>
    <span class="k">return</span> <span class="n">mid</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">11</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;estimated:</span><span class="se">\t</span><span class="si">{</span><span class="n">binary_sqrt</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="si">}</span><span class="s1">, </span><span class="se">\t</span><span class="s1"> sqrt(</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">): </span><span class="se">\t</span><span class="s1"> </span><span class="si">{</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
<p>output:</p>
<pre><code>estimated:	0.9999999999708962, 	 sqrt(1): 	 1.0
estimated:	1.4142135623842478, 	 sqrt(2): 	 1.4142135623730951
estimated:	1.7320508075645193, 	 sqrt(3): 	 1.7320508075688772
estimated:	2.0000000000000000, 	 sqrt(4): 	 2.0
estimated:	2.2360679775010794, 	 sqrt(5): 	 2.23606797749979
estimated:	2.4494897427794060, 	 sqrt(6): 	 2.449489742783178
estimated:	2.6457513110653963, 	 sqrt(7): 	 2.6457513110645907
estimated:	2.8284271247393917, 	 sqrt(8): 	 2.8284271247461903
estimated:	2.9999999999890860, 	 sqrt(9): 	 3.0
estimated:	3.1622776601579970, 	 sqrt(10): 	 3.1622776601683795
</code></pre>
<h2>牛顿法</h2>
<p>我就算不画图也能把这事说清楚。</p><p>牛顿法用的是斜率的思想，对$f(x)=0$，选一个足够接近目标值($x$)的点($x_0$)，计算其切线与X轴的交点($x_1$），这个交点$x_1$往往比$x_0$更接近$x$，数次迭代后肯定越来越接近目标值$x$。</p><ol>
<li>问题转化成一个求函数上任一点($x_n$)的切线与X轴的交点($x_{n+1}$)的问题(我们假设<code>n+1</code>在<code>n</code>的左边，即向左来逼近$x_0$)</li>
<li>令$\Delta x = x_n - x_{n+1}, \Delta y = f(x_n) - f(x_{n+1})$，则$f'(x_n) = 该点斜率 = \frac{\Delta y}{\Delta x}$</li>
<li>展开:$f'(x_n) = \frac{f(x_n) - f(x_{n +1})}{x_n - x_{n+1}}$</li>
<li>$\because f(x_{n+1})=0\ \Rightarrow x_{n +1} = x_n - \frac{f(x_n)}{f'(x_n)}$</li>
<li>至此，我们用$x_n$推出了一个离$x_0$更近的点：$x_{n+1}$</li>
<li>如此往复则可以推出足够精度的解。</li>
</ol>
<p>而求任意正整数$a$的平方根，</p><ol>
<li>函数就变成了 $g(x) = a, \Rightarrow g(x) = x^2$，</li>
<li>那么我们有: $f(x) = g(x) - a = 0 = x^2 - a = 0$</li>
<li>$f'(x) = 2x$</li>
<li>$f(x),f'(x)$都有了，就能代入上面得到的公式：$x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}$了</li>
<li>得到$x_{n+1} = x_n - \frac{x_n^2-a}{2x_n}$</li>
</ol>
<p>现在可以写代码了，不断去迭代，求下一个$x_{n+1}$:</p><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">newton_sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">x_n</span> <span class="o">=</span> <span class="n">n</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-10</span>             <span class="c1"># quit flag</span>

    <span class="n">f_x</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">a</span> <span class="p">:</span> <span class="n">a</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">n</span>   <span class="c1"># f(x)=x^2 - a</span>
    <span class="n">df_x</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">a</span> <span class="p">:</span> <span class="mi">2</span><span class="o">*</span><span class="n">a</span>       <span class="c1"># derivative of f(x)</span>
    <span class="n">x_next</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">a</span> <span class="p">:</span> <span class="n">a</span> <span class="o">-</span> <span class="n">f_x</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">/</span> <span class="n">df_x</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

    <span class="k">while</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x_n</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">-</span> <span class="n">n</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">epsilon</span><span class="p">:</span>
        <span class="n">x_n</span> <span class="o">=</span> <span class="n">x_next</span><span class="p">(</span><span class="n">x_n</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_n</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;sqrt(</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">)</span><span class="se">\t</span><span class="si">{</span><span class="n">newton_sqrt</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
<p>output</p>
<pre><code>sqrt(1)	1.000000000000001
sqrt(2)	1.4142135623746899
sqrt(3)	1.7320508075688772
sqrt(4)	2.0
sqrt(5)	2.23606797749979
sqrt(6)	2.4494897427831788
sqrt(7)	2.6457513110646933
sqrt(8)	2.8284271247493797
sqrt(9)	3.0
</code></pre>
<h2>梯度下降法</h2>
<p>梯度下降法的数学原理是$f(x_1,x_2,\dots$)的gradient（$\nabla f$）就是其最陡爬升方向（<code>steepest ascent</code>）。</p><p>可以拿这个当成结论，也可以去感性认识，而要去证明的话，网上有数不清的教程，在花书(《Deep Learning深度学习》)和可汗学院里，都是用的<code>directional derivate</code>来解释（证明）的，即”<strong>自定义方向</strong>上的瞬时变化率“，也是我认为在如果有多变量微积分的基础下，比较容易让人接受且简单直白的解释：
<figure class="vertical-figure" style="flex: 34.96774193548387" ><img width="1084" height="1550" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/e7889d3178ce0131e3ac9dcc816d03c3.png" alt=""/></figure></p><ul>
<li>$\overset{\rightarrow}{v} \cdot \nabla f = |\overset{\rightarrow}{v}|\cdot|\nabla f|\cdot cos\theta$</li>
<li>$\overset{\rightarrow}{v}$ 就是指的任意方向，如果是x, y等方向，那就是普通的偏导了。</li>
<li>显然上式当$\theta = 0$时拥有最大值，即$\overset{\rightarrow}{v}$指向的是$\nabla f$的方向，那就是梯度的方向了</li>
<li>所以梯度方向就是<code>爬升最陡峭</code>的方向</li>
</ul>
<p>在一元方程里，”梯度“就是过某点的斜率（<code>slope</code>)，或者说函数的导数（<code>derivative</code>）。</p><p>我们要到局部最小值，显然就应该向相向的方向走。并且由于越接近目标值（谷底），斜率越小，所以即使我们选择一个固定的步长（<code>learning rate</code>），也是会有一个越来越小的步进值去逼近极值，而无需刻意去调整步长。</p><p>以上是思路，只是要注意它$\color{red}{并不是作用到要求的函数本身}$上去的，而是精心设计的<code>loss</code>，或者说<code>diff</code>、<code>error</code>函数。</p><p>其实它跟前面的<code>二分法</code>很类似，就是不断指导里应该在哪个区间里去尝试下一个x值，再来结果与真值的差异（而<code>牛顿法</code>则是直接朝着直值去逼近，并不是在“尝试“）。</p><p>二分法里我随便设计了一个判断loss的函数（即中值的平方减真值），而梯度下降里不能那么随便了，它需要是一个连续的函数（即可微分），还要至少拥有局部极小值：</p><p>我们令$e(x)$表示不同的x取值下与目标值$Y$的差的平方（损失函数<em>loss</em>），既然是一个简单二次函数，就能求极值，且它的最小值意味着当x值为该值时估算原函数$f(x)=Y$的<strong>误差最小</strong>，有：</p><p>$e(x) = \frac{1}{2}(f(x) - Y)^2$  (1/2的作用仅仅是为了取导数时消除常数项，简化计算)<br />
$e'(x) = (f(x) - Y) \cdot f'(x) = \Delta y \cdot f'(x)\quad \color{green}{\Leftarrow Chain\ Rule}$<br />
$\Delta x = e'(x) \cdot lr = \Delta y \cdot f'(x) \cdot lr\ \color{red}{\Leftarrow这里得到了更新x的依据}$<br />
$x_{n+1} = x_n - \Delta x = x_n - \Delta y \cdot f'(x) \cdot lr \Leftarrow 公式有了$</p><p>这时可以写代码了</p><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gradient_sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">x</span>       <span class="o">=</span> <span class="n">n</span> <span class="o">/</span> <span class="mi">2</span>       <span class="c1"># first try</span>
    <span class="n">lr</span>      <span class="o">=</span> <span class="mf">0.01</span>        <span class="c1"># learning rate</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-10</span>       <span class="c1"># quit flag</span>

    <span class="n">f_x</span>     <span class="o">=</span> <span class="k">lambda</span> <span class="n">a</span> <span class="p">:</span> <span class="n">a</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">df_dx</span>   <span class="o">=</span> <span class="k">lambda</span> <span class="n">a</span> <span class="p">:</span> <span class="mi">2</span><span class="o">*</span><span class="n">a</span>
    <span class="n">delta_y</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">a</span> <span class="p">:</span> <span class="n">f_x</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">-</span><span class="n">n</span>
    <span class="n">e_x</span>     <span class="o">=</span> <span class="k">lambda</span> <span class="n">a</span> <span class="p">:</span> <span class="n">delta_y</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="mf">0.5</span>     <span class="c1"># funcon of loss</span>
    <span class="n">de_dx</span>   <span class="o">=</span> <span class="k">lambda</span> <span class="n">a</span> <span class="p">:</span> <span class="n">delta_y</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">df_dx</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>   <span class="c1"># derivative of loss</span>
    <span class="n">delta_x</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">a</span> <span class="p">:</span> <span class="n">de_dx</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">lr</span>

    <span class="n">count</span>   <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">n</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">epsilon</span><span class="p">:</span>
        <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">delta_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">count</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;sqrt(</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">): </span><span class="si">{</span><span class="n">gradient_sqrt</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="si">}</span><span class="s1">次&#39;</span><span class="p">)</span>
</pre></div>
<p>output</p>
<pre><code>sqrt(1): (0.9999999999519603, 593)次
sqrt(2): (1.4142135623377403, 285)次
sqrt(3): (1.7320508075423036, 181)次
sqrt(4): (2.0, 0)次
sqrt(5): (2.236067977522142, 103)次
sqrt(6): (2.449489742798969, 87)次
sqrt(7): (2.645751311082885, 73)次
sqrt(8): (2.828427124761154, 63)次
sqrt(9): (3.00000000001166, 55)次
</code></pre>
<hr />
<p><strong>Bonus</strong></p><h2>梯度下降解二次方程</h2>
<ul>
<li>求解方程：$(x_1 - 3)^2 + (x_2 + 4)^2 = 0$的根</li>
</ul>
<p>$f(x) = (x_1 - 3)^2 + (x_2 + 4)^2 = 0$</p><p>$e(x) = \frac{1}{2}(f(x)-Y)^2$</p><p>$\frac{\partial}{\partial x_1}e(x)=(f(x)-Y)\cdot(f(x) -Y)'
= (f(x)-Y)\cdot\frac{\partial}{\partial x_1}((x_1 - 3)^2 + (x_2 + 4)^2-Y)$</p><p>$\therefore
\begin{cases}
\frac{\partial}{\partial x_1}e(x)=\Delta y \cdot 2(x_1 - 3) \
\frac{\partial}{\partial x_2}e(x)=\Delta y \cdot 2(x_2 + 4)
\end{cases}
$</p><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gradient_f</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span>  <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>        <span class="c1"># first try</span>
    <span class="n">lr</span>      <span class="o">=</span> <span class="mf">0.01</span>        <span class="c1"># learning rate</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-4</span>        <span class="c1"># quit flag</span>

    <span class="n">f_x</span>     <span class="o">=</span> <span class="k">lambda</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="p">:</span> <span class="p">(</span><span class="n">x1</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">x2</span><span class="o">+</span><span class="mi">4</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">dfx1</span>    <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">dfx2</span>    <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">delta_y</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="p">:</span> <span class="n">f_x</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span> <span class="o">-</span> <span class="n">n</span>
    <span class="n">e_x</span>     <span class="o">=</span> <span class="k">lambda</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="p">:</span> <span class="n">delta_y</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="mf">0.5</span>     <span class="c1"># cost function</span>
    <span class="n">dedx1</span>   <span class="o">=</span> <span class="k">lambda</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="p">:</span> <span class="n">delta_y</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span> <span class="o">*</span> <span class="n">dfx1</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>   <span class="c1"># partial derivative of loss \</span>
    <span class="n">dedx2</span>   <span class="o">=</span> <span class="k">lambda</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="p">:</span> <span class="n">delta_y</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span> <span class="o">*</span> <span class="n">dfx2</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>   <span class="c1"># with Chain Rule</span>
    <span class="n">delt_x1</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="p">:</span> <span class="n">dedx1</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span> <span class="o">*</span> <span class="n">lr</span>
    <span class="n">delt_x2</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="p">:</span> <span class="n">dedx2</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span> <span class="o">*</span> <span class="n">lr</span>

    <span class="n">count</span>   <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="nb">abs</span><span class="p">(</span><span class="n">f_x</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span> <span class="o">-</span> <span class="n">n</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">epsilon</span><span class="p">:</span>
        <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">x1</span> <span class="o">-=</span> <span class="n">delt_x1</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">-=</span> <span class="n">delt_x2</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">count</span>

<span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">gradient_f</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">a </span><span class="se">\t</span><span class="s1">= </span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s1"></span>
<span class="s1">b </span><span class="se">\t</span><span class="s1">= </span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s1"> </span>
<span class="s1">f(a, b) = </span><span class="si">{</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">b</span><span class="o">+</span><span class="mi">4</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="si">}</span><span class="s1"></span>
<span class="s1">count </span><span class="se">\t</span><span class="s1">= </span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>
<p>output</p>
<pre><code>a 	= 2.9967765158140387
b 	= -3.9905337923806563 
f(a, b) = 9.999993698966316e-05
count 	= 249990
</code></pre>
<p>之所以做两个练习， 就是因为第一个是故意把过程写得非常详细，如果直接套公式的话，而不是把每一步推导都写成代码，可以简单很多（其实就是最后一步的结果）:$\Delta x = \Delta y \cdot f'(x) \cdot lr$</p><h2>梯度下降解反三角函数</h2>
<ul>
<li>求解arcsin(x)，在$x = 0.5$和$x = \frac{\sqrt{3}}{2}$的值</li>
</ul>
<p>即估算两个x值，令$f(x)=sin(x)=0.5$和$f(x)=sin(x)=\frac{\sqrt{3}}{2}$<br />
这次不推导了，套一次公式吧$\Delta x = \Delta y \cdot f'(x) \cdot lr$</p><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>

<span class="k">def</span> <span class="nf">arcsin</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">x</span>       <span class="o">=</span> <span class="mi">1</span>           <span class="c1"># first try</span>
    <span class="n">lr</span>      <span class="o">=</span> <span class="mf">0.1</span>        <span class="c1"># learning rate</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-8</span>        <span class="c1"># quit flag</span>

    <span class="n">f_x</span>     <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">math</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">delta_y</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">f_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">n</span>
    <span class="n">delta_x</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">delta_y</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">lr</span>

    <span class="k">while</span> <span class="nb">abs</span><span class="p">(</span><span class="n">f_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">n</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">epsilon</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">-=</span> <span class="n">delta_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">degrees</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;&#39;&#39;sin(</span><span class="si">{</span><span class="n">arcsin</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span><span class="si">}</span><span class="s1">) ≈ 0.5</span>
<span class="s1">sin(</span><span class="si">{</span><span class="n">arcsin</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s1">) ≈ sqrt(3)/2</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>
<p>output</p>
<pre><code>sin(30.000000638736502) ≈ 0.5
sin(59.999998857570986) ≈ sqrt(3)/2

</code></pre>
</div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/%E4%BA%8C%E5%88%86%E6%B3%95%E3%80%81%E7%89%9B%E9%A1%BF%E6%B3%95%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E5%BC%80%E6%A0%B9%E5%8F%B7%E5%92%8C%E8%A7%A3%E6%96%B9%E7%A8%8B/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
</section>

<div class="container">
    <section id="prism__page__pagination" class="prism-pagination" class="col-md-8 offset-md-2">
        <ul>
            
            <li class="next">
                <a class="no-link" href="/page/3/" target="_self"><i class="fa fa-chevron-left" aria-hidden="true"></i>Newer</a>
            </li>
            
            
            <li class="prev">
                <a class="no-link" href="/page/5/" target="_self">Older<i class="fa fa-chevron-right" aria-hidden="true"></i></a>
            </li>
            
        </ul>
    </section>
</div>


</main>

            <footer id="prism__footer">
                <section>
                    <div>
                        <nav class="social-links">
                            <ul><li><a class="no-link" title="Twitter" href="https://twitter.com/walkerwzy" target="_blank" rel="noopener noreferrer nofollow"><i class="gi gi-twitter"></i></a></li><li><a class="no-link" title="GitHub" href="https://github.com/walkerwzy" target="_blank" rel="noopener noreferrer nofollow"><i class="gi gi-github"></i></a></li><li><a class="no-link" title="Weibo" href="https://weibo.com/1071696872" target="_blank" rel="noopener noreferrer nofollow"><i class="gi gi-weibo"></i></a></li></ul>
                        </nav>
                    </div>

                    <section id="prism__external_links">
                        <ul>
                            
                            <li>
                                <a class="no-link" target="_blank" href="https://github.com/AlanDecode/Maverick" rel="noopener noreferrer nofollow">Maverick</a>：🏄‍ Go My Own Way.
                                <span>|</span>
                            </li>
                            
                            <li>
                                <a class="no-link" target="_blank" href="https://www.imalan.cn" rel="noopener noreferrer nofollow">Triple NULL</a>：Home page for AlanDecode.
                                <span>|</span>
                            </li>
                            
                        </ul>
                    </section>

                    <div class="copyright">
                        <p class="copyright-text">
                            <span class="brand">walker's code blog</span>
                            <span>Copyright © 2022 AlanDecode</span>
                        </p>
                        <p class="copyright-text powered-by">
                            | Powered by <a href="https://github.com/AlanDecode/Maverick" class="no-link" target="_blank" rel="noopener noreferrer nofollow">Maverick</a> | Theme <a href="https://github.com/Reedo0910/Maverick-Theme-Prism" target="_blank" class="no-link" rel="noopener noreferrer nofollow">Prism</a>
                        </p>
                    </div>
                    <div class="footer-addon">
                        
                    </div>
                </section>
                <script>
                    var site_build_date = "2019-12-06T12:00+08:00"

                </script>
                <script src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/prism-efa8685153.js"></script>
            </footer>
        </div>
    </div>
    </div>

    <script src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/ExSearch/jquery.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/ExSearch/ExSearch-493cb9cd89.js"></script>

    <!--katex-->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/katex.min.js"></script>
    <script>
        mathOpts = {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "\\[", right: "\\]", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false }
            ]
        };

    </script>
    <script defer src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/auto-render.min.js" onload="renderMathInElement(document.body, mathOpts);"></script>

    
</body>

</html>