<!DOCTYPE HTML>
<html lang="english">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="renderer" content="webkit">
    <meta name="HandheldFriendly" content="true">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Maverick,AlanDecode,Galileo,blog" />
    <meta name="generator" content="Maverick 1.2.1" />
    <meta name="template" content="Prism" />
    <link rel="alternate" type="application/rss+xml" title="walker's code blog &raquo; RSS 2.0" href="/feed/index.xml" />
    <link rel="alternate" type="application/atom+xml" title="walker's code blog &raquo; ATOM 1.0" href="/feed/atom/index.xml" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/prism-b9d78ff38a.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/ExSearch/ExSearch-182e5a8869.css">
    <link href="https://fonts.googleapis.com/css?family=Fira+Code&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css">
    <script>
        var ExSearchConfig = {
            root: "",
            api: "https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/0792c859af00d57f17774077bdd3dbf1.json"
        }

    </script>
    
<title>walker's code blog</title>
<meta name="author" content="AlanDecode" />
<meta name="description" content="coder, reader" />
<meta property="og:title" content="walker's code blog" />
<meta property="og:description" content="coder, reader" />
<meta property="og:site_name" content="walker's code blog" />
<meta property="og:type" content="website" />
<meta property="og:url" content="/page/5/" />
<meta property="og:image" content="walker's code blog" />
<meta name="twitter:title" content="walker's code blog" />
<meta name="twitter:description" content="coder, reader" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/android-chrome-512x512.png" />


    
</head>

<body>
    <div class="container prism-container">
        <header class="prism-header" id="prism__header">
            <h1 class="text-uppercase brand"><a class="no-link" href="/" target="_self">walker's code blog</a></h1>
            <p>coder, reader</p>
            <nav class="prism-nav"><ul><li><a class="no-link text-uppercase " href="/" target="_self">Home</a></li><li><a class="no-link text-uppercase " href="/archives/" target="_self">Archives</a></li><li><a class="no-link text-uppercase " href="/about/" target="_self">About</a></li><li><a href="#" target="_self" class="search-form-input no-link text-uppercase">Search</a></li></ul></nav>
        </header>
        <div class="prism-wrapper" id="prism__wrapper">
            
<main>    
    

<section id="prism__post-list" class="prism-section row">
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/%E6%88%91%E7%9A%84%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/" target="_self">我的知识图谱入门笔记</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/%E6%88%91%E7%9A%84%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/" target="_self">
                <time class="text-uppercase">
                    June 17 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><h1>Knowledge Graph</h1>
<ul>
<li>信息是指外部的客观事实。举例：这里有一瓶水，它现在是7°。</li>
<li>知识是对外部客观规律的归纳和总结。举例：水在零度的时候会结冰。</li>
</ul>
<p>换句话说，知识图谱是由一条条知识组成，每条知识表示为一个SPO三元组(Subject-Predicate-Object)。</p><p>$\boxed{Subject} \xrightarrow{Predicate} \boxed{Object}$</p><h1>语义网络(Semantic Network)</h1>
<p>语义网络由相互连接的节点和边组成，节点表示概念或者对象，边表示他们之间的关系(is-a关系，比如：猫是一种哺乳动物；part-of关系，比如：脊椎是哺乳动物的一部分)</p><p>。在表现形式上，语义网络和知识图谱相似，但语义网络更侧重于描述概念与概念之间的关系，（有点像生物的层次分类体系——界门纲目科属种），而知识图谱则更偏重于描述实体之间的关联。</p><h1>RDF(Resoure Description Framework)</h1>
<p>RDF(Resource Description Framework)，即资源描述框架，是W3C制定的，用于描述实体/资源的标准数据模型。RDF图中一共有三种类型，International Resource Identifiers(IRIs)，blank nodes 和 literals。下面是SPO每个部分的类型约束：</p><ul>
<li>Subject可以是IRI或blank node。可以理解为<code>URI</code></li>
<li>Predicate是IRI。</li>
<li>Object三种类型都可以。</li>
</ul>
<p>也就是说字面量不能做主语？</p><p>将罗纳尔多的原名与中文名关联起来的RDF表示：</p><p>$\boxed{www.kg.com/person/1} \xrightarrow{kg:chineseName} \boxed{罗纳尔多·路易斯·纳扎里奥·达·利马}$</p><blockquote>
<p>可见，主语的指代性要强很多，所以字面量（宾语）用作主语会丧失这种精确性（唯一性）。</p></blockquote>
<ul>
<li>&quot;<code>www.kg.com/person/1</code>&quot;是一个IRI，用来唯一的表示“罗纳尔多”这个实体。&quot;kg:chineseName&quot;也是一个IRI，用来表示“中文名”这样一个属性。&quot;kg:&quot;是RDF文件中所定义的prefix，如下所示。</li>
<li>@<code>prefix kg</code>: <a href="http://www.kg.com/ontology/">http://www.kg.com/ontology/</a> 即，kg:chineseName其实就是&quot;http:// www.kg.com/ontology/chineseName&quot;的缩写。</li>
</ul>
<p>这样知识图谱的正确表示其实是：</p><figure  style="flex: 81.6618911174785" ><img width="1140" height="698" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/055b61a7e0a74762f15ece471ec22ee8.png" alt=""/></figure><p>而不是网传的简单的画几个对象连几根线，也就是说，能用URI表示的，尽量都用URI表示。</p><h1>Identifying graph-shaped problems(应用场景)</h1>
<ul>
<li><p>does our problem involve understanding <code>relationships</code> between entities?</p><ul>
<li>Recommendations</li>
<li>Next best action</li>
<li>Fraud detection</li>
<li>Identity resolution</li>
<li>Data lineage</li>
</ul>
</li>
<li><p>does our problem involve a lot of <code>self-referencing</code> to the same type of entity?</p><ul>
<li>Organisational hierachies</li>
<li>Social influencers</li>
<li>Friends of friends</li>
<li>Churn detection</li>
</ul>
</li>
<li><p>does the problem explore <code>relationships of varying or unknown depth</code>?</p><ul>
<li>Supply chain visibility</li>
<li>Bill of  Materials(BOM)</li>
<li>Network management</li>
</ul>
</li>
<li><p>does our problem involve discovering lots of <code>different routers or paths</code>?</p><ul>
<li>Logistics and routing</li>
<li>Infrastructure management</li>
<li>Dependency tracing</li>
</ul>
</li>
</ul>
<h1>Neo4j</h1>
<figure  style="flex: 88.44507845934379" ><img width="1240" height="701" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/1da110e1163eb31331ea2c21528a4ae2.png" alt=""/></figure><ol>
<li><code>:person</code>, <code>:Car</code>, <code>:Vehicle</code> are <code>Label</code></li>
<li>even <code>relationship</code> can also have(own) properties</li>
</ol>
<h2>AsciiArt</h2>
<h3>for Nodes</h3>
<p><code>(p:Person:Mammal{name:'walker'})</code></p><h3>for Relationships</h3>
<p><code>- [:HIRED {type: 'fulltime'}] -&gt;</code></p><h2>CRUD</h2>
<h3>Create</h3>
<figure  style="flex: 87.94326241134752" ><img width="1240" height="705" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/1664c809c2f78ab7dc14392c1ef00ac9.png" alt=""/></figure><h4>Constraints</h4>

<pre><code>CREATE  CONSTRAINT ON (p:Person)
ASSERT p.name IS UNIQUE
</code></pre>
<p>所以如下语句会报错：</p>
<pre><code>CREATE (a:Person {name: &quot;Ann&quot;})
CREATE (a) - [:HAS_PET] -&gt; (:Dog {name: &quot;Sam&quot;})
</code></pre>
<p>要用<code>merge</code></p>
<pre><code>MERGE (a:Person {name: &quot;Ann&quot;})
CREATE (a) - [:HAS_PET] -&gt; (:Dog {name: &quot;Sam&quot;})
</code></pre>
<h4>Set</h4>
<p>属性可以用<code>JSON</code>格式写，也可以用<code>SET</code>语法（下面的查询语句也是一样）</p>
<pre><code>MERGE (a:Person {name: &quot;Ann&quot;})
ON CREATE SET
    a.twitter = &quot;@ann&quot;
MERGE (a) - [:HAS_PET] -&gt; (:Dog {name: &quot;Sam&quot;})
</code></pre>
<p>同时，看到了吗？<code>create</code>只能出现一次（同一个对象的话）</p><h3>Read</h3>
<blockquote>
<p>who drives a car owned by a lover?</p></blockquote>
<div class="highlight"><pre><span></span><span class="k">MATCH</span><span class="w"></span>
<span class="p">(</span><span class="n">p1</span><span class="p">:</span><span class="n">Person</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">[:</span><span class="n">DRIVES</span><span class="p">]</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">(</span><span class="k">c</span><span class="p">:</span><span class="n">Car</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">[:</span><span class="n">OWNED_BY</span><span class="p">]</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">(</span><span class="n">p2</span><span class="p">:</span><span class="n">Person</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="p">[:</span><span class="n">LOVES</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">(</span><span class="n">P1</span><span class="p">)</span><span class="w"></span>
<span class="k">RETURN</span><span class="w"></span>
<span class="n">p1</span><span class="w"></span>
</pre></div>
<p>其中，因为问的是lover，没有指向性，所以如果是两个互相相爱，后半截也可以是p2指向p1</p>
<pre><code>match p = (n) -[*1..2] -&gt; (m) where n.name='特朗普' return p
match p =  ({name: '特朗普'}) - [*1..2] -&gt; () return p # 简化
match p = (n)-[m]-&gt;(q) where m.name = '丈夫' return n,q skip 10 limit 5
match p = (n)-[:丈夫]-&gt;(q) return n,q skip 10 limit 5 # 简化
</code></pre>
<p>解读：</p><ol>
<li>上面写法很简略，注意观察一下</li>
<li>又一次演示了直接用json来做where和单独用<code>where</code>关键字的写法区别（要多命名一个变量）</li>
<li>p跟n的区别，p是返了整个网络，如果<code>return n</code>，那么就是n自身(一个节点）。</li>
</ol>
<p>4, 但是如果<code>return n, m</code>，那么又把一层网络给select出来了 
5. [*1..2]表示跟踪两层
6. 如果不需要对n,m进行where,set操作，可以不设置变量
7. <code>relationship</code>也可以过滤，也有name等属性
8. <code>limit</code>, <code>skip</code> 等用法</p><h4>Tabular Results</h4>
<div class="highlight"><pre><span></span><span class="k">MATCH</span><span class="w"> </span><span class="p">(</span><span class="n">p</span><span class="p">:</span><span class="n">Person</span><span class="w"> </span><span class="err">{</span><span class="n">name</span><span class="p">:</span><span class="w"> </span><span class="ss">&quot;Tom Hanks&quot;</span><span class="err">}</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">[</span><span class="n">r</span><span class="p">:</span><span class="n">ACTED_IN</span><span class="o">|</span><span class="n">DIRECTED</span><span class="p">]</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">(</span><span class="n">m</span><span class="p">:</span><span class="n">Movie</span><span class="p">)</span><span class="w"></span>
<span class="k">RETURN</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="n">r</span><span class="p">,</span><span class="n">m</span><span class="w"></span>
<span class="k">RETURN</span><span class="w"> </span><span class="n">p</span><span class="p">.</span><span class="n">name</span><span class="p">,</span><span class="w"> </span><span class="k">type</span><span class="p">(</span><span class="n">r</span><span class="p">),</span><span class="w"> </span><span class="n">m</span><span class="p">.</span><span class="n">title</span><span class="w"></span>
</pre></div>
<p>前者返回Graph，后者返回表格数据</p><h3>Update</h3>
<blockquote>
<p>P.S. <code>where</code>是对属性做限制，所以查询条件既可以写在属性里，也可以用<code>where</code>语句来做过滤.</p></blockquote>
<p>要对查询结果进行修改，用<code>set</code>（有则改，无则加）</p>
<pre><code>MATCH
(:Person {name: &quot;张三&quot;}) - [:LOVES] -&gt; (p:Person)
RETURN
p

MATCH
(p1:Person) - [:LOVES] -&gt; (p2:Person)
WHERE
p1.name = &quot;张三&quot;
SET
p2.age = 33  # set by property
# or
p2 += {age: 33, height: 180}  # set by JSON
RETURN
p2
</code></pre>
<p>可以把<code>Neo4j</code>理解为命名实体识别(<code>NER</code>)，即你创造一句话，为句子里的每个实体打上标签，然后你想要谁就用实体标签把它取出来。</p><p>比如“张三爱李四”：</p>
<pre><code>step1: 写框架
CREATE () - [] -&gt; ()
step2: 填节点和关联
CREATE (:Person {name: &quot;张三&quot;}) - [:LOVES] -&gt; (:Person {name: &quot;李四&quot;})
</code></pre>
<p>而你要问张三爱谁:</p>
<pre><code>MATCH
(:Person {name: &quot;张三&quot;}) - [:LOVES] -&gt; (p:Person)
RETURN
p
</code></pre>
<p>看到查询语句了吗？除了<code>CREATE</code>, <code>MATCH</code>等关键词，句子顺序是完全一样的，也就是说，一直是在“<strong>陈述</strong>”一件事。</p><p>而事实上，这个<code>NER</code>在知识图谱中表示为<code>RDF</code>。</p><h3>Delete</h3>
<div class="highlight"><pre><span></span><span class="k">match</span><span class="w"> </span><span class="n">p</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="k">where</span><span class="w"> </span><span class="n">n</span><span class="p">.</span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;小明&#39;</span><span class="w"> </span><span class="n">detach</span><span class="w"> </span><span class="k">delete</span><span class="w"> </span><span class="n">n</span><span class="w"></span>
</pre></div>
<h2>Query</h2>
<h3>最短距离</h3>
<div class="highlight"><pre><span></span><span class="k">MATCH</span><span class="w"></span>
<span class="w">  </span><span class="p">(</span><span class="n">martin</span><span class="p">:</span><span class="n">Person</span><span class="w"> </span><span class="err">{</span><span class="n">name</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;Martin Sheen&#39;</span><span class="err">}</span><span class="p">),</span><span class="w"></span>
<span class="w">  </span><span class="p">(</span><span class="n">oliver</span><span class="p">:</span><span class="n">Person</span><span class="w"> </span><span class="err">{</span><span class="n">name</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;Oliver Stone&#39;</span><span class="err">}</span><span class="p">),</span><span class="w"></span>
<span class="w">  </span><span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">shortestPath</span><span class="p">((</span><span class="n">martin</span><span class="p">)</span><span class="o">-</span><span class="p">[</span><span class="o">*</span><span class="p">..</span><span class="mi">15</span><span class="p">]</span><span class="o">-</span><span class="p">(</span><span class="n">oliver</span><span class="p">))</span><span class="w"> </span><span class="o">#</span><span class="w"> </span><span class="mi">1</span><span class="w"></span>
<span class="w">  </span><span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">allShortestPath</span><span class="p">(....)</span><span class="w"> </span><span class="o">#</span><span class="w"> </span><span class="mi">2</span><span class="w"></span>
<span class="w">  </span><span class="k">WHERE</span><span class="w"> </span><span class="k">none</span><span class="p">(</span><span class="n">r</span><span class="w"> </span><span class="k">IN</span><span class="w"> </span><span class="n">relationships</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="w"> </span><span class="k">WHERE</span><span class="w"> </span><span class="k">type</span><span class="p">(</span><span class="n">r</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;FATHER&#39;</span><span class="p">)</span><span class="w"> </span><span class="o">#</span><span class="w"> </span><span class="mi">3</span><span class="w"></span>
<span class="k">RETURN</span><span class="w"> </span><span class="n">p</span><span class="w"></span>
</pre></div>
<ol>
<li>限定了15层</li>
<li>Finds <code>all</code> the shortest paths between two nodes.</li>
<li>排除了关系<code>type</code>为<strong>FATHER</strong>的</li>
</ol>
<p>给你们看一下一个<code>relationships</code>长啥样：</p><div class="highlight"><pre><span></span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="nt">&quot;identity&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">36629</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="nt">&quot;start&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">31343</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="nt">&quot;end&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">33922</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;author-&gt;title&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="nt">&quot;properties&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;author-&gt;title&quot;</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
<h3>模糊匹配(%)</h3>
<div class="highlight"><pre><span></span><span class="k">match</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">-</span><span class="p">[</span><span class="o">*</span><span class="mi">1</span><span class="p">..</span><span class="mi">2</span><span class="p">]</span><span class="o">-&gt;</span><span class="p">(</span><span class="n">b</span><span class="p">:</span><span class="n">content</span><span class="p">)</span><span class="w"> </span><span class="k">where</span><span class="w"> </span><span class="n">a</span><span class="p">.</span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;李白&#39;</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">b</span><span class="p">.</span><span class="n">name</span><span class="w"> </span><span class="o">=~</span><span class="w"> </span><span class="s1">&#39;.*明月.*&#39;</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="w"></span>
</pre></div>
<p><code>.*</code>就相当于sql里的<code>%</code>吧</p><h3>get by id</h3>

<pre><code>MATCH (n)
WHERE id(n) IN [0, 3, 5]
RETURN n
</code></pre>
<ol>
<li>id(n) -&gt; search with id</li>
<li>multiple id use <code>in</code></li>
</ol>
<h3>outer join (optional relationships)</h3>
<div class="highlight"><pre><span></span><span class="k">match</span><span class="w"> </span><span class="p">(</span><span class="n">a</span><span class="p">:</span><span class="n">author</span><span class="w"> </span><span class="err">{</span><span class="n">name</span><span class="p">:</span><span class="s1">&#39;李白&#39;</span><span class="err">}</span><span class="p">)</span><span class="w"></span>
<span class="n">optional</span><span class="w"> </span><span class="k">match</span><span class="w"> </span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="c1">--&gt;(b)</span>
<span class="k">return</span><span class="w"> </span><span class="n">b</span><span class="w"></span>
</pre></div>
<p>在实例中，b包含了两种实例：</p><ol>
<li>title</li>
<li>introduce</li>
</ol>
<p>等同于sql中user表outer join了两个表(title, introduce)</p></div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/%E6%88%91%E7%9A%84%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/HMM%E3%80%81NER%E3%80%81PoS%E3%80%81Viterbi%E7%AC%94%E8%AE%B0/" target="_self">HMM、NER、PoS、Viterbi笔记</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/HMM%E3%80%81NER%E3%80%81PoS%E3%80%81Viterbi%E7%AC%94%E8%AE%B0/" target="_self">
                <time class="text-uppercase">
                    June 14 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><p>开局一句话，隐马尔可夫，就是在“溯源”，即产生你这个现象的源头在哪。</p><ul>
<li>比如你掷出的这个显示为6的骰子，是来自于六面体的还是四面体的，或是来自于普通的还是灌铅了的</li>
<li>又比如你一句话里的某一个词，它是处于开始位置还是中间位置，或是它是一个人名还是一个地点或是一个介词</li>
</ul>
<p>任何一种表现形式，都有一个它的“原因”或“属性”。 现在正式开始，来自我能理解的网络资料，我的课程，以及一些思考</p><p>首先几个基础概念：</p><h1>命名实体识别(NER)</h1>
<p><strong>实体</strong>：人物(PER)，地点(LOC)，等
<strong>BIOES</strong>: 开始(Begin)， 中间(Inner)， 结尾(E)，单个(Single)，其它(Other)</p><p>比如人名：张北京，就可以被识别为$\Rightarrow$ B-PER, I-PER, E-PER</p><h1>Part-of-Speech Tagging（词性标注）</h1>
<p>词性标注是为输入文本中的每个词性标注词分配词性标记的过程。标记算法的输入是一系列(标记化的)单词和标记集，输出是一系列标记，每个标记一个。</p><p>标记是一项消除歧义的任务;单词是模糊的，有不止一个可能的词性(歧义)，我们的目标是为这种情况找到正确的标签。例如，book可以是动词(book that flight)，也可以是名词(hand me that book)。That可以是一个限定词(Does that flight serve dinner)，也可以是一个补语连词(I thought that your flight was earlier)。后置标记的目标是解决这些分辨率模糊，为上下文选择合适的标记</p><h1>Sequence model</h1>
<p>Sequence models are central to NLP: they are models where there is some sort of <code>dependence through time</code> between your inputs.</p><ul>
<li>The classical example of a sequence model is the <code>Hidden Markov Model</code> for <strong>part-of-speech tagging</strong>. (词性标注)</li>
<li>Another example is the <code>conditional random field</code>.</li>
</ul>
<p>HMM模型的典型应用是词性标注</p><figure  style="flex: 94.51219512195122" ><img width="1240" height="656" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/8246ff19ee962171c5d3b15abd234eec.png" alt=""/></figure><p>词性标注语料库是统计标注算法的关键训练(和测试)集。三个主要的标注语料库始终用于训练和测试英语词性标注器。</p><ol>
<li>布朗语料库是1961年在美国出版的500篇不同体裁的书面文本的100万单词样本。</li>
<li>《华尔街日报》语料库收录了1989年发表在《华尔街日报》上的100万个单词。</li>
<li>总机语料库由1990-1991年收集的200万字电话对话组成。语料库的创建是通过在文本上运行一个自动的词性标记，然后由人工注释器手工更正每个标记。</li>
</ol>
<h1>HMM</h1>
<p>HMM是一个序列模型(<code>sequence model</code>)。序列模型或序列分类器是一个模型，其工作是为序列中的每个单元分配一个标签或类，从而将一个观察序列(观察状态)映射到一个标签序列(隐藏状态)。HMM是一种概率序列模型：给定一个单位序列(单词、字母、语素、句子等等)，它计算可能的标签序列的概率分布，并选择最佳标签序列。</p><ul>
<li>3个骰子，6面体，4面体，8面体(D6, D4, D8)</li>
<li>每次随机选出一个骰子投掷，得到一个数字</li>
<li>共十次，得到10个数字</li>
</ul>
<ol>
<li><code>可见状态链</code>：10次投掷得到10个数字(1,3,5...)$\Rightarrow$对应你看得的10个单词</li>
<li><code>隐含状态链</code>：每一次投掷都有可能拿到三种骰子之一，(D6, D6, D4...) $\Rightarrow$对应为每个单词的词性</li>
<li>转换概率（<code>transition probability</code>）：隐含状态之间的概率($\Rightarrow$对应为语法)：<ul>
<li>每一次拿到某种骰子之后，下一次拿到三种骰子的概率（[1/3,1/3,1/3],...)</li>
<li>或者说主动决策下一次用哪个骰子的概率[a,b,c...] (相加为1)</li>
</ul>
</li>
<li>可见状态之间没有转换概率</li>
<li>输出概率（<code>emission probability</code>）：隐含状态和可见状态之间的概率，比如D4下1的概率为1/4，D6下为1/6 (表现概率，激发概率，多种翻译)</li>
</ol>
<figure  style="flex: 106.16438356164383" ><img width="1240" height="584" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/1c9b14ebecf2d171b7f511296c425412.png" alt=""/></figure><p>应用HMM模型时候，往往是缺失了一部分信息的，</p><ul>
<li>有时候你知道骰子有几种，每种骰子是什么，但是不知道掷出来的骰子序列；</li>
<li>有时候你只是看到了很多次掷骰子的结果，剩下的什么都不知道。</li>
</ul>
<p>如何应用算法去估计这些缺失的信息，就成了一个很重要的问题，这也是HMM模型能做的几件事：</p><h2>Decoding</h2>
<p>解码的过程就是在给出一串序列和已知HMM模型的情况下，找到最可能的隐性状态序列。</p><p>比如结果是：1 6 3 5 2 7 3 5 2 4, 求最可能的骰子序列</p><h3>Viterbi algorithm</h3>
<ol>
<li>掷出1的最大概率是4面体： P1(D4) = P(1|D4) * P(D4) = 1/4 * 1/3</li>
<li>掷出6的最大概率是 P2(D6) = P(6|D6) * P(D6) = 1/6 * 1/3</li>
<li>连续1，6的概率就成了1的概率 * 2的概率 P2(D6) = P1(D4) * P2(D6) = 1/216</li>
<li>1,6,3 =&gt; P3(D4) = P2(D6) * P(3|D4) * P(D4) = $\frac{1}{216} \cdot \frac{1}{3} \cdot \frac{1}{4}$</li>
<li>and so on</li>
<li>但这个例子忽略了转移概率，即P(D6|D4), P(D4|D6,D4)，或者说默认了转移概率就是1/3，即每次挑中三个骰子的机率均等。</li>
</ol>
<h2>Evaluation</h2>
<p>根据条件和序列结果求这一序列的概率是多少，比如三种骰子，投出了1，6，3的结果：</p><figure  style="flex: 169.86301369863014" ><img width="1240" height="365" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/3008ce6fdd893286e56d1b7f9ad1a342.png" alt=""/></figure><ul>
<li>第1列表示第一次投掷得到1的可能性和为0.18</li>
<li>第2列为1 6的的可能性和为0.05</li>
<li>第3列为1 6 3的可能性和为0.03</li>
</ul>
<p>如果远低于或远高于这个概率，必然有做过手脚的骰子。</p><h2>转移概率的矩阵表示</h2>
<p>这次假定不同的骰子是用来作弊的，作弊者会根据情况来挑选骰子，这样转移概率就不可能是均等的了：</p><figure  style="flex: 83.33333333333333" ><img width="500" height="300" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/8259518e5781e3ce798778e8da69de85.png" alt=""/></figure><p>很幸运，这么复杂的概率转移图，竟然能用矩阵表达：</p><p>$$A = 
\begin{bmatrix}
0.15 &amp; 0.45 &amp; 0.4 \\
0.25 &amp; 0.35 &amp; 0.4 \\
0.10 &amp; 0.55 &amp; 0.35
\end{bmatrix}
$$</p>
<p>既然是3行3列，显然$A_{ij}$就是从i切换到j的概率，比如$A_{12}$ 就应该是这个人把骰子从作弊骰子1切换到2的概率。</p><figure  style="flex: 102.04081632653062" ><img width="500" height="245" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/653c3cb7f8c2f3a541d170421fe489bf.png" alt=""/></figure><p>相应地，发射概率（即不同骰子摇出的点数的概率）也能表示为矩阵：</p><p>$$B = 
\begin{bmatrix}
0.16 &amp; 0.16 &amp; 0.16 &amp; 0.16 &amp; 0.16 &amp; 0.16 \\
0.02 &amp; 0.02 &amp; 0.02 &amp; 0.02 &amp; 0.02 &amp; 0.90 \\
0.40 &amp; 0.20 &amp; 0.25 &amp; 0.05 &amp; 0.05 &amp; 0.05 \\
\end{bmatrix}
$$</p>
<p>现在有了转移概率和发射概率，我们再来看看前面的掷出1，6，3的骰子的概率：
骰子设为D1 D2 D3, 每一轮的可能性为P1 P2 P3, 则P = P3D1 + P3D2 + P3D3 即第3轮时3种骰子能投出3的概率和</p><p>我来推导一下P3D1怎么来的，上面的表格是我从别人的博客里复制的，这里就不做一个一模一样的图了，我们一步步来吧：</p><ul>
<li>第一次投掷每个骰子的概率应该是隐含了各为1/3吧？(这个好像叫&quot;<code>初始隐状态</code>&quot; $\pi$)</li>
<li>P1D1 = 0.16 * 0.33, 即1/3概率拿到D1，0.16概率投出1，同理：<ul>
<li>P1D2 = 0.02 * 0.33</li>
<li>P1D3 = 0.40 * 0.33</li>
</ul>
</li>
<li>P2D1 =<ul>
<li>P1D1 * $A_{00}$ * $B_{05}$ = P1D1 * 0.15 * 0.16 即P1D1前提下，乘上D1换到D1的概率，再乘上D1选出6的概率</li>
<li>$+$</li>
<li>P1D2 * $A_{10}$ * $B_{05}$ = P1D1 * 0.25 * 0.16 即P1D2前提下，乘上D2换到D1的概率，再乘上D1选出6的概率</li>
<li>$+$</li>
<li>P1D3 * $A_{20}$ * $B_{05}$ = P1D1 * 0.10 * 0.16 即P1D3前提下，乘上D3换到D1的概率，再乘上D1选出6的概率</li>
<li>以此类推得到P2D2, P2D3</li>
</ul>
</li>
<li>P3D2 = （<em>D1的概率太平均，这次换个D2来演示</em>）<ul>
<li>P2D1 * $A_{01}$ * $B_{12}$ = P2D1 * 0.45 * 0.02 即P2D1前提下，乘上D1换到D2的概率，再乘上D2选出3的概率</li>
<li>$+$</li>
<li>P2D2 * $A_{11}$ * $B_{12}$ = P2D1 * 0.35 * 0.02 即P2D2前提下，乘上D2换到D2的概率，再乘上D2选出3的概率</li>
<li>$+$</li>
<li>P2D3 * $A_{21}$ * $B_{12}$ = P2D1 * 0.35 * 0.02 即P2D3前提下，乘上D3换到D2的概率，再乘上D2选出3的概率</li>
<li>以此类推得到P3D1, P3D2</li>
</ul>
</li>
<li>P = P3D1 + P3D2 + P3D3</li>
</ul>
<p>$$
\sum_{r\in R}\prod_t^TP(v(t)|w_r(t)) | w_r(t-1))
$$</p>
<ul>
<li>v: visible 可见序列</li>
<li>w: 隐性状态序列</li>
<li>R: 所有隐状态的可能性</li>
</ul>
<ol>
<li>t-1隐状态前提下得到t的概率（转移概率）如D2换到D3的概率</li>
<li>上一概率前提下得到v(t)的概率，如D3扔出1的概率</li>
<li>一种隐状态下出序列的结果为累乘</li>
<li>所有隐状态下出该序列的结果为3的累加</li>
</ol>
<p>简单来说：</p><ol>
<li>可见序列$v(t)$的概率依赖当前$t$下的隐状态（比如是不是作弊了的骰子）$w_r(t)$<ul>
<li>得到：$P(v(t)\ \color{red}|\ w_r(t))$</li>
</ul>
</li>
<li>当前隐状态$w_r(t)$又有两个特征:<ol>
<li>由$w_r(t-1)$转换而来的: $P(v(t)|w_r(t))\color{red}{|}w_r(t-1)$</li>
<li>$T$是链式的，概率累乘： $\color{red}{\prod_t^T}P(v(t)|w_r(t)) | w_r(t-1))$</li>
</ol>
</li>
<li>最后一步时的隐状态显然是几种之一，累加起来就是所有可能性：<ul>
<li>$\color{red}{\sum_{r\in R}}\prod_t^TP(v(t)|w_r(t)) | w_r(t-1))$</li>
</ul>
</li>
</ol>
<h1>应用</h1>
<ol>
<li>初始概率</li>
</ol>
<p>以<code>BMES</code>为例（参考NER），把其认为是隐状态，然后认为每个词（里的字）是由隐状态产生的。</p><p>即<code>B</code>对应的字可能有“<code>中</code>”，“<code>国</code>”，等等，能作为词语打头的字都可能由隐状态<code>B</code>产生，其它状态依次类推。</p><p>就像我们三种骰子的初始概率，完全取决于每种骰子占总数的多少一样，HHM应用到语言模型里，初始概率就是先把文字全部用<code>BMES</code>表示，然后分别数出个数，与总数做个对比。（此时已经可以判断出<code>M</code>和<code>E</code>的概率只能是0了。</p><ol start="2">
<li>转移概率</li>
</ol>
<p>应该是4个循环吧，每次把当前状态后面跟上四个状态的情况都数出来，就是一个隐状态到其它四个状态的转移概率，四行拼到一起就是一个转移概率的矩阵，类似上面的三种骰子互相切换的矩阵。</p><p>也可以用字典，比如 BE BS BB BM等共16个键，两两遍历整个字符串完后，16个count就出来了，group后就能得到概率了。</p><ol start="3">
<li>观测概率（发射概率）</li>
</ol>
<p>这个就是每一个隐状态下对应不同表面文字的概率了，比如：{s:{&quot;周&quot;: 0.3357, &quot;爬&quot;:0.00003}...}</p><p>要知道，三种概率里面是有很多0的，意思就是在现有的语法体系里面不可能出现的场景，比如第一个字不可能是M和E，B后面不可能跟S，B，而M后面不可能跟B，S，以及S后面不可能跟M，E等，再比如假如哪个字永远不可能是第一个字，那么它的观测概率在S里面就永远是0，等等。</p><p>这里要计算的话，因为隐状态是用文字推断出来的，所以这个映射关系还在，那么整理一下两个数组就能把每个隐状态能对应的文字全部映射上了。</p><hr />
<p>以下是我课程里的笔记，理解了上面的内容，理解下面是没有任何障碍的。</p><h1>viterbi in NLP</h1>
<p>$\overbrace{
  0
  \xrightarrow[农]{2.5}
  1
  \xrightarrow[产]{4.0}
  2
}^{1.4}
\xrightarrow[物]{2.3}
3$</p><p>$0
\xrightarrow[农]{2.5}
\underbrace{
  1
  \xrightarrow[产]{4.0}
  2
  \xrightarrow[物]{2.3}
  3
}_{2.1}$</p><blockquote>
<p>数字画圈的写法 $\enclose{circle}{3}$ 这个生成器暂不支持</p></blockquote>
<ul>
<li>node: $\enclose{circle}{2}$ ，圆圈，就是位置索引</li>
<li>edge: 词， 箭头，很好理解：string[0,1] = '农'</li>
<li>Each edge weight is a <code>negative log probality</code><ul>
<li>-log(P(农)) = 2.5</li>
<li>-log(P(产)) = 4.0</li>
<li>-log(P(农产)) = 1.4</li>
<li>-log(P(产物)) = 2.1</li>
</ul>
</li>
<li>Each path is a segmentation for the sentence</li>
<li>Each path weight is a sentence <code>unigram</code> negative log probability<ul>
<li>-log(P(农产)) + -log(P(物)) = 1.4 + 2.3 = 3.7</li>
<li>农 + 产 + 物 = 2.5 + 4.0 + 2.3 = 8.8</li>
<li>农 + 产物 = 2.5 + 2.1 = 4.6</li>
</ul>
</li>
</ul>
<h2>two step</h2>
<p>1.前向，从左往右，找到<strong>最佳路径</strong>的分数
2.后向，从右往左，创建一条最佳路径</p><h3>forward algorithm</h3>
<p>pseudo code</p><div class="highlight"><pre><span></span><span class="n">best_score</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>
<span class="k">for</span> <span class="n">each</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">graph</span> <span class="p">(</span><span class="n">ascending</span> <span class="n">order</span><span class="p">)</span>
  <span class="n">best_score</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="err">∞</span>
  <span class="k">for</span> <span class="n">each</span> <span class="n">incoming</span> <span class="n">edge</span> <span class="n">of</span> <span class="n">node</span>
    <span class="n">score</span><span class="o">=</span><span class="n">best_score</span><span class="p">[</span><span class="n">edgeprev_node</span><span class="p">]</span><span class="o">+</span><span class="n">edge</span><span class="o">.</span><span class="n">score</span>
    <span class="k">if</span> <span class="n">score</span> <span class="o">&lt;</span> <span class="n">best_score</span><span class="p">[</span><span class="n">node</span><span class="p">]</span>
      <span class="n">best_score</span><span class="p">[</span><span class="n">node</span><span class="p">]</span><span class="o">=</span><span class="n">score</span>
      <span class="n">best_edge</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span><span class="n">edge</span>
</pre></div>
<p>example:
<figure  style="flex: 82.65765765765765" ><img width="734" height="444" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/5aa5426eb70b4c6cd0b8c4b1dacda749.png" alt=""/></figure></p><ul>
<li>初始节点打分0，其它节点打分为$\infty$</li>
<li>每个节点打分由其(<code>incoming edge</code>)(即来源箭头)和来源节点的打分构成</li>
<li>如果有多个来源，则计算出该来源的得分，与该节点当前的得分做对比，取得分低的那个</li>
<li>把该节点的分值和来源edge存到该节点上（edge就是词）。</li>
</ul>
<ol>
<li>简单来说，还是和之前的骰子一样，每一次算出到当前节点的最低分数的路径。</li>
<li>上图中，我们就把e1, e2, e5选出来了，这个过程中，删除了e3, e4这几条路径</li>
<li>best_score=(0.0, 2.5, 1.4, 3.7), best_edge = (NULL, e1, e2, e5)</li>
<li>用字典来把Node映射上去：{0:(0.0, NULL), 1:(2.5, e1), 2:(1.4, e2), 3:(3.7, e5)}</li>
</ol>
<h3>backward algorithm</h3>
<div class="highlight"><pre><span></span><span class="n">best_path</span><span class="o">=</span><span class="p">[]</span>
<span class="n">next_edge</span><span class="o">=</span><span class="n">best_edge</span><span class="p">[</span><span class="n">best_edge</span><span class="o">.</span><span class="n">length</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="k">while</span> <span class="n">next_edge</span> <span class="o">!=</span> <span class="n">NULL</span>
  <span class="n">add</span> <span class="n">next_edge</span> <span class="n">to</span> <span class="n">best_path</span>
  <span class="n">next_edge</span> <span class="o">=</span><span class="n">best_edge</span><span class="p">[</span><span class="n">next_edge</span><span class="o">.</span><span class="n">prev_node</span><span class="p">]</span>
<span class="n">reverse</span> <span class="n">best</span> <span class="n">path</span>
</pre></div>
<p>举例：
<figure  style="flex: 102.56410256410257" ><img width="800" height="390" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/c97b27230bd42318efb72375c245c50b.png" alt=""/></figure></p><ul>
<li>从图片可知，<code>path</code>就是<code>edge</code></li>
<li>初始path是空，[]</li>
<li>从<code>forward</code>的结果字典里找到node 3的best_edge，就是e5 [e5]</li>
<li>e5的来源的是node 2</li>
<li>从字典里找到2的best_edge，是e2 [e5, e2]</li>
<li>e2的来源是node 0</li>
<li>0的best_edge是NULL，结束递归</li>
<li>reverse: [e2, e5]</li>
</ul>
<figure  style="flex: 53.25581395348837" ><img width="458" height="430" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/c7650195782b845d9be6e36dac55f277.png" alt=""/></figure><p>这个很好理解</p><ol>
<li>0到农，到农产，到农产物的概率，表示为0.0+ -log(p(农/农产/农产物))</li>
<li>在农的前提下，就有农到产，和农到产物：best(1) + -log(P(产/产物))</li>
<li>在产的前提下，就只有best(2) + -log(P(物))了</li>
</ol>
<p>应用到NLP：</p><figure  style="flex: 73.99193548387096" ><img width="734" height="496" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/5e643e29e4fe62ef780ba545dc3a04fb.png" alt=""/></figure><p>这里就是把node, egde具体了一下：</p><ol>
<li>多包了一层for-each，意思是前面的代码是处理一行的</li>
<li>node对应是单词结尾(word_end)，其实就是一个index，前面说过了</li>
<li>edge对应是单词(word)，前面也说过了，即<code>string[5,7]</code>的意思</li>
<li>score由uni-gram来计算</li>
<li>计算上，就是找到以基准字当作单词结尾，然后前面的字跟它拼起来的所有可能性，找最低分：<ul>
<li>比如abcdefg, 如果当前是e，那么分别比较：abced, bcde, cde, de</li>
</ul>
</li>
<li>接上例，输出结果应该这么解读：<ul>
<li>以b为结尾的单词，最有可能的是xxx, 它的得分是，它的索引是，</li>
<li>以c为结尾的单词，最有可能是bc或是abc，它的得分是，bc/abc的索引是(1,2)，这样</li>
</ul>
</li>
</ol>
<figure  style="flex: 90.2439024390244" ><img width="592" height="328" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/9de76a49c53e036f86c0c2e7a950c8cd.png" alt=""/></figure><ol>
<li>显然这里已经知道edge不知道是一个词，而且是一个词的首尾边界</li>
<li>也知道存到best_edges里面的其实就是词的位置索引</li>
<li>反向的时候，从最后一个索引找到得分最低的词，再从这个单词向前找，一直找到<ul>
<li>所以next_edge[0]其实就是当前单词词首，[1]就是词尾</li>
<li>所以把当前单词存进去后，向前搜索就要以next_edge[0]为字典，找对应的best_edge</li>
<li>再从best_edge里面解析出最合适的单词的首尾索引，存到结果数组里</li>
</ul>
</li>
</ol>
</div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/HMM%E3%80%81NER%E3%80%81PoS%E3%80%81Viterbi%E7%AC%94%E8%AE%B0/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/Mac%E8%BF%9C%E7%A8%8BWindows-10%E9%87%8C%E7%94%A8Anaconda%E8%A3%85%E7%9A%84Jupyter-lab/" target="_self">Mac远程Windows-10里用Anaconda装的Jupyter-lab</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/Mac%E8%BF%9C%E7%A8%8BWindows-10%E9%87%8C%E7%94%A8Anaconda%E8%A3%85%E7%9A%84Jupyter-lab/" target="_self">
                <time class="text-uppercase">
                    June 13 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><p>家里台式机配置比笔记本好多了，但又习惯了苹果本，怎么在小本本上直接跑windows上的jupyter呢？</p><p>首先，给Windows 10 装上<a href="https://docs.microsoft.com/en-us/windows-server/administration/openssh/openssh_install_firstuse">OpenSSH</a></p><p>如果你不是用的Anaconda等虚拟环境而是把python和jupyter lab装在了本机以及写在了path里，理论上你用ssh连上windows后在shell里直接<code>jupyter lab</code>就好了，可是我是用了Anaconda的，ssh进去以及windows自身的命令行环境里都是执行不了conda和jupyter的</p><blockquote>
<p>可能仅仅只是path的原因，但应该没这么简单，考虑到端口转发已经能实现我的目的了，就不深究了。</p></blockquote>
<p>这时使用<code>ssh</code>的本地端口转发功能可以达到目的：</p><div class="highlight"><pre><span></span>$ ssh -L <span class="m">2121</span>:host2:21 host3
</pre></div>
<p>即把<code>host3</code>的端口<code>21</code>转发到<code>host2</code>的2121上去，当然，大多数情况下<code>host2</code>就是本机，那么<code>localhost</code>就好了：</p><div class="highlight"><pre><span></span>$ ssh -L <span class="m">8000</span>:localhost:8889 windows-server
</pre></div>
<p>当然，<code>8889</code>是你在windows上运行<code>--no-browser</code>的jupyter lab设定的端口：</p><div class="highlight"><pre><span></span>jupyter lab --no-browser --post<span class="o">=</span><span class="m">8889</span>
</pre></div>
</div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/Mac%E8%BF%9C%E7%A8%8BWindows-10%E9%87%8C%E7%94%A8Anaconda%E8%A3%85%E7%9A%84Jupyter-lab/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/Semi-supervised-Learning/" target="_self">Semi supervised Learning</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/Semi-supervised-Learning/" target="_self">
                <time class="text-uppercase">
                    June 07 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><p>李宏毅机器学习2021spring的家庭作业里面有一个<code>Semi-supervised Learning</code>的任务。</p><p>具体来说，就是一个图片分类的任务（11个食品类别），但只给了你几百个有标注的图片，同时，还给了你几千张没有标的图片（用来训练，而不是测试）。</p><p>思路也很简单，既然样本量过小，我们就得自己扩充样本量，但这次不是用数据增广(<code>Augumentation</code>)，而是自己造样本：</p><ol>
<li>用小样本训练一个模型，用这个模型来predict没有标注的图片（文本有补述）</li>
<li>对预测输出的11个类别softmax后，观察最大值，如果大于你设定的某个threshold，比如0.68，就把该图片和最大值所映射的类别当成一组真值添加到训练集里去</li>
<li>我用的是<code>torch.utils.data</code>里的<code>TensorDataset</code>来构建手动创建的增强数据集，然后用了<code>ConcatDataset</code>与原训练集拼接：</li>
</ol>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">TensorDataset</span>

<span class="k">def</span> <span class="nf">get_pseudo_labels</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.65</span><span class="p">):</span>
    <span class="c1"># This functions generates pseudo-labels of a dataset using given model.</span>
    <span class="c1"># It returns an instance of DatasetFolder containing images whose prediction confidences exceed a given threshold.</span>
    <span class="c1"># You are NOT allowed to use any models trained on external data for pseudo-labeling.</span>
    <span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>

    <span class="c1"># Construct a data loader.</span>
    <span class="n">data_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Make sure the model is in eval mode.</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="c1"># Define softmax function.</span>
    <span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Iterate over the dataset by batches.</span>
    <span class="n">images</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([])</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([])</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">data_loader</span><span class="p">):</span>
        <span class="n">img</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">batch</span>

        <span class="c1"># Forward the data</span>
        <span class="c1"># Using torch.no_grad() accelerates the forward process.</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>

        <span class="c1"># Obtain the probability distributions by applying softmax on logits.</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>

        <span class="c1"># ---------- TODO ----------</span>
        <span class="c1"># 在这里根据阈值判断是否保留</span>
        <span class="c1"># Filter the data and construct a new dataset.</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">prob</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">probs</span><span class="p">):</span>
            <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">prob</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">images</span><span class="p">,</span> <span class="n">img</span><span class="p">[</span><span class="n">idx</span><span class="p">]))</span>   <span class="c1"># 用索引选出对应的图片</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">targets</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">c</span><span class="p">)))</span> <span class="c1"># 用最大值索引当class</span>

    <span class="n">dataset</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>  <span class="c1"># 拼成tensor dataset</span>

    <span class="c1"># # Turn off the eval mode.</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">dataset</span>
</pre></div>
<p>使用：</p><div class="highlight"><pre><span></span><span class="n">pseudo_set</span> <span class="o">=</span> <span class="n">get_pseudo_labels</span><span class="p">(</span><span class="n">unlabeled_set</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>

<span class="c1"># Construct a new dataset and a data loader for training.</span>
<span class="c1"># This is used in semi-supervised learning only.</span>
<span class="n">concat_dataset</span> <span class="o">=</span> <span class="n">ConcatDataset</span><span class="p">([</span><span class="n">train_set</span><span class="p">,</span> <span class="n">pseudo_set</span><span class="p">])</span> <span class="c1"># 拼接两个dataset(只要有感兴趣的两组数组即可)</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">concat_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
<p>看来，所谓的半监督仍然是有监督，对于没有标注的数据，仍然要想办法用已有数据去为它打标，接下来就是普通的监督学习了。</p><hr />
<p>最后，在实际的demo代码中，能看到并不是我最初理解的“先用小样本训练好一个模型”，再用它来过滤un-labeled样本，增广到训练集去，即对训练集的增广是一劳永逸的（像别的增广方案一样）</p><p>而是每一个epoch里面都<strong>重新</strong>去增广一次，这个思路更类似于GAN（生成对抗网络），<code>generator</code>和<code>discriminator</code>是一起训练的。</p><p>也所以，第一次去增广的时候，其实就是一个初始化的model，也就是说，一个比较垃圾的数据集（当然，初始化的model未必能预测出置信度高的结果，以至于并不会有太多pseudo labels进入训练集）</p><p>因此，相比较纯监督学习，假如训练集是2000条，那么整个epoch轮次里，都是2000条数据在训练；而半监督学习里，可能是200, 220, 350, 580, 1000, 1500...这样累增的样本量（随着模型越来越好，置信度应该是越来越高的），如果epoch数量不够，可能并没有在相同2000左右的样本量下得到足够的训练</p></div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/Semi-supervised-Learning/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/RNN%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E6%8E%A8%E5%AF%BC/" target="_self">RNN梯度消失与梯度爆炸推导</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/RNN%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E6%8E%A8%E5%AF%BC/" target="_self">
                <time class="text-uppercase">
                    May 09 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><figure  style="flex: 82.77777777777777" ><img width="596" height="360" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/794a90e7b252e2aa03842eeee7fad8de.png" alt=""/></figure>
$$
\large
\begin{aligned}
h_t &amp;=\sigma(z_t) = \sigma(Ux_t+Wh_{t-1} + b) \
y_t &amp;= \sigma(Vh_t + c)
\end{aligned}
$$<h2>梯度消失与爆炸</h2>
<p>假设一个只有 3 个输入数据的序列，此时我们的隐藏层 h1、h2、h3 和输出 y1、y2、y3 的计算公式：</p><p>$$
\large
\begin{aligned}
h_1 &amp;= \sigma(Ux_1 + Wh_0 + b) \\
h_2 &amp;= \sigma(Ux_2 + Wh_1 + b) \\
h_3 &amp;= \sigma(Ux_3 + Wh_2 + b) \\
y_1 &amp;= \sigma(Vh_1 + c) \\
y_2 &amp;= \sigma(Vh_2 + c) \\
y_3 &amp;= \sigma(Vh_3 + c)
\end{aligned}
$$</p>
<p>RNN 在时刻 t 的损失函数为 Lt，总的损失函数为 $L = L1 + L2 + L3 \Longrightarrow  \sum_{t=1}^TL_T$</p><p>t = 3 时刻的损失函数 L3 对于网络参数 U、W、V 的梯度如下：</p><p>$$
\begin{aligned}
\frac{\partial L_3}{\partial V} &amp;= \frac{\partial L_3}{\partial y_3} \frac{\partial y_3}{\partial V} \\
\frac{\partial L_3}{\partial U} &amp;= \frac{\partial L_3}{\partial y_3} \frac{\partial y_3}{\partial h_3} \frac{\partial h_3}{\partial U} + \frac{\partial L_3}{\partial y_3} \frac{\partial y_3}{\partial h_3} \frac{\partial h_3}{\partial h_2} \frac{\partial h_2}{\partial U} + \frac{\partial L_3}{\partial y_3} \frac{\partial y_3}{\partial h_3} \frac{\partial h_3}{\partial h_2} \frac{\partial h_2}{\partial h_1} \frac{\partial h_1}{\partial U} \\
\frac{\partial L_3}{\partial W} &amp;= \frac{\partial L_3}{\partial y_3} \frac{\partial y_3}{\partial h_3} \frac{\partial h_3}{\partial W} 
+ \frac{\partial L_3}{\partial y_3} \frac{\partial y_3}{\partial h_3} \frac{\partial h_3}{\partial h_2} \frac{\partial h_2}{\partial W} 
+ \frac{\partial L_3}{\partial y_3} \frac{\partial y_3}{\partial h_3} \frac{\partial h_3}{\partial h_2} \frac{\partial h_2}{\partial h_1} \frac{\partial h_1}{\partial W} \\
\end{aligned}
$$</p>
<p>其实主要就是因为：</p><ul>
<li>对V求偏导时，$h_3$是常数</li>
<li>对U求偏导时：<ul>
<li>$h_3$里有U，所以要继续对h3应用<code>chain rule</code></li>
<li>$h_3$里的$W, b$是常数，但是$h_2$里又有U，继续<code>chain rule</code></li>
<li>以此类推，直到$h_0$</li>
</ul>
</li>
<li>对W求偏导时一样</li>
</ul>
<p>所以：</p><ol>
<li>参数矩阵 V (对应输出 $y_t$) 的梯度很显然并没有长期依赖</li>
<li>U和V显然就是连乘($\prod$)后累加($\sum$)</li>
</ol>
<p>$$
\begin{aligned}
\frac{\partial L_t}{\partial U} = \sum_{k=0}^{t} \frac{\partial L_t}{\partial y_t} \frac{\partial y_t}{\partial h_t}
(\prod_{j=k+1}^{t}\frac{\partial h_j}{\partial h_{j-1}})
\frac{\partial h_k}{\partial U} \\
\frac{\partial L_t}{\partial W} = \sum_{k=0}^{t} \frac{\partial L_t}{\partial y_t} \frac{\partial y_t}{\partial h_t}
(\prod_{j=k+1}^{t}\frac{\partial h_j}{\partial h_{j-1}})
\frac{\partial h_k}{\partial W}
\end{aligned}
$$</p>
<p>其中的连乘项就是导致 RNN 出现梯度消失与梯度爆炸的罪魁祸首，连乘项可以如下变换：</p><ul>
<li>$h_j = tanh(Ux_j + Wh_{j-1} + b)$</li>
<li>$\prod_{j=k+1}^{t}\frac{\partial h_j}{\partial h_{j-1}} =\prod_{j=k+1}^{t} tanh' \times W$</li>
</ul>
<p>tanh' 表示 tanh 的导数，可以看到 RNN 求梯度的时候，实际上用到了 (tanh' × W) 的连乘。当 (tanh' × W) &gt; 1 时，多次连乘容易导致梯度爆炸；当 (tanh' × W) &lt; 1 时，多次连乘容易导致梯度消失。</p></div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/RNN%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E6%8E%A8%E5%AF%BC/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/RNN%E4%B8%ADbidirectional%E5%92%8Cnum_layer%E5%AF%B9output%E5%92%8Chidden%E5%BD%A2%E7%8A%B6%E7%9A%84%E5%BD%B1%E5%93%8D/" target="_self">RNN中bidirectional和num_layer对output和hidden形状的影响</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/RNN%E4%B8%ADbidirectional%E5%92%8Cnum_layer%E5%AF%B9output%E5%92%8Chidden%E5%BD%A2%E7%8A%B6%E7%9A%84%E5%BD%B1%E5%93%8D/" target="_self">
                <time class="text-uppercase">
                    April 13 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><h2>Batch first</h2>
<p>首先，我们要习惯接受<code>batch_first=False</code>（就是默认值）的思维，因为NLP中批量处理句子，是每一句取第一个词，第二个词，以此类推。
按我们习惯的把数据放在同一批（即<code>batch_first=True</code>）的思路虽然可以做到（善用切片即可），但是绕了弯路。但是如果第1批都是第1个字，第2批全是第2个字，这会自然很多（<strong>行优先</strong>）。</p><p>所以至少<code>Pytorch</code>内部，你设了True，内部也是按False来处理的，只是给了你一个语法糖（当然你组织数据就必须按True来组织了。</p><p>看个实例：</p><figure  style="flex: 79.69151670951157" ><img width="1240" height="778" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/d6a0d9ea71601f8b89c0e8465751baf4.png" alt=""/></figure><ol>
<li>假定批次是64，句长截为70，在还没有向量化的数据中，那么显然一次的输入应该为(70x64)，批次在第2位</li>
<li>注意第一行，全是2，这是设定的<code>&lt;bos&gt;</code>，这已经很好地表示了在行优先的系统里（比如<code>Matlab</code>就是列优先），会自然而且把<strong>每句话</strong>的第一个词读出来的设定了。</li>
</ol>

<pre><code># 我用的torchtext的Field进行演示， SRC是一个Field
[SRC.vocab.itos[i] for i in range(1,4)]  
['&lt;pad&gt;', '&lt;bos&gt;', '&lt;eos&gt;']
</code></pre>
<ol start="3">
<li>可见，2是开始，3是结束，1是空格（当然这是我设置的）</li>
<li>同时也能注意到，最后一行有的是3，有的是1，有的都不是，就说明句子是以70为长度进行截断的，自然结束的是3，补<code>&lt;pad&gt;</code>的是1，截断的那么那个字是多少就是多少</li>
<li>竖向取一条就是一整句话，打印出来就是箭头指向的那一大坨（共70个数字）</li>
<li>对它进行<code>index_to_string</code>(itos)，则还原出了这句话</li>
<li>nn.Embedding做了两件事：</li>
</ol>
<ul>
<li>根据vocabulary进行one-hot（稀疏）$\rightarrow$ 所以你要告诉它词典大小</li>
<li>然后再embedding成指定的低维向量（稠密）</li>
<li>所以70个数字就成了70x300，拼上维度，就是70x64x300</li>
</ul>
<p>既然讲到这了，多讲两行，假定hidden_dim=256, 一个<code>nn.RNN</code>会输出的<code>outputs</code>和<code>hidden</code>的形状如下：</p>
<pre><code>&gt;&gt;&gt; outputs.shape
torch.Size([70, 64, 256])
&gt;&gt;&gt; hidden.shape
torch.Size([1, 64, 256])
</code></pre>
<ol>
<li>即300维进去，256维出来，但是因为句子有70的长度，那就是70个output，hidden是从前传到后的，当然是最后一个</li>
<li>也因此，如果你不需要叠加多层RNN，你只需要最后一个字的output就行了<code>outputs[-1,:,:]</code>, 这个结果送到全连接层里去进行分类。</li>
</ol>
<h2>自己写一个RNN</h2>
<p>其实就是要自己把上述形状变化做对就行了。就是几个线性变换，所以我们用<code>nn.Linear</code>来拼接:</p><ol>
<li>input: 2x5x3 $\Rightarrow$ 5个序列，每一个2个词，每个词用3维向量表示</li>
<li>hidden=10, 无embedding，num_class=7</li>
<li>期待形状：</li>
</ol>
<ul>
<li>output: 2x5x7</li>
<li>hidden:1x5x10</li>
</ul>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">RNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">i2h</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">i2o</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">):</span>
        <span class="n">combined</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="nb">input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
        <span class="c1"># input shape: (2, 5, 3)</span>
        <span class="c1"># hidden shape: (2, 5, 10)</span>
        <span class="c1"># combine shape (2, 5, 13)</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">i2h</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">i2o</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span>

    <span class="k">def</span> <span class="nf">initHidden</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>

<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">num_class</span> <span class="o">=</span> <span class="mi">7</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">,(</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>

<span class="n">rnn</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span><span class="n">hidden_size</span><span class="p">,</span><span class="n">num_class</span><span class="p">)</span>
<span class="n">out</span><span class="p">,</span> <span class="n">hid</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">))</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">hid</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
<p>output:</p>
<pre><code>(torch.Size([2, 5, 7]), torch.Size([2, 5, 10]))
</code></pre>
<p>可见，output是一样的，hidden的形状不一样，事实上每一个字确实是会产生hidden的，但是pytorch并没有把它返出来（消费掉就没用了）。这里就pass了，我们主要是看一下双向和多层的情况下形状的变化，下面我们用pytorch自己的RNN来测试。</p><h1>num_layers</h1>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">num_layers</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span> <span class="c1"># 几层就需要初始几个hidden</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># input: 5x3 -&gt; 1x12 # N个批次， 5个序列(比如5个字，每个字由3个数字的向量组成)</span>
<span class="n">o</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span> <span class="c1"># 5个output, 一个final hidden</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;output shape&#39;</span><span class="p">,</span> <span class="n">o</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;hidden shape&#39;</span><span class="p">,</span> <span class="n">h</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<p>输出：</p>
<pre><code>output shape torch.Size([5, 2, 12])  # 2个批次，5个词，12维度输出
hidden shape torch.Size([3, 2, 12]) # 3层会输出3个hidden，2个批次
</code></pre>
<p>加上embedding, RNN改成GRU</p><div class="highlight"><pre><span></span><span class="c1"># 这次加embedding</span>
<span class="c1"># 顺便把 RNN 改 GRU</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">embed_size</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">3</span>
<span class="c1"># 要求词典长度不超过5，输出向量长度为10</span>
<span class="n">emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span> 
<span class="c1"># 输入为embeding维度，输出（和隐层）为8维度</span>
<span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># 这次设了num_layers=2，就要求有两个hidden了</span>
<span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
<span class="c1"># 因为数据会用embedding包一次，所以input没有了维度要求（只有大小要求，每个数字要小于字典长度）</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">))</span> 
<span class="n">e</span> <span class="o">=</span> <span class="n">emb</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;input.shape:&#39;</span><span class="p">,</span> <span class="n">x0</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;embedding.shape:&#39;</span><span class="p">,</span> <span class="n">e</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (3,4)会扩展成（3,4,10), 10维是rnn的input维度，正好</span>
<span class="n">o</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;output.shape:</span><span class="si">{</span><span class="n">o</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">, hidden.shape:</span><span class="si">{</span><span class="n">h</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

<pre><code>input.shape: torch.Size([5, 3])
embedding.shape: torch.Size([5, 3, 10])
output.shape:torch.Size([5, 3, 8]), hidden.shape:torch.Size([2, 3, 8])
</code></pre>
<p>唯一要注意的变化就是input，因为embedding是把字典大小的维度转换成指定大小的维度，暗含了你里面的每一个数字都是字典的索引，所以你组装demo数据的时候，要生成小于字典大小(<code>vocab_size</code>）的数字作为输入。</p><h2>bidirectional</h2>
<p>这次加<strong>bidirectional</strong></p><ul>
<li>batch_first = False</li>
<li>x (5, 3) -&gt; 3个序列，每个序列5个数</li>
<li>embedding(5, 10) -&gt; 输入字典长5，输出向量长10 -&gt; (5, 3, 10) -&gt; 3个序列，每个序列5个10维向量</li>
<li>hidden必须为8维，4个（num_layers=2, bidirection),3个批次 -&gt; (4,3,8)</li>
<li>rnn(10, 8) -&gt; 输入10维，输出8维</li>
</ul>
<div class="highlight"><pre><span></span><span class="c1"># 这次加 bidirection</span>

<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">embed_size</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">num_layers</span> <span class="o">=</span> <span class="mi">2</span>
<span class="c1"># 要求词典长度不超过5，输出向量长度为10</span>
<span class="n">emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span> 
<span class="c1"># 输入为embeding维度，输出（和隐层）为8维度</span>
<span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># 这次设了num_layers=2，就要求有两个hidden了</span>
<span class="c1"># 加上双向，就有4个了，这里乘以2</span>
<span class="c1"># h0 = (torch.rand(2, batch_size, hidden_size), torch.rand(2, batch_size, hidden_size))</span>
<span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">num_layers</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
<span class="c1"># 因为数据会用embedding包一次，所以input没有了维度要求（只有大小要求，每个数要小于字典长度）</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">))</span> 
<span class="n">e</span> <span class="o">=</span> <span class="n">emb</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;input.shape:&#39;</span><span class="p">,</span> <span class="n">x0</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;embedding.shape:&#39;</span><span class="p">,</span> <span class="n">e</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (3,4)会扩展成（3,4,10), 10维是rnn的input维度，正好</span>
<span class="c1"># hidden = torch.cat((h0[-2,:,:], h0[-1,:,:]),1)</span>
<span class="n">o</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;output.shape:</span><span class="si">{</span><span class="n">o</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">, hidden.shape:</span><span class="si">{</span><span class="n">h</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

<pre><code>input.shape: torch.Size([5, 3])
embedding.shape: torch.Size([5, 3, 10])
output.shape:torch.Size([5, 3, 16]), hidden.shape:torch.Size([4, 3, 8])
</code></pre>
<p>可见，双向会使输出多一倍，可以用<code>[:hidden_size], [hidden_size:]</code>分别取出来，我们<strong>验证</strong>一下，用框架生成一个双向的GRU，然后手动生成一个正向的一个负向的，复制参数，看一下输出：</p><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="c1"># 制作一个正序和反序的input</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">17</span><span class="p">)</span>
<span class="n">random_input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">normal_</span><span class="p">(),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">reverse_input</span> <span class="o">=</span> <span class="n">random_input</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">:,</span> <span class="p">:]</span>

<span class="n">bi_grus</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">reverse_gru</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">gru</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">reverse_gru</span><span class="o">.</span><span class="n">weight_ih_l0</span> <span class="o">=</span> <span class="n">bi_grus</span><span class="o">.</span><span class="n">weight_ih_l0_reverse</span>
<span class="n">reverse_gru</span><span class="o">.</span><span class="n">weight_hh_l0</span> <span class="o">=</span> <span class="n">bi_grus</span><span class="o">.</span><span class="n">weight_hh_l0_reverse</span>
<span class="n">reverse_gru</span><span class="o">.</span><span class="n">bias_ih_l0</span> <span class="o">=</span> <span class="n">bi_grus</span><span class="o">.</span><span class="n">bias_ih_l0_reverse</span>
<span class="n">reverse_gru</span><span class="o">.</span><span class="n">bias_hh_l0</span> <span class="o">=</span> <span class="n">bi_grus</span><span class="o">.</span><span class="n">bias_hh_l0_reverse</span>
<span class="n">gru</span><span class="o">.</span><span class="n">weight_ih_l0</span> <span class="o">=</span> <span class="n">bi_grus</span><span class="o">.</span><span class="n">weight_ih_l0</span>
<span class="n">gru</span><span class="o">.</span><span class="n">weight_hh_l0</span> <span class="o">=</span> <span class="n">bi_grus</span><span class="o">.</span><span class="n">weight_hh_l0</span>
<span class="n">gru</span><span class="o">.</span><span class="n">bias_ih_l0</span> <span class="o">=</span> <span class="n">bi_grus</span><span class="o">.</span><span class="n">bias_ih_l0</span>
<span class="n">gru</span><span class="o">.</span><span class="n">bias_hh_l0</span> <span class="o">=</span> <span class="n">bi_grus</span><span class="o">.</span><span class="n">bias_hh_l0</span>

<span class="n">bi_output</span><span class="p">,</span> <span class="n">bi_hidden</span> <span class="o">=</span> <span class="n">bi_grus</span><span class="p">(</span><span class="n">random_input</span><span class="p">)</span>
<span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="n">gru</span><span class="p">(</span><span class="n">random_input</span><span class="p">)</span>
<span class="n">reverse_output</span><span class="p">,</span> <span class="n">reverse_hidden</span> <span class="o">=</span> <span class="n">reverse_gru</span><span class="p">(</span><span class="n">reverse_input</span><span class="p">)</span>  <span class="c1"># 分别取[(4,3,2,1,0),:,:] -&gt; 即倒序送入input</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;bi_output:&#39;</span><span class="p">,</span> <span class="n">bi_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">bi_output</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">bi_output</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>                <span class="c1"># 双向输出中的后半截</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">reverse_output</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="p">))</span> <span class="c1"># 反向输出</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>                   <span class="c1"># 单独一个rnn的输出 </span>
<span class="nb">print</span><span class="p">(</span><span class="n">bi_output</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>                <span class="c1"># 双向输出中的前半截</span>
</pre></div>

<pre><code>bi_output: torch.Size([5, 1, 2])
tensor([[-0.2336, -0.3068],
        [ 0.0660, -0.6004],
        [ 0.0859, -0.5620],
        [ 0.2164, -0.5750],
        [ 0.1229, -0.3608]])
tensor([-0.3068, -0.6004, -0.5620, -0.5750, -0.3608])
tensor([-0.3068, -0.6004, -0.5620, -0.5750, -0.3608])
tensor([-0.2336,  0.0660,  0.0859,  0.2164,  0.1229])
tensor([-0.2336,  0.0660,  0.0859,  0.2164,  0.1229])
</code></pre>
<p>现在你们应该知道<code>bidirectional</code>的双倍输出是怎么回事了，再来看看hidden</p><div class="highlight"><pre><span></span><span class="n">hidden</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">reverse_hidden</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">bi_hidden</span><span class="o">.</span><span class="n">shape</span>
<span class="n">bi_hidden</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">reverse_hidden</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">hidden</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">data</span>
</pre></div>

<pre><code>(torch.Size([1, 1, 1]), torch.Size([1, 1, 1]), torch.Size([2, 1, 1]))
(tensor([ 0.1229, -0.3068]), tensor([-0.3068]), tensor([0.1229]))
</code></pre>
<ul>
<li>正向的输出就是单向rnn</li>
<li>反向的输出就是把数据反传的单向rnn</li>
<li>双向rnn出来的第最后一个hidden（后半截）就是反向完成后的hidden</li>
</ul>
<figure  style="flex: 83.6996336996337" ><img width="914" height="546" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/67bb7c8df3810d83a7f07909fbd601f9.png" alt=""/></figure><p>由打印出来的数据可知：</p><ul>
<li>最后一个hidden，就是反向RNN的最后一个hidden（时间点在开头）</li>
<li>也是双向RNN里的第一个输出（<strong>的最后一个元素</strong>）</li>
<li>也是单向RNN（但是数据反传）（或者正向，但逆时序）里的最后一个输出</li>
</ul>
<hr />
<p>双向RNN里</p><ul>
<li>倒数第二个hidden，是正向的最后一个hidden（时间点在结尾）</li>
<li>它也是output里面的值，它是双向输出里的最后一个的<strong>第一个元素</strong></li>
</ul>
<p>总的来说</p><ul>
<li>output由正反向输出横向拼接（所有）</li>
<li>hidden由正反向hidden竖向拼接（top layer)</li>
</ul>
<figure  style="flex: 71.92575406032482" ><img width="1240" height="862" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/671265db99cdb4c1fcda808d82a08794.png" alt=""/></figure></div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/RNN%E4%B8%ADbidirectional%E5%92%8Cnum_layer%E5%AF%B9output%E5%92%8Chidden%E5%BD%A2%E7%8A%B6%E7%9A%84%E5%BD%B1%E5%93%8D/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/%E5%87%A0%E7%A7%8D%E6%95%99%E6%9D%90%E9%87%8C%E6%B1%82%E8%A7%A3Ax%3D0%E7%AC%94%E8%AE%B0/" target="_self">几种教材里求解Ax=0笔记</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/%E5%87%A0%E7%A7%8D%E6%95%99%E6%9D%90%E9%87%8C%E6%B1%82%E8%A7%A3Ax%3D0%E7%AC%94%E8%AE%B0/" target="_self">
                <time class="text-uppercase">
                    March 05 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><p>如果一个矩阵化简为</p><p>$$
A=
\left[
    \begin{array}{cccc|c}
    1&amp;2&amp;2&amp;2&amp;0 \\
    0&amp;0&amp;1&amp;2&amp;0 \\
    0&amp;0&amp;0&amp;0&amp;0
    \end{array}
\right] \tag{0}
$$</p>
<p>求解$\bf{A}\it\vec{x}=0$</p><p>对比在不同教材中的解题思路。</p><h2>可汗学院解法</h2>
<p>先继续化简为<code>Reduced Row Echelon Form</code> (RREF)</p><p>$$
\left[
    \begin{array}{cccc|c}
    1&amp;2&amp;0&amp;-2&amp;0 \\
    0&amp;0&amp;1&amp;2&amp;0 \\
    0&amp;0&amp;0&amp;0&amp;0
    \end{array}
\right] \tag{1. 1}
$$</p>
<p>还原为方程组:</p><p>$$ 
\begin{cases}
    x_1=-2x_2+2x_4 \\
    x_3=-2x_4\\
\end{cases} \tag{1.2}
$$</p>
<p>用$x_2$和$x_4$来表示$x_1$和$x_3$，填满矩阵相应位置即可得解：</p><p>$$
\left[\begin{smallmatrix} x_1\\x_2\\x_3\\x_4 \end{smallmatrix}\right]=
x_2 \left[\begin{smallmatrix} -2\\1\\0\\0 \end{smallmatrix}\right] +
x_4 \left[\begin{smallmatrix} 2\\0\\-2\\1 \end{smallmatrix}\right] \tag{1.3}
$$</p>
<p>如果不是太直观的话，其实就是把以下方程写成了矩阵的形式：</p><p>$$
\begin{cases}
    x_1=-2x_2+2x_4 \\
    x_2=x_2\\
    x_3=-2x_4\\
    x_4=x_4
\end{cases}\tag{1. 4}
$$</p>
<hr />
<h2>剑桥教材解法</h2>
<blockquote>
<p>《Mathematics for Machine Learning》</p></blockquote>
<p>by Marc Peter Deisenroth, A Aldo Faisal, Cheng Soon Ong,
Cambridge University</p><p>化简为<code>RREF</code>后，观察到$c_1$和$c_3$列可组成一个单位矩阵（<code>identity matrix</code>）$\left[\begin{smallmatrix} 1&amp;0\0&amp;1 \end{smallmatrix}\right]$</p><blockquote>
<p>如果是解$\bf{A}\it\vec{x}=b$，此时可用此矩阵求出特解，但此处是0，所以此步省略，直接求通解</p></blockquote>
<p>我们用$c_1$和$c_3$来表示其它列：</p><p>$$
\begin{cases}
c_2=2c_1 \\
c_4=-2c_1+2c_3
\end{cases} \tag{2.1}
$$</p>
<p>我们利用$c_2-c_2=0, c_4-c_4=0$来构造0值（通解都是求0）：</p><p>$$
\begin{cases}
2c_1-\color{green}{c_2}=0 \\
-2c_1+2c_3-\color{green}{c_4}=0
\end{cases} \tag{2.2}
$$</p>
<p>补齐方程，整理顺序（以便直观地看到系数）得：</p><p>$$
\begin{cases}
\color{red}2c_1\color{red}{-1}c_2+\color{red}{0}c_3+\color{red}{0}c_4=0 \\
\color{red}{-2}c_1+\color{red}0c_2+\color{red}2c_3\color{red}{-1}c_4=0
\end{cases} \tag{2. 3}
$$</p>
<p>因为矩阵乘向量可以理解为矩阵和<code>列向量</code>$\vec{c}$与向量$x$的点积之和$\sum_{i=1}^4 x_ic_i$，所以红色的系数部分其实就是$(x_1, x_2, x_3, x_4)$，得解：</p><p>$$
\left\{x\in\mathbb{R}^4:x=\lambda_1\left[\begin{smallmatrix} 2\\-1\\0\\0 \end{smallmatrix}\right]+\lambda_2\left[\begin{smallmatrix} 2\\0\\-2\\1 \end{smallmatrix}\right],\lambda_1,\lambda_2\in\mathbb{R}\right\} \tag{2.4}
$$</p>
<blockquote>
<p>与<strong>可汗学院</strong>的解得到的两个向量比较下，是一样的，都是$[2,-1,0,0]^T$和$[2,0,-2,1]^T$。</p></blockquote>
<hr />
<p>##麻省理工教材解法</p><blockquote>
<p>《Introduction to Linear Alegebra》</p></blockquote>
<p>by Gilbert Strang, 
Massachusetts Institute of Technology</p><p>无需继续化简为<code>RREF</code>，直接对方程组：</p><p>$$ 
\begin{cases}
    x_1=-2x_2+2x_4 \\
    x_3=-2x_4\\
\end{cases} \tag{3.1}
$$</p>
<p>使用特解。考虑到$x_1,x_3$为主元（<code>pivot</code>），那么分别设$[\begin{smallmatrix} x_2 \ x_4 \end{smallmatrix}]$ 为$[\begin{smallmatrix} 1 \ 0 \end{smallmatrix}]$ 和$[\begin{smallmatrix} 0 \ 1 \end{smallmatrix}]$ 。
两种情况各代入一次，解出$x_1,x_3$，仍然是$[2,\color{red}{-1},0,\color{red}0]^T$和$[2,\color{red}0,-2,\color{red}1]^T$，红色标识了代入值，黑色即为代入后的解。</p><p><code>MIT</code>不止提供了这一个思路，解法二如下：</p><p>这次需要化简为<code>RREF</code>，然后互换第<code>2</code>列和第<code>3</code>列（<strong><code>记住这次互换</code></strong>），还记得剑桥的方法里发现$c_1,c_3$能组成一个单位矩阵吗？这里的目的是通过移动列，直接在表现形式上变成单位矩阵：</p><p>$$
\left[
    \begin{array}{cc:cc}
    1&amp;0&amp;2&amp;-2\\
    0&amp;1&amp;0&amp;2\\
    \hdashline
    0&amp;0&amp;0&amp;0
    \end{array}
\right] \tag{3.2}
$$</p>
<p>这里把用虚线反矩阵划成了四个区，左上角为一个<code>Identity Matrix</code>，我们记为<code>I</code>，右上角为自由列，我们记为<code>F</code>，矩阵（这次我们标记为<strong>R</strong>）变成了</p><p>$$
\bf{\it{R}}=
\begin{bmatrix}
I&amp;F\\
0&amp;0
\end{bmatrix} \tag{3. 3}
$$</p>
<p>求解$\bf{\it{R}}\it\vec{x}=0$，得到$x=\left[\begin{smallmatrix} -F\I \end{smallmatrix}\right]$，把<strong>F</strong>和<strong>I</strong>分别展开(<code>记得F要乘上-1</code>)：</p><p>$$
\begin{bmatrix}
-2&amp;2\\
0&amp;-2\\
1&amp;0\\
0&amp;1
\end{bmatrix} \tag{3.4}
$$</p>
<p>还记得前面加粗提示的交换了两列吗？我们交换了两列，倒置后，我们要把第<code>2, 3</code><strong>行</strong>给交换一下：</p><p>$$
\begin{bmatrix}
-2&amp;2\\
1&amp;0\\
0&amp;-2\\
0&amp;1
\end{bmatrix} \tag{3.5}
$$</p>
<p>是不是又得到了两个熟悉的$[2,-1,0,0]^T$和$[2,0,-2,1]^T$。？</p><blockquote>
<p>当时看到Gilbert教授简单粗暴地用$[\begin{smallmatrix} 1 \ 0 \end{smallmatrix}]$ 和$[\begin{smallmatrix} 0 \ 1 \end{smallmatrix}]$ 直接代入求出解，道理都不跟你讲，然后又给你画大饼，又是F又是I的，觉得可能他的课程不适合初学者，LOL。不过，这些Gilbert教授在此演示的解法并不适用于$\bf{A}\it\vec{x}=b$。</p></blockquote>
<p>在此特用笔记把几本教材里的思路都记录一下。</p></div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/%E5%87%A0%E7%A7%8D%E6%95%99%E6%9D%90%E9%87%8C%E6%B1%82%E8%A7%A3Ax%3D0%E7%AC%94%E8%AE%B0/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/scikit-learn%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0%28%E4%B8%80%29/" target="_self">scikit-learn官网教程笔记(一)</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/scikit-learn%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0%28%E4%B8%80%29/" target="_self">
                <time class="text-uppercase">
                    March 02 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><p>当初翻scikit learn文档的时候，越翻越多，干脆把它的教程拿出来看了看，只有前面的部分，主要想看看scikit learn角度整理的知识体系，果然，一开始就是从监督和非监督讲起：</p><h2>Machine learning</h2>
<p>In general, a learning problem considers a set of n samples of data and then tries to predict properties of unknown data.</p><p>Learning problems fall into a few categories:</p><ul>
<li><a href="https://scikit-learn.org/stable/supervised_learning.html#supervised-learning">supervised learning</a>, in which the data comes with additional attributes that we want to predict. This problem can be either:<ul>
<li><strong>classification</strong>: samples belong to two or more classes and we want to learn from already labeled data how to predict the class of unlabeled data.</li>
<li><strong>regression</strong>: if the desired output consists of one or more continuous variables, then the task is called regression.</li>
</ul>
</li>
<li><a href="https://scikit-learn.org/stable/unsupervised_learning.html#unsupervised-learning">unsupervised learning</a>, in which the training data consists of a set of input vectors x without any corresponding target values.<ul>
<li><strong>clustering</strong>: The goal in such problems may be to discover groups of similar examples within the data</li>
<li><strong>density estimation</strong>: to determine the <em>distribution</em> of data within the input space</li>
<li><strong>down dimensional</strong>: project the data from a high-dimensional space down to two or three dimensions for the purpose of visualization.</li>
</ul>
</li>
</ul>
<h2>dataset</h2>
<p><a href="https://scikit-learn.org/stable/datasets/loading_other_datasets.html#external-datasets">see more</a> dataset load method</p><h3>data, targets</h3>
<p>A <code>dataset</code> is a dictionary-like object that holds all the data and some metadata about the data. This data is stored in the <code>.data</code> member, which is a n_samples, n_features array. In the case of supervised problem, one or more response variables are stored in the <code>.target</code> member.</p><h2>estimator</h2>
<p>In scikit-learn, an <code>estimator</code> for classification is a Python object that <strong>implements</strong> the methods <code>fit(X, y)</code> and <code>predict(T)</code>,  an estimator is any object that learns from data</p><p>An example of an estimator is the class <code>sklearn.svm.SVC</code>, which implements <code>support vector classification</code>.</p><ul>
<li>estimator.param1 表示传入的参数</li>
<li>estimator.param1_   表示estimated param</li>
</ul>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="n">iris</span>   <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">digits</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_digits</span><span class="p">()</span>

<span class="c1"># digits.data.shape,  (1797, 64)</span>
<span class="c1"># digits.images.shape, (1797, 8, 8)</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="c1"># first, we treat the estimator as a black box, and set params manually</span>
<span class="c1"># or we can use `grid search` and `cross validation` to determine the best params</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">100.</span><span class="p">)</span>

<span class="c1"># train(or learn) from all (except last one) digits</span>
<span class="c1"># validate with the last digit</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:])</span>
</pre></div>

<pre><code>array([8])
</code></pre>
<figure  style="flex: 50.81967213114754" ><img width="496" height="488" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/5f41cd44fe4cf94f9dbfee9954b82ec6.png" alt=""/></figure><div class="highlight"><pre><span></span><span class="c1"># better practise</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># flatten the images</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># Create a classifier: a support vector classifier</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="c1"># Split data into 50% train and 50% test subsets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Learn the digits on the train subset</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predict the value of the digit on the test subset</span>
<span class="n">predicted</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
<h2>classification_report</h2>
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report">classification_report</a> builds a text report showing the main classification metrics.</p><h2>confusion matrix</h2>
<p><a href="https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix">plot_confusion_matrix</a> can e used to visually represent a confusion matrix:</p><p>$
\begin{array}{c|c}
&amp; Positive &amp; Negative \
\hline
True &amp; TP &amp; TN \
\hline
False &amp; FP &amp; FN
\end{array}
$</p><p>混淆矩阵还有另一种写法，即横纵轴都以positive，和negative表示，而不是如上的一个是指标，一个是判断（正确，错误）。这些都不重要，自己看清楚不要臆测就好了。
如果：
$
\begin{array}{c|c}
&amp; Positive &amp; Negative \
\hline
True &amp;3 &amp; 4 \
\hline
Fa lse &amp;1 &amp; 2
\end{array}
$</p><p>解读：</p><ul>
<li>预测Positive共4例，成功3，失败1</li>
<li>预测Negative共6例，成功4，失败2</li>
</ul>
<p>所以反推正样本3+2=5，负样1+4=5，共10例</p><p>大原则，我们看到的时候已经是结果，所以只能从结果反推真实情况，比如正确的<code>正</code>和错误的<code>负</code>，加起来就是样本的<code>正</code>，等等</p><ul>
<li>准确率：7/10 （TP+TN/total) 根据上文的文字描述，其实就是判断成功的次数</li>
<li>精确率：3/4 (TP/TP+FP) 即<strong>只关注一个指标</strong>(等于是竖向统计），比如正例，或负例，然后观察它错了多少。<ul>
<li>本例中，只预测了4个正，就错了一个</li>
</ul>
</li>
<li>召回率：3/5 (TP/TP+FN) 仍然只关注一个指标，比如正例，但是召回率关心你把所有的“正例“找出来多少<ul>
<li>也就是说，如果你把所有的样本判断为正例，召回率可达100%</li>
</ul>
</li>
</ul>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Classification report for classifier </span><span class="si">{</span><span class="n">clf</span><span class="si">}</span><span class="s2">:</span><span class="se">\n</span><span class="s2">&quot;</span>
      <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>

<pre><code>Classification report for classifier SVC(gamma=0.001):
              precision    recall  f1-score   support

           0       1.00      0.99      0.99        88
           1       0.99      0.97      0.98        91
           2       0.99      0.99      0.99        86
           3       0.98      0.87      0.92        91
           4       0.99      0.96      0.97        92
           5       0.95      0.97      0.96        91
           6       0.99      0.99      0.99        91
           7       0.96      0.99      0.97        89
           8       0.94      1.00      0.97        88
           9       0.93      0.98      0.95        92

    accuracy                           0.97       899
   macro avg       0.97      0.97      0.97       899
weighted avg       0.97      0.97      0.97       899
</code></pre>
<div class="highlight"><pre><span></span><span class="n">disp</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">figure_</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Confusion matrix:</span><span class="se">\n</span><span class="si">{</span><span class="n">disp</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
<figure  style="flex: 54.00696864111498" ><img width="620" height="574" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/2088af7c0ab08ef8b680eab7d06d4bcd.png" alt=""/></figure><h2>Conventions</h2>
<h3>Type Casting</h3>
<p>Unless otherwise specified, input will be cast to float64:</p><div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">random_projection</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">X</span><span class="o">.</span><span class="n">dtype</span>
<span class="n">dtype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">transformer</span> <span class="o">=</span> <span class="n">random_projection</span><span class="o">.</span><span class="n">GaussianRandomProjection</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">X_new</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">X_new</span><span class="o">.</span><span class="n">dtype</span>
<span class="n">dtype</span><span class="p">(</span><span class="s1">&#39;float64&#39;</span><span class="p">)</span>
</pre></div>
<p>the example above, the <code>float32</code> X is casst to <code>float64</code> by <code>fit_transform(X)</code></p><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">()</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="mi">3</span><span class="p">])))</span>

<span class="c1"># fit string</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="mi">3</span><span class="p">])))</span>
<span class="c1"># [&#39;setosa&#39;, &#39;setosa&#39;, &#39;setosa&#39;]</span>
</pre></div>

<pre><code>[0, 0, 0]
['setosa', 'setosa', 'setosa']
</code></pre>
<h3>Refitting and updating parameters</h3>
<p>Hyper-parameters of an estimator can be updated after it has been <strong>constructed</strong> via the <code>set_params()</code>. then you call <code>fit()</code>, the learned will be overwrite.</p><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>    <span class="c1"># 注意换了种load方式</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">()</span>
<span class="n">clf</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">5</span><span class="p">]))</span>

<span class="n">clf</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">5</span><span class="p">]))</span>
</pre></div>

<pre><code>linear [0 0 0 0 0]
rbf [0 0 0 0 0]
</code></pre>
<h2>Multiclass vs. multilabel fitting</h2>
<p>When using multiclass classifiers, the learning and prediction task that is performed is <strong>dependent on</strong> the format of the target data fit upon:</p><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.multiclass</span> <span class="kn">import</span> <span class="n">OneVsRestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelBinarizer</span>

<span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>

<span class="c1"># 注意一行的写法</span>
<span class="n">classif</span> <span class="o">=</span> <span class="n">OneVsRestClassifier</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">SVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> 
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;1d:&#39;</span><span class="p">,</span> <span class="n">classif</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="c1"># one-hot</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">LabelBinarizer</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;y:&#39;</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;one-hot:&#39;</span><span class="p">,</span> <span class="n">classif</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>  <span class="c1"># 可以看到，已经开始不准确了b</span>
</pre></div>

<pre><code>1d: [0 0 1 1 2]
y: [[1 0 0]
 [1 0 0]
 [0 1 0]
 [0 1 0]
 [0 0 1]]
one-hot: [[1 0 0]
 [1 0 0]
 [0 1 0]
 [0 0 0]
 [0 0 0]]
</code></pre>
<h3>multiple label</h3>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MultiLabelBinarizer</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]</span>  <span class="c1"># 一个instance被赋予多个label，（甚至第4个有3个label)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">MultiLabelBinarizer</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">classif</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>

<pre><code>array([[1, 1, 0, 0, 0],
       [1, 0, 1, 0, 0],
       [0, 1, 0, 1, 0],
       [1, 0, 1, 0, 0],
       [1, 0, 1, 0, 0]])
</code></pre>
<h2>KNN (k nearest neighbors classification)</h2>
<p>KNN是一种用身边最近的n个数据点哪个类别最多来推断自己类别的的方法，所以本质上还是有标签的（周边数据点都是打标的）</p><blockquote>
<p>我还写过一种无监督的<strong>聚类</strong>方法，叫<code>k-means</code>，就因为都有个<code>k</code>，一度都让我混淆了起来。其实没有关系，<code>k-means</code>是随机选k个点当作中心点，找出与它们最近的点来聚类，然后再每个点取中心，这么迭代N次之后，聚类好的数据也会越来越远。这里只是作个旁记。</p></blockquote>

<pre><code>import numpy as np
from sklearn import datasets
iris_X, iris_y = datasets.load_iris(return_X_y=True)
# Split iris data in train and test data
# A random permutation, to split the data randomly
np.random.seed(0)
indices = np.random.permutation(len(iris_X))
iris_X_train = iris_X[indices[:-10]]
iris_y_train = iris_y[indices[:-10]]
iris_X_test = iris_X[indices[-10:]]
iris_y_test = iris_y[indices[-10:]]
# Create and fit a nearest-neighbor classifier
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(iris_X_train, iris_y_train)
print(knn.predict(iris_X_test))
print(iris_y_test)
</code></pre>

<pre><code>[1 2 1 0 0 0 2 1 2 0]
[1 1 1 0 0 0 2 1 2 0]
</code></pre>
<h3>The curse of dimensionality</h3>
<p>nearest neighbor算法，维数越高，需要的数据越多，才能保证在一点的附近有足够多的neighbor。所以一般来说当特征很多时KNN的效果会下降。当然也有例外，某次做一个20个特征的KNN时候，结果居然比随机森林还要好_(:з」∠)_场面一度十分尴尬… <a href="https://www.zhihu.com/question/27836140/answer/145952018">参考</a></p><h2>Shrinkage</h2>
<p>If there are <strong>few</strong> data points per dimension, noise in the observations induces <strong>high variance</strong>:</p><h1>train with very few data</h1>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span> <span class="mf">.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
<span class="n">regr</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span> 

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span> 
    <span class="n">this_X</span> <span class="o">=</span> <span class="mf">.1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">+</span> <span class="n">X</span>
    <span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">this_X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test</span><span class="p">))</span> 
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">this_X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
<figure  style="flex: 75.50607287449392" ><img width="746" height="494" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/22a14395fafb62dcbaabdca600191fd6.png" alt=""/></figure><h2>Ridge regression:</h2>
<p>A solution in high-dimensional statistical learning is to shrink the regression coefficients to zero: any two randomly chosen set of observations are likely to be uncorrelated. This is called Ridge regression.</p><p>This is an example of <code>bias/variance</code> tradeoff:</p><p>the <strong>larger</strong> the ridge <code>alpha</code> parameter,</p><ul>
<li>the <strong>higher</strong> the <code>bias</code></li>
<li>the <strong>lower</strong> the <code>variance</code>.</li>
</ul>
<p>lasso 回归和岭回归（ridge regression）其实就是在标准线性回归的基础上分别加入 L1 和 L2 正则化（regularization）。相比直接把一些特征的系数置零，只是把它们的“贡献”变小，即乘一下较低的权重（惩罚，imposing a penalty on the size of the coefficients）。</p><p>Lasso 更多用于估计稀疏样本的系数。</p><p>以下关于几个加了正则的demo和调优是整理笔记整理岔了，不是官方教程里的，但是也是我的学习笔记，正好演示一些demo和cross validation的用法就不删了。</p><ul>
<li>L1-norm (Lasso)</li>
<li>L2-norm (Ridge)</li>
<li>(Elastic Net) (l1+l2)</li>
</ul>
<p>Lasso:<br />
$$J(\theta) = \frac{1}{2}\sum_{i}^{m}(y^{(i)} - \theta^Tx^{(i)})^2 + 
            \color{red} {\lambda \sum_{j}^{n}|\theta_j|}$$</p><p>Ridge:<br />
$$J(\theta) = \frac{1}{2}\sum_{i}^{m}(y^{(i)} - \theta^Tx^{(i)})^2 +
            \color{blue} {\lambda \sum_{j}^{n}\theta_j^2}$$</p><p>ElasticNet:<br />
$$J(\theta) = \frac{1}{2}\sum_{i}^{m}(y^{(i)} - \theta^Tx^{(i)})^2 + 
            \lambda(\rho 
            \color{red}{\sum_{j}^{n}|\theta_j|} + 
            (1-\rho)\color{blue}{ \sum_{j}^{n}\theta_j^2})$$</p><h4>岭回归demo</h4>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_boston</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">boston</span> <span class="o">=</span> <span class="n">load_boston</span><span class="p">()</span>   <span class="c1"># 还记得上一节课 load_iris() 吗？</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">boston</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">boston</span><span class="o">.</span><span class="n">target</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>   <span class="c1"># 用岭回归构建模型</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>                 <span class="c1"># 拟合</span>
<span class="n">train_score</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span> <span class="c1"># 模型对训练样本得准确性</span>
<span class="n">test_score</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>    <span class="c1"># 模型对测试集的准确性</span>

<span class="nb">print</span><span class="p">(</span><span class="n">boston</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="mi">5</span><span class="p">],</span> <span class="n">boston</span><span class="o">.</span><span class="n">target</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;train_score: </span><span class="si">{</span><span class="n">train_score</span><span class="si">}</span><span class="s2">, test_score: </span><span class="si">{</span><span class="n">test_score</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>

<pre><code>[[6.3200e-03 1.8000e+01 2.3100e+00 0.0000e+00 5.3800e-01 6.5750e+00
  6.5200e+01 4.0900e+00 1.0000e+00 2.9600e+02 1.5300e+01 3.9690e+02
  4.9800e+00]
 [2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 6.4210e+00
  7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9690e+02
  9.1400e+00]
 [2.7290e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 7.1850e+00
  6.1100e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9283e+02
  4.0300e+00]
 [3.2370e-02 0.0000e+00 2.1800e+00 0.0000e+00 4.5800e-01 6.9980e+00
  4.5800e+01 6.0622e+00 3.0000e+00 2.2200e+02 1.8700e+01 3.9463e+02
  2.9400e+00]
 [6.9050e-02 0.0000e+00 2.1800e+00 0.0000e+00 4.5800e-01 7.1470e+00
  5.4200e+01 6.0622e+00 3.0000e+00 2.2200e+02 1.8700e+01 3.9690e+02
  5.3300e+00]] [24.  21.6 34.7 33.4 36.2]

train_score: 0.723706995939315, test_score: 0.7926416423787221
</code></pre>
<h4>岭回归调优</h4>
<ul>
<li>Ridge regression is a penalized linear regression model for predicting a numerical value</li>
<li>and it can be very effective when applied to classification</li>
<li>the important parameter to tune is the regularization strength (<code>alpha</code>) in (0.1, 1.0) step = 0.1</li>
</ul>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RidgeCV</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_boston</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">boston</span> <span class="o">=</span> <span class="n">load_boston</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">boston</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">boston</span><span class="o">.</span><span class="n">target</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># 用redge cross validation建模而不是Ridge</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">RidgeCV</span><span class="p">(</span><span class="n">alphas</span><span class="o">=</span><span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.005</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.0005</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">],</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">alpha_</span><span class="p">)</span>
</pre></div>

<pre><code>0.01
</code></pre>
<h4>lass demo和调优</h4>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>

<span class="n">lasso_reg</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">lasso_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;lasso score&quot;</span><span class="p">,</span> <span class="n">lasso_reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>

<span class="c1"># 调优</span>
<span class="n">lscv</span> <span class="o">=</span> <span class="n">LassoCV</span><span class="p">(</span><span class="n">alphas</span><span class="o">=</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.005</span><span class="p">,</span> <span class="mf">0.0025</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.00025</span><span class="p">),</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">lscv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Lasso optimal alpha: </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">lscv</span><span class="o">.</span><span class="n">alpha_</span><span class="p">)</span>
</pre></div>

<pre><code>lasso score 0.7956864030940746
Lasso optimal alpha: 0.010
</code></pre>
<h4>弹性网络</h4>
<p>大多情况下应该避免使用纯线性回归，如果特征数比较少，更倾向于<code>Lasso回归</code>或者<code>弹性网络</code>，因为它们会将无用的特征权重降为0，一般来说<code>弹性网络</code>优于<code>Lasso回归</code></p><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNet</span>

<span class="n">e_net</span> <span class="o">=</span> <span class="n">ElasticNet</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">e_net</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;e_net score:&quot;</span><span class="p">,</span> <span class="n">e_net</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>

<span class="c1"># 调优</span>
<span class="n">encv</span> <span class="o">=</span> <span class="n">ElasticNetCV</span><span class="p">(</span><span class="n">alphas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.005</span><span class="p">,</span> <span class="mf">0.0025</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">),</span> 
                    <span class="n">l1_ratio</span><span class="o">=</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">),</span> 
                    <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">encv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ElasticNet optimal alpha: </span><span class="si">%.3f</span><span class="s1"> and L1 ratio: </span><span class="si">%.4f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">encv</span><span class="o">.</span><span class="n">alpha_</span><span class="p">,</span> <span class="n">encv</span><span class="o">.</span><span class="n">l1_ratio_</span><span class="p">))</span>
</pre></div>

<pre><code>e_net score: 0.7926169728251697
ElasticNet optimal alpha: 0.001 and L1 ratio: 0.5000
</code></pre>
<p>回到教程，对前例（数据过少引起的过拟合），加入了惩罚项后：</p><div class="highlight"><pre><span></span><span class="n">regr</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">.1</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span> 
    <span class="n">this_X</span> <span class="o">=</span> <span class="mf">.1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">+</span> <span class="n">X</span>
    <span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">this_X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test</span><span class="p">))</span> 
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">this_X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span> 

<span class="c1"># 观察图像的不同，其实可以理解为样本过少时的”过拟合“，引入忽略的指标后虽然对训练集的准确率大打折扣，但确实降低了方差</span>
</pre></div>
<figure  style="flex: 76.02459016393442" ><img width="742" height="488" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/97bc036cb6f5c3ce99362eb4bbba4c3d.png" alt=""/></figure><h4>Diabetes dataset</h4>
<p>换个数据源，<code>estimator</code>并不需要更换，如果需要换超参，前文也已经讲过了：</p><div class="highlight"><pre><span></span><span class="n">diabetes_X</span><span class="p">,</span> <span class="n">diabetes_y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_diabetes</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">diabetes_X_train</span> <span class="o">=</span> <span class="n">diabetes_X</span><span class="p">[:</span><span class="o">-</span><span class="mi">20</span><span class="p">]</span>
<span class="n">diabetes_X_test</span>  <span class="o">=</span> <span class="n">diabetes_X</span><span class="p">[</span><span class="o">-</span><span class="mi">20</span><span class="p">:]</span>
<span class="n">diabetes_y_train</span> <span class="o">=</span> <span class="n">diabetes_y</span><span class="p">[:</span><span class="o">-</span><span class="mi">20</span><span class="p">]</span>
<span class="n">diabetes_y_test</span>  <span class="o">=</span> <span class="n">diabetes_y</span><span class="p">[</span><span class="o">-</span><span class="mi">20</span><span class="p">:]</span>

<span class="c1"># observe the alpha and the score:</span>

<span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>  <span class="c1"># log10(-4)到log10(-1)共6个数做alpha</span>
<span class="nb">print</span><span class="p">(</span><span class="n">alphas</span><span class="p">)</span>
<span class="nb">print</span><span class="p">([</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">regr</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">diabetes_X_train</span><span class="p">,</span> <span class="n">diabetes_y_train</span><span class="p">)</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">diabetes_X_test</span><span class="p">,</span> <span class="n">diabetes_y_test</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">%&#39;</span>
       <span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">])</span>
</pre></div>

<pre><code>[0.0001     0.00039811 0.00158489 0.00630957 0.02511886 0.1       ]
['58.51%', '58.52%', '58.55%', '58.56%', '58.31%', '57.06%']
</code></pre>
<h2>Lasso regression</h2>
<p>Lasso = least absolute shrinkage and selection operator</p><p>相比Ridge, Lasso会真的把一些feature系数置0 (<strong>sparse method</strong>)，适用奥卡姆剃刀原理(<strong>Occam’s razor</strong>: prefer simpler models)</p><div class="highlight"><pre><span></span><span class="n">regr</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">Lasso</span><span class="p">()</span>
<span class="n">scores</span> <span class="o">=</span> <span class="p">[</span><span class="n">regr</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
              <span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">diabetes_X_train</span><span class="p">,</span> <span class="n">diabetes_y_train</span><span class="p">)</span>
              <span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">diabetes_X_test</span><span class="p">,</span> <span class="n">diabetes_y_test</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">]</span>
<span class="n">best_alpha</span> <span class="o">=</span> <span class="n">alphas</span><span class="p">[</span><span class="n">scores</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">scores</span><span class="p">))]</span>
<span class="n">regr</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">best_alpha</span>   <span class="c1"># 不链式调用的话不需要用set_params</span>
<span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">diabetes_X_train</span><span class="p">,</span> <span class="n">diabetes_y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
</pre></div>

<pre><code>[   0.         -212.43764548  517.19478111  313.77959962 -160.8303982
   -0.         -187.19554705   69.38229038  508.66011217   71.84239008]
</code></pre>
<h3>Different algorithms for the same problem</h3>
<p>Different algorithms can be used to solve the same mathematical problem. For instance the <code>Lasso</code> object in scikit-learn solves the lasso regression problem using a <code>coordinate descent</code> method, that is efficient on <strong>large datasets</strong>. However, scikit-learn also provides the <code>LassoLars</code> object using the <code>LARS</code> algorithm, which is very efficient for problems in which the weight vector estimated is very <strong>sparse</strong> (i.e. problems with <strong>very few</strong> observations).</p><h2>Classification</h2>
<h3>Logistic Regerssion</h3>
<figure  style="flex: 66.66666666666667" ><img width="400" height="300" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/386b1a1250985ccfdacd014b942dff98.png" alt=""/></figure><div class="highlight"><pre><span></span><span class="n">log</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">1e5</span><span class="p">)</span>
<span class="n">log</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris_X_train</span><span class="p">,</span> <span class="n">iris_y_train</span><span class="p">)</span>
</pre></div>

<pre><code>LogisticRegression(C=100000.0)
</code></pre>
<ul>
<li>The <code>C</code> parameter controls the <strong>amount of regularization</strong> in the LogisticRegression object:<ul>
<li>a large value for <code>C</code> results in <strong>less regularization</strong>.</li>
</ul>
</li>
<li><code>penalty=&quot;l2&quot;</code> gives <strong>Shrinkage</strong> (i.e. non-sparse coefficients),</li>
<li><code>penalty=&quot;l1&quot;</code> gives <strong>Sparsity</strong>.</li>
</ul>
<p><strong>DEMO</strong>: 比较<code>KNN</code>和<code>LogisticRegression</code></p><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">neighbors</span><span class="p">,</span> <span class="n">linear_model</span>

<span class="n">X_digits</span><span class="p">,</span> <span class="n">y_digits</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_digits</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">X_digits</span> <span class="o">=</span> <span class="n">X_digits</span> <span class="o">/</span> <span class="n">X_digits</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>

<span class="n">n_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_digits</span><span class="p">)</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">X_digits</span><span class="p">[:</span><span class="nb">int</span><span class="p">(</span><span class="mf">.9</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">)]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y_digits</span><span class="p">[:</span><span class="nb">int</span><span class="p">(</span><span class="mf">.9</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">)]</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_digits</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="mf">.9</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">):]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">y_digits</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="mf">.9</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">):]</span>

<span class="n">knn</span> <span class="o">=</span> <span class="n">neighbors</span><span class="o">.</span><span class="n">KNeighborsClassifier</span><span class="p">()</span>
<span class="n">logistic</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;KNN score: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;LogisticRegression score: </span><span class="si">%f</span><span class="s1">&#39;</span>
      <span class="o">%</span> <span class="n">logistic</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
</pre></div>

<pre><code>KNN score: 0.961111
LogisticRegression score: 0.933333
</code></pre>
<h2>Support vector machines (SVMs)</h2>
<p>支持向量机(support vector machines,SVM)是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器。除此之外，SVM算法还包括核函数，核函数可以使它成为非线性分类器。</p><p><code>Support Vector Machines</code> belong to the <strong>discriminant model family</strong>:</p><p>they try to find a combination of samples to <strong>build a plane</strong> maximizing the margin between the two classes. <code>Regularization</code> is set by the <code>C</code> parameter:</p><ul>
<li>a <strong>small</strong> value for C means the margin is calculated using <strong>many or all</strong> of the observations around the separating line (more regularization);</li>
<li>a <strong>large</strong> value for C means the margin is calculated on observations <strong>close to</strong> the separating line (less regularization).</li>
</ul>
<p>SVMs can be used：</p><ul>
<li>in regression –<code>SVR</code> (Support Vector Regression)–,</li>
<li>or in classification –<code>SVC</code> (Support Vector Classification).</li>
</ul>
<p>SVM模型有两个非常重要的参数C与gamma。其中:</p><ul>
<li>C是惩罚系数，即对误差的宽容度。c越高，说明越不能容忍出现误差,容易过拟合。C越小，容易欠拟合。C过大或过小，泛化能力变差</li>
<li>gamma是选择RBF函数作为kernel后，该函数自带的一个参数。隐含地决定了数据映射到新的特征空间后的分布，gamma越大，支持向量越少，gamma值越小，支持向量越多。支持向量的个数影响训练与预测的速度。</li>
</ul>
<h3>Using kernels</h3>
<p>Classes are not always <strong>linearly separable</strong> in feature space. The solution is to <code>build a decision function</code> that is not linear but may be <strong>polynomial</strong> instead.</p><p>This is done using the <code>kernel</code> trick that can be seen as creating a decision energy by positioning kernels on observations:</p><h4>Linear kernal</h4>
<div class="highlight"><pre><span></span><span class="n">svc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)</span>
</pre></div>
<h4>Polynomial kernel</h4>
<div class="highlight"><pre><span></span><span class="n">svc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
<h4>RBF kernel (Radial Basis Function)</h4>
<div class="highlight"><pre><span></span><span class="n">svc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rfb&#39;</span><span class="p">)</span>
</pre></div>
<p><strong>DEMO</strong>: Plot different SVM classifiers in the iris dataset</p><ul>
<li>LinearSVC minimizes the squared hinge loss while SVC minimizes the regular hinge loss.</li>
<li>LinearSVC uses the One-vs-All (also known as One-vs-Rest) multiclass reduction while SVC uses the One-vs-One multiclass reduction.</li>
</ul>
<div class="highlight"><pre><span></span><span class="c1"># 代码片段，定义4个estimator</span>
<span class="n">C</span> <span class="o">=</span> <span class="mf">1.0</span>  <span class="c1"># SVM regularization parameter</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">(</span><span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">),</span>
          <span class="n">svm</span><span class="o">.</span><span class="n">LinearSVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">),</span>
          <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">),</span>
          <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">))</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">clf</span> <span class="ow">in</span> <span class="n">models</span><span class="p">)</span>
</pre></div>
<figure  style="flex: 79.81132075471699" ><img width="846" height="530" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/73af5427b414af6455402c92562d3524.png" alt=""/></figure><h2>cross validation</h2>
<p><a href="https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation">https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation</a></p><p><strong>RAW DEMO</strong>: KFold cross-validation:</p><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">svm</span>

<span class="n">X_digits</span><span class="p">,</span> <span class="n">y_digits</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_digits</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">svc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)</span>

<span class="n">score</span> <span class="o">=</span> <span class="n">svc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_digits</span><span class="p">[:</span><span class="o">-</span><span class="mi">100</span><span class="p">],</span> <span class="n">y_digits</span><span class="p">[:</span><span class="o">-</span><span class="mi">100</span><span class="p">])</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_digits</span><span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="p">:],</span> <span class="n">y_digits</span><span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="p">:])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>

<span class="n">X_folds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">X_digits</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># 分成了3个fold</span>
<span class="n">y_folds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">y_digits</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="c1"># We use &#39;list&#39; to copy, in order to &#39;pop&#39; later on</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">X_folds</span><span class="p">)</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="c1"># 取出最后一个fold # 不对，python居然可以Pop任意一个索引</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> <span class="c1"># 把剩下的fold拼回去</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">y_folds</span><span class="p">)</span>
    <span class="n">y_test</span> <span class="o">=</span> <span class="n">y_train</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
    <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">svc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
<span class="n">scores</span>
</pre></div>

<pre><code>0.98
[0.9348914858096828, 0.9565943238731218, 0.9398998330550918]
</code></pre>
<p>Scikit-learn肯定是提供了官方支持的：</p><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">KFold</span><span class="p">,</span> <span class="n">cross_val_score</span>

<span class="c1"># step 1: 用KFold和fold数做一个KFold对象，然后用这个KFold对象去循环（其实就是一个generator)</span>
<span class="c1"># step 2: 每次循环自己手动计算score</span>
<span class="n">k_fold</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="p">[</span><span class="n">svc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_digits</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">y_digits</span><span class="p">[</span><span class="n">train</span><span class="p">])</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_digits</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">y_digits</span><span class="p">[</span><span class="n">test</span><span class="p">])</span> \
         <span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">k_fold</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X_digits</span><span class="p">)]</span>
<span class="n">scores</span>
</pre></div>
<p>[0.9348914858096828, 0.9565943238731218, 0.9398998330550918]</p>
<pre><code>可见官方api分折和我们手动split，每次从后向前取一折做测试集结果是一致的
当然，打印cross_validation的结果也是有封装的：
```python
# 把KFolder对象传入即可
scores = cross_val_score(svc, X_digits, y_digits, cv=k_fold)
print(scores)

# 定制scoring method:
scores = cross_val_score(svc, X_digits, y_digits, cv=k_fold, scoring='precision_macro')
print(scores)
</code></pre>

<pre><code>[0.93489149 0.95659432 0.93989983]
[0.93969761 0.95911415 0.94041254]
</code></pre>
<h5>Cross-validation generators:</h5>
<p><a href="https://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html">see more</a> cross-validation generators:</p><p>$
\begin{array}{l|l|l}
KFold &amp; StratifiedKFold &amp; GroupKFold \
\hline
ShuffleSplit &amp; StratifiedShuffleSplit &amp; GroupShuffleSplit \
\hline
LeaveOneGroupOut &amp; LeavePGroupOut &amp; LeaveOneOut \
\hline
LeavePOut &amp; PredefinedSplit
\end{array}
$</p><h4>Datatransformation with held out data</h4>
<div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
<span class="o">...</span>     <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">X_train_transformed</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_transformed</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">X_test_transformed</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_transformed</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="mf">0.9333</span><span class="o">...</span>

<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">clf</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">preprocessing</span><span class="o">.</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">)</span>
<span class="n">array</span><span class="p">([</span><span class="mf">0.977</span><span class="o">...</span><span class="p">,</span> <span class="mf">0.933</span><span class="o">...</span><span class="p">,</span> <span class="mf">0.955</span><span class="o">...</span><span class="p">,</span> <span class="mf">0.933</span><span class="o">...</span><span class="p">,</span> <span class="mf">0.977</span><span class="o">...</span><span class="p">])</span>
</pre></div>
<h4>cross_validate v.s. cross_val_score</h4>
<p>The cross_validate function differs from cross_val_score in two ways:</p><ul>
<li>It allows specifying multiple metrics for evaluation.</li>
<li>It returns a dict containing fit-times, score-times (and optionally training scores as well as fitted estimators) in addition to the test score.</li>
</ul>
<div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">recall_score</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">scoring</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;precision_macro&#39;</span><span class="p">,</span> <span class="s1">&#39;recall_macro&#39;</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">scoring</span><span class="p">)</span>  <span class="c1"># 以字典返回validate几个指标</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="p">[</span><span class="s1">&#39;fit_time&#39;</span><span class="p">,</span> <span class="s1">&#39;score_time&#39;</span><span class="p">,</span> <span class="s1">&#39;test_precision_macro&#39;</span><span class="p">,</span> <span class="s1">&#39;test_recall_macro&#39;</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_recall_macro&#39;</span><span class="p">]</span>
<span class="n">array</span><span class="p">([</span><span class="mf">0.96</span><span class="o">...</span><span class="p">,</span> <span class="mf">1.</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.96</span><span class="o">...</span><span class="p">,</span> <span class="mf">0.96</span><span class="o">...</span><span class="p">,</span> <span class="mf">1.</span>        <span class="p">])</span>
</pre></div>
<h2>grid search</h2>
<p><a href="https://scikit-learn.org/stable/modules/grid_search.html#grid-search">https://scikit-learn.org/stable/modules/grid_search.html#grid-search</a></p><p>对不同参数进行组合遍历，目的是为了<code>maximize the cross-validation score</code></p><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span><span class="p">,</span> <span class="n">cross_val_score</span>
<span class="n">Cs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">svc</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="n">Cs</span><span class="p">),</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_digits</span><span class="p">[:</span><span class="mi">1000</span><span class="p">],</span> <span class="n">y_digits</span><span class="p">[:</span><span class="mi">1000</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;best score:&#39;</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">best_score_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;best estimator.c:&#39;</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">C</span><span class="p">)</span>
<span class="c1"># Prediction performance on test set is not as good as on train set</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_digits</span><span class="p">[</span><span class="mi">1000</span><span class="p">:],</span> <span class="n">y_digits</span><span class="p">[</span><span class="mi">1000</span><span class="p">:])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;score:&#39;</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>
</pre></div>

<pre><code>best score: 0.95
best estimator.c: 0.0021544346900318843
score: 0.946047678795483
</code></pre>
<p>与此同时，每个estimator也有自己的CV版本（跟之前串课的笔记呼应上了）</p><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span><span class="p">,</span> <span class="n">datasets</span>
<span class="n">lasso</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LassoCV</span><span class="p">()</span>
<span class="n">X_diabetes</span><span class="p">,</span> <span class="n">y_diabetes</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_diabetes</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_diabetes</span><span class="p">,</span> <span class="n">y_diabetes</span><span class="p">)</span>
<span class="c1"># The estimator chose automatically its lambda:</span>
<span class="n">lasso</span><span class="o">.</span><span class="n">alpha_</span>
</pre></div>

<pre><code>0.003753767152692203
</code></pre>
<h2>Unsupervised learning: seeking representations of the data</h2>
<h3>Clustreing: grouping observations together</h3>
<h3>K-means clustreing</h3>
<p>随机选k个质心计算所有的点的距离，然后再取每个群里的均值做质心，如此往复。结果的随机性很强（前面剧透了，把k-means和knn搞混过）</p><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">cluster</span><span class="p">,</span> <span class="n">datasets</span>
<span class="n">X_iris</span><span class="p">,</span> <span class="n">y_iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">k_means</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">k_means</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_iris</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">k_means</span><span class="o">.</span><span class="n">labels_</span><span class="p">[::</span><span class="mi">10</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_iris</span><span class="p">[::</span><span class="mi">10</span><span class="p">])</span>
</pre></div>

<pre><code>[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2]
[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2]
</code></pre>
<div class="highlight"><pre><span></span><span class="n">k_means</span><span class="o">.</span><span class="n">cluster_centers_</span>
</pre></div>

<pre><code>array([[5.006     , 3.428     , 1.462     , 0.246     ],
       [5.9016129 , 2.7483871 , 4.39354839, 1.43387097],
       [6.85      , 3.07368421, 5.74210526, 2.07105263]])
</code></pre>
<h3>未完结</h3>
<p><a href="https://scikit-learn.org/stable/tutorial/statistical_inference/unsupervised_learning.html">https://scikit-learn.org/stable/tutorial/statistical_inference/unsupervised_learning.html</a></p></div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/scikit-learn%E5%AE%98%E7%BD%91%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0%28%E4%B8%80%29/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/%E4%BB%8E%E6%8A%95%E5%BD%B1%E3%80%81%E6%AD%A3%E4%BA%A4%E8%A1%A5%E8%A7%92%E5%BA%A6%E8%AF%81%E6%98%8E%EF%BC%88%E6%8E%A8%E5%AF%BC%EF%BC%89%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E5%85%AC%E5%BC%8F/" target="_self">从投影、正交补角度证明（推导）最小二乘法公式</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/%E4%BB%8E%E6%8A%95%E5%BD%B1%E3%80%81%E6%AD%A3%E4%BA%A4%E8%A1%A5%E8%A7%92%E5%BA%A6%E8%AF%81%E6%98%8E%EF%BC%88%E6%8E%A8%E5%AF%BC%EF%BC%89%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E5%85%AC%E5%BC%8F/" target="_self">
                <time class="text-uppercase">
                    January 25 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><p>学习线性回归的时候，会教我们$X\theta=y$可以直接用<strong>最小二乘法</strong>直接把$\theta$求出来：</p><p>$\theta=(X^TX)^{-1}X^Ty$</p><p>并且还在我<a href="https://blog.wzy.one/archives/%E7%9F%A9%E9%98%B5%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2%E7%9F%A9%E9%98%B5/">之前的博文</a>里直接应用了一番（那是根据公式来应用，即如何构建正确的A和y，从而应用公式直接求解$\theta$)，里面还引了一篇详实的证明文章。</p><p>首先，在吴恩达的教材里，这个并不叫最小二乘(<code>least suqare</code>），而是叫<code>Normal Equation method</code>，这个不重要，毕竟在可汗学院的教材里，又叫最小二乘了^^。今天补充的内容，就是在回顾之前的笔记的时候，发现了大量的证明和应用这个公式的地方，而且全是在引入了投影(<code>Projection</code>)概念之后。因为那个时候并没有接触机器学习，看了也就看了，现在看到了应用场景，那就闭环了，回顾一下：</p><p>首先，预备知识</p><h2>子空间</h2>
<figure class="vertical-figure" style="flex: 35.609397944199706" ><img width="970" height="1362" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/454cb3771fb0eb69bf0301b42ad8d9f3.png" alt=""/></figure><p>笔记很清楚了，对于一个矩阵
$A= \begin{bmatrix}
   -2 &amp; -1 &amp; -3 \
   4 &amp; 2 &amp; 6 \
\end{bmatrix}$ 它的列空间是自然是C(A)，行空间自然是A的转置的后的<code>列空间</code>，然后各自拥有一个对应的零空间（即求解$Ax=0, A^Tx=0）$</p><p>上图用红框框出来的部分即是具体这个矩阵$A$的四个子空间。同时，拥有如下性质：</p><ol>
<li>$C(A)$与$N(A^T)$正交(<code>orthogonal</code>)，即列空间与左零空间正交</li>
<li>$C(A^T)$与$N(A)$正交，即行空间与零空间正交</li>
</ol>
<h2>正交补</h2>
<p>$V^\bot = {\vec x \in \R^n | \vec x \cdot \vec{v} = 0; for; every\ \vec{v} \in V \text{}}$ 即V的正交补为垂直于V内任意一个向量的所有向量。</p><p>那么:</p><ul>
<li>$C(A) = (N(A^T))^\bot$</li>
<li>$C(A^T) = (N(A))^\bot$</li>
</ul>
<h2>投影是一个线性变换</h2>
<figure  style="flex: 66.66666666666667" ><img width="1240" height="930" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/d73cc50bd39caeb73a1816384ee8e8d7.png" alt=""/></figure>
这里已经看到我们熟悉的$(A^TA)^{-1}A^Tx$了，我们来看一下推导过程：<ol>
<li>$\vec x$在$V \in \R^n$上的投影$Proj_V^{\vec x} = \vec v$必然能表示成该空间的<code>basis</code>{$\vec b_1, \vec b_2, \vec b_3, \dots$}的线性变换：$\vec v \in V = y_1\vec b_1 + y_2\vec b_2 + \cdots + y_k \vec b_k =  A\vec y$</li>
<li>求出$\vec y$则求出了这个投影在哪里</li>
<li>$\vec x$能向$V$投影，自然也能向$V^\bot$投影($\vec w$)</li>
</ol>
<ul>
<li>这里是故意这么说的，强调都是投影，其实在向$V$投影时，在$V^\bot$的投影（$\vec w$）就是那条<strong>垂线</strong></li>
</ul>
<ol start="4">
<li>$V \Rightarrow C(A),; V^\bot \Rightarrow N(A^T), \vec v \in V, \vec w \in V^\bot$</li>
<li>左零空间只不过是转置的零空间，那么零空间的特性是什么呢？即$A\vec x = 0$的空间，那么$\vec w$在左零空间里，意味着: $A^T\vec w = 0$</li>
<li>$\vec w = \vec x - \vec v = \vec x - A\vec y \Rightarrow A^T(\vec x - A\vec y) = 0 \Rightarrow A^T \vec x = A^TA\vec y$</li>
<li>只要$A^TA$可逆的话: $\Rightarrow \vec  y= (A^TA)^{-1}A^T\vec x$</li>
<li>$\therefore Proj_V^{\vec x} = A\vec y = A(A^TA)^{-1}A^T\vec x$</li>
<li>得证$\vec x$在$V$上的投影就是一个线性变换</li>
<li>$\vec y$即是机器学习中我们需要学习到的<strong>系数</strong> = $(A^TA)^{-1}A^T$</li>
</ol>
<h2>最小二乘逼近</h2>
<p>由此到了下一课，<code>the lease squares approximation</code>，讲的就是$A\vec x = \vec b$无解时，意思就是在$\vec b$不存在A的张成子空间中，所以无论进行怎样的<strong>线性变换</strong>，都是不可能得到$\vec b$的，则取$\vec x$在$C(A)$中的投影作为近似的解（证明就不再展开了）
<figure class="vertical-figure" style="flex: 49.5603517186251" ><img width="1240" height="1251" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/d390aad67946ec06a573bddf79d29339.png" alt=""/></figure>
仍然用的是同一个思路，即&quot;垂线在左零空间中&quot;，来构造$A^T\cdot \vec w = \vec 0$</p><h2>应用最小二乘拟合一条回归线</h2>
<p>这里终于讲到了与机器学习最接近的内容：<code>regression</code>
<figure  style="flex: 164.89361702127658" ><img width="1240" height="376" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/2da3910c441eb8061e11ea708824fd7d.png" alt=""/></figure>
可以看到，毫无业务思维的花花肠子，很多机器学习课程里会花大量工夫从感性到理性上给你讲这些内容，因为它的期望从0跟你讲清楚，而在循序渐进的数学理论体系里，这些根本就不需要关联感性认识的，什么每年的房价啊，数学关注的只是建模。</p><p>这个回归实例里，因为需要拟合的是一条直线：$y = b + ax$，那么既有的数据就成了机器学习里的“样本”，但我们这里不需要这么理解，而是直接理解为矩阵，得到
方程组：</p><p>$$\begin{cases}
b + a = 1 \\
b + 2a = 2 \\
b + 3a = 2
\end{cases}$$</p>
<p>提取矩阵：</p><p>$$A = \begin{bmatrix}1&amp;1\\1&amp; 2\\ 1&amp; 3\end{bmatrix}, \vec b = \begin{bmatrix}1\\2\\ 2\end{bmatrix} \Rightarrow A\vec x = \vec b$$</p>
<p>好了，在上面提到的<a href="https://www.jianshu.com/p/c2d0c743dc5d">这篇博文</a>里，我们不明就里地直接用了公式，已知A和b求变换矩阵M(即这里的$\vec x$)，还当成是机器学习的内容，而现在我们已经知道自己是在做什么，就是找b在$A$的张成子空间里的投影，就能得到最近似的解</p><p>$$\vec x \approx (A^TA)^{-1}A^T\vec b$$</p>
</div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/%E4%BB%8E%E6%8A%95%E5%BD%B1%E3%80%81%E6%AD%A3%E4%BA%A4%E8%A1%A5%E8%A7%92%E5%BA%A6%E8%AF%81%E6%98%8E%EF%BC%88%E6%8E%A8%E5%AF%BC%EF%BC%89%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E5%85%AC%E5%BC%8F/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
    <article class="yue prism-post-item col-md-8 offset-md-2">
        <h1 class="prism-post-title"><a class="no-link" href="/archives/%E7%9F%A9%E9%98%B5%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2%E7%9F%A9%E9%98%B5/" target="_self">矩阵最小二乘法求解仿射变换矩阵</a></h1>
        <div class="prism-post-time">
            <a class="no-link" href="/archives/%E7%9F%A9%E9%98%B5%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2%E7%9F%A9%E9%98%B5/" target="_self">
                <time class="text-uppercase">
                    January 17 2021
                </time>
            </a>
        </div>
        <div class="prism-content"><p>一个矩形三个顶点<code>(0,0), (50, 0), (50, 50)</code>, 变换后为<code>(30, 30), (30, 130), (130, 130)</code>, 求其仿射矩阵。</p><p>我们分别设起始和结束矩阵的坐标为：$(a_x, a_y), (b_x, b_y), (c_x, c_y)$， 变换后的加一个prime（$ ^\prime$)符号，以此类推。<br />
要知道，一个3X2的矩阵是不可能右乘一个矩阵得到一个3X2的矩阵（只能左乘一个3X3的），<br />
然后，每一个新坐标，都是由原坐标的(x, y)经过变换得到(x', y‘），即使是新坐标的X值，也是需要原坐标的(x, y)值参与过来进行变化的（乘以合适的系数），然后还要加上偏移的系数，以<code>x'</code>为例，应该是这样：$a^\prime_x = a_x m_{00} + a_y m_{01} + m_{02} $
我们根据矩阵特征，补一个1，构造这个矩阵看看效果：</p><p>$$
\begin{bmatrix}
\color{red}{a_x} &amp; \color{red}{a_y} &amp; \color{red}1 \\
b_x &amp; b_y &amp; 1 \\
c_x &amp; c_y &amp; 1 \\
\end{bmatrix}
\begin{bmatrix}
\color{red}{m_{00}} \\ \color{red}{m_{01}} \\ \color{red}{m_{02}}
\end{bmatrix} = 
\begin{bmatrix}
\color{red}{a^\prime_x} \\ b^\prime_x \\ c^\prime_x
\end{bmatrix} \tag{红色部分即为上面的等式}
$$</p>
<p>这只是把三个x给变换出来了，<strong>其实你也可以认为这是把y给变换出来了</strong>（因为原理一样，只是系数不同）。<br />
做到这一步，我们已经知道要如何求y坐标了，即我们只补一列的话，只能得到一个坐标的x值（或y值），要求另一半，根据坐标相乘的原理，看来只能把前三列置零，再把后三列复制进去了（__这样仿射矩阵也就变成6X1了__），其实就是上面矩阵乘法的重复，只不过交错一下形成x,y交错的排列：</p><p>$$
\begin{bmatrix}
a_x &amp; a_y &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; a_x &amp; a_y &amp; 1 \\
b_x &amp; b_y &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; b_x &amp; b_y &amp; 1 \\
c_x &amp; c_y &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; c_x &amp; c_y &amp; 1 
\end{bmatrix}
\begin{bmatrix}
m_{00} \\ m_{01} \\ m_{02} \\ m_{10} \\ m_{11} \\ m_{12}
\end{bmatrix} = 
\begin{bmatrix}
a^\prime_x \\ a^\prime_y \\ b^\prime_x \\ b^\prime_y \\ c^\prime_x \\ c^\prime_y \\
\end{bmatrix}
$$</p>
<p>原理当然就是把第一个公式补全：</p><p>$$
\begin{cases}
    \; a^\prime_x = a_x m_{00} + a_y m{01} + m_{02} \\
    \; a^\prime_y = a_x m_{10} + a_y m{11} + m_{12} \\
    \\
    \; b^\prime_x = b_x m_{00} + b_y m{01} + m_{02} \\
    \; b^\prime_y = b_x m_{10} + b_y m{11} + m_{12} \\
    \\
    \; c^\prime_x = c_x m_{00} + c_y m{01} + m_{02} \\
    \; c^\prime_y = c_x m_{10} + c_y m{11} + m_{12} \\
\end{cases}
$$</p>
<p>最小二乘的公式如下：</p><p>$$
\begin{aligned}
&amp;\lVert A\beta - Y \rVert{^2_2} \quad A \in \mathbb{R}^{(m\times n+1)}, \beta \in \mathbb{R}^{(n+1)\times 1}, Y \in \mathbb{R}^{m\times 1} \\
&amp;\hat \beta = (A^TA)^{-1}A^TY
\end{aligned}
$$</p>
<p><a href="https://iewaij.github.io/introDataScience/OLS.html">推导过程见此</a></p><blockquote>
<p>奇异矩阵没有逆矩阵，$(A^TA)^{-1}$会出现无法求解的问题，也就是该方法对数据是有约束的，这个有解，另议。</p></blockquote>
<p>我们把A和Y都做出来了，直接套用公式即可，为了编程方便，我们把前后矩阵设为A和B，仿射矩阵为M，就成了：</p><p>$$
M = (A^TA)^{-1}A^TB
$$</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">A</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">]]</span>
<span class="n">B</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">30</span><span class="p">,</span> <span class="mi">30</span><span class="p">],</span> <span class="p">[</span><span class="mi">130</span><span class="p">,</span> <span class="mi">30</span><span class="p">],</span> <span class="p">[</span><span class="mi">130</span><span class="p">,</span> <span class="mi">130</span><span class="p">]]</span>

<span class="c1"># 分别整理成上面分析的6x6和6x1的矩阵</span>
<span class="c1"># 先定义变量保留6个坐标的值</span>
<span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">ay</span><span class="p">),</span> <span class="p">(</span><span class="n">bx</span><span class="p">,</span> <span class="n">by</span><span class="p">),</span> <span class="p">(</span><span class="n">cx</span><span class="p">,</span> <span class="n">cy</span><span class="p">)</span> <span class="o">=</span> <span class="n">A</span>
<span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ay1</span><span class="p">),</span> <span class="p">(</span><span class="n">bx1</span><span class="p">,</span> <span class="n">by1</span><span class="p">),</span> <span class="p">(</span><span class="n">cx1</span><span class="p">,</span> <span class="n">cy1</span><span class="p">)</span> <span class="o">=</span> <span class="n">B</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="n">ax</span><span class="p">,</span> <span class="n">ay</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">ay</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="n">bx</span><span class="p">,</span> <span class="n">by</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">bx</span><span class="p">,</span> <span class="n">by</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="n">cx</span><span class="p">,</span> <span class="n">cy</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">cx</span><span class="p">,</span> <span class="n">cy</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="p">])</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ay1</span><span class="p">,</span> <span class="n">bx1</span><span class="p">,</span> <span class="n">by1</span><span class="p">,</span> <span class="n">cx1</span><span class="p">,</span> <span class="n">cy1</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># 比手写6X1矩阵要省事</span>
<span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">A</span><span class="p">)</span> <span class="o">@</span> <span class="n">A</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">B</span> <span class="c1"># 套公式</span>
<span class="n">M</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
<p>输出：</p><div class="highlight"><pre><span></span>array<span class="o">([[</span> <span class="m">2</span>.,  <span class="m">0</span>., <span class="m">30</span>.<span class="o">]</span>,
       <span class="o">[</span> <span class="m">0</span>.,  <span class="m">2</span>., <span class="m">30</span>.<span class="o">]])</span>
</pre></div>
<hr />
<p>上就是最小二乘的一个应用，也给了一篇链接介绍推导，后来我翻阅学习线代时的笔记，其实有从投影方面给的解释，直观易懂，于是<a href="https://www.jianshu.com/p/39db42a6dd5a">另写了篇博文</a>来介绍这个推导。</p></div>
        
        <div class="prism-action-bar">
            <div class="comment-action action-item"><a class="no-link text-uppercase" href="/archives/%E7%9F%A9%E9%98%B5%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E8%A7%A3%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2%E7%9F%A9%E9%98%B5/#prism__comment" target="_self"><i class="fa fa-comment"></i>Comment</a></div>
        </div>
        
    </article>
    
</section>

<div class="container">
    <section id="prism__page__pagination" class="prism-pagination" class="col-md-8 offset-md-2">
        <ul>
            
            <li class="next">
                <a class="no-link" href="/page/4/" target="_self"><i class="fa fa-chevron-left" aria-hidden="true"></i>Newer</a>
            </li>
            
            
            <li class="prev">
                <a class="no-link" href="/page/6/" target="_self">Older<i class="fa fa-chevron-right" aria-hidden="true"></i></a>
            </li>
            
        </ul>
    </section>
</div>


</main>

            <footer id="prism__footer">
                <section>
                    <div>
                        <nav class="social-links">
                            <ul><li><a class="no-link" title="Twitter" href="https://twitter.com/walkerwzy" target="_blank" rel="noopener noreferrer nofollow"><i class="gi gi-twitter"></i></a></li><li><a class="no-link" title="GitHub" href="https://github.com/walkerwzy" target="_blank" rel="noopener noreferrer nofollow"><i class="gi gi-github"></i></a></li><li><a class="no-link" title="Weibo" href="https://weibo.com/1071696872" target="_blank" rel="noopener noreferrer nofollow"><i class="gi gi-weibo"></i></a></li></ul>
                        </nav>
                    </div>

                    <section id="prism__external_links">
                        <ul>
                            
                            <li>
                                <a class="no-link" target="_blank" href="https://github.com/AlanDecode/Maverick" rel="noopener noreferrer nofollow">Maverick</a>：🏄‍ Go My Own Way.
                                <span>|</span>
                            </li>
                            
                            <li>
                                <a class="no-link" target="_blank" href="https://www.imalan.cn" rel="noopener noreferrer nofollow">Triple NULL</a>：Home page for AlanDecode.
                                <span>|</span>
                            </li>
                            
                        </ul>
                    </section>

                    <div class="copyright">
                        <p class="copyright-text">
                            <span class="brand">walker's code blog</span>
                            <span>Copyright © 2022 AlanDecode</span>
                        </p>
                        <p class="copyright-text powered-by">
                            | Powered by <a href="https://github.com/AlanDecode/Maverick" class="no-link" target="_blank" rel="noopener noreferrer nofollow">Maverick</a> | Theme <a href="https://github.com/Reedo0910/Maverick-Theme-Prism" target="_blank" class="no-link" rel="noopener noreferrer nofollow">Prism</a>
                        </p>
                    </div>
                    <div class="footer-addon">
                        
                    </div>
                </section>
                <script>
                    var site_build_date = "2019-12-06T12:00+08:00"

                </script>
                <script src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/prism-efa8685153.js"></script>
            </footer>
        </div>
    </div>
    </div>

    <script src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/ExSearch/jquery.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/ExSearch/ExSearch-493cb9cd89.js"></script>

    <!--katex-->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/katex.min.js"></script>
    <script>
        mathOpts = {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "\\[", right: "\\]", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false }
            ]
        };

    </script>
    <script defer src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/auto-render.min.js" onload="renderMathInElement(document.body, mathOpts);"></script>

    
</body>

</html>